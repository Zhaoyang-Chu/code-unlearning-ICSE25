doc_id,hash,copies,corpus,text
2999391,-8841777715640657257,17,BigQuery,"
    if obj.func == Q.integer:
        ret = e.is_integer
    if obj.func == Q.imaginary:
        ret = e.is_imaginary
    if obj.func == Q.commutative:
        ret = e.is_commutative

    if ret is None:
        return obj
    return ret


def evaluate_old_assump(pred):
    """"""
    Replace assumptions of expressions replaced with their values in the old
    assumptions (like Q.negative(-1) => True). Useful because some direct
    computations for numeric objects is defined most conveniently in the old
    assumptions"
301882,3414960771891105081,7,BigQuery,"

    def send_data(self):
        """"""Send data packets from the local file to the server""""""
        if not self.connection._sock:
            raise err.InterfaceError(""(0, '')"")
        conn = self.connection

        try:
            with open(self.filename, 'rb') as open_file:
                packet_size = min(conn.max_allowed_packet, 16*1024)  # 16KB is efficient enough
                while True:
                    chunk = open_file.read(packet_size)
                    if not chunk:
                        break
                    conn.write_packet(chunk)
        except"
6317932,4775274123006778408,23,BigQuery,"word.Reserved, ('#pop', 'label')),
            (r'(case|extern|if|intrinsic|return|static|while)\b',
             Keyword.Reserved),
            (r'catch\b', Keyword.Reserved, ('#pop', 'catch')),
            (r'class\b', Keyword.Reserved,
             ('#pop', 'object-body/no-braces', 'class')),
            (r'(default|do|else|finally|try)\b', Keyword.Reserved, '#pop'),
            (r'(dictionary|property)\b', Keyword."
5120791,-5804944245284862985,6,BigQuery,"        return ""<%s at %#x>"" % (self.__class__.__name__, id(self))

    def __setitem__(self, key, value):
        self.data[ref(key, self._remove)] = value

    def copy(self):
        new = WeakKeyDictionary()
        with _IterationGuard(self):
            for key, value in self.data.items():
                o = key()
                if o is not None:
                    new[o] = value
        return new

    __copy__ = copy

    def __deepcopy__(self, memo):
        from copy import deepcopy
        new"
2210638,8749859720797626587,12,BigQuery,"Size() const [member function]
    cls.add_method('GetSerializedSize', 
                   'uint32_t', 
                   [], 
                   is_const=True, is_virtual=True)
    ## lr-wpan-mac-trailer.h (module 'lr-wpan'): static ns3::TypeId ns3::LrWpanMacTrailer::GetTypeId() [member function]
    cls.add_method('GetTypeId', 
                   'ns3::TypeId', 
                   [], 
                   is_static=True)
    ## lr-wpan-mac-trailer."
5393413,-2890569154616112577,7,BigQuery,"    t4 = time.time()

    print('Reading took %.2f seconds. Calculating took %.2f seconds' % ((t2 - t1), (t4 - t3)),
          file=sys.stderr)

#
#  Copyright (c) 2013, Novartis Institutes for BioMedical Research Inc.
#  All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
#
#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
"
5666746,-6562833976724985021,66,BigQuery,"_INSIDE')])
    return

def register_Ns3Hasher_methods(root_module, cls):
    ## hash.h (module 'core'): ns3::Hasher::Hasher(ns3::Hasher const & arg0) [copy constructor]
    cls.add_constructor([param('ns3::Hasher const &', 'arg0')])
    ## hash.h (module 'core'): ns3::Hasher::Hasher() [constructor]
    cls.add_constructor([])
    ## hash.h (module 'core'): ns3::Hasher::Hasher(ns3::Ptr<"
4879071,4107607322018819070,49,BigQuery,"')
        self.assertEqual(parser.unescape('&#0038;'),'&')



class AttributesTestCase(TestCaseBase):

    def test_attr_syntax(self):
        output = [
          (""starttag"", ""a"", [(""b"", ""v""), (""c"", ""v""), (""d"", ""v""), (""e"", None)])
        ]
        self._run_check(""""""<a b='v' c=""v"" d=v e>"""""", output)
        self._run_check(""""""<a  b = 'v' c = ""v"" d = v e>"""""", output)
        "
5075938,9062416393791489435,18,BigQuery,"  This only processes the
        following chunk types, all others are ignored: ``IHDR``,
        ``PLTE``, ``bKGD``, ``tRNS``, ``gAMA``, ``sBIT``.
        """"""

        type, data = self.chunk()
        if type == 'IHDR':
            # http://www.w3.org/TR/PNG/#11IHDR
            if len(data)!= 13:
                raise FormatError('IHDR chunk has incorrect length.')
            (self.width, self.height, self.bitdepth, self.color_type,
             self.compression"
5878534,-4813076472140111368,28,BigQuery,"
            if got == want:
                return True

        # This flag causes doctest to ignore any differences in the
        # contents of whitespace strings.  Note that this can be used
        # in conjunction with the ELLIPSIS flag.
        if optionflags & NORMALIZE_WHITESPACE:
            got =''.join(got.split())
            want =''.join(want.split())
            if got == want:
                return True

        # The ELLIPSIS flag says to let the sequence ""..."" in `want`
        # match any substring in `got`.
        if optionflags & ELLIPSIS:
            if"
3909499,-2329170672686107309,61,BigQuery," could provide additional values.

        Return an int indicating the new absolute position.
        """"""
        self._unsupported(""seek"")

    def tell(self):
        """"""Return an int indicating the current stream position.""""""
        return self.seek(0, 1)

    def truncate(self, pos=None):
        """"""Truncate file to size bytes.

        Size defaults to the current IO position as reported by tell().  Return
        the new size.
        """"""
        self._unsupported(""truncate"")

    ### Flush and close ###

    def flush(self):
        """"""Flush write buffers, if applicable.

        This is not implemented"
544871,5777199578632530049,9,BigQuery,",
            selectforeground=select_colors['foreground'],
            selectbackground=select_colors['background'],
            )

    IDENTCHARS = string.ascii_letters + string.digits + ""_""

    def colorize_syntax_error(self, text, pos):
        text.tag_add(""ERROR"", pos)
        char = text.get(pos)
        if char and char in self.IDENTCHARS:
            text.tag_add(""ERROR"", pos + "" wordstart"", pos)
        if '\n' == text.get(pos):   # error at line end
            text.mark_set"
1786173,4535584132718191894,14,BigQuery,"add_method('Add', 
                   'void', 
                   [param('ns3::ByteTagList const &', 'o')])
    ## byte-tag-list.h (module 'network'): void ns3::ByteTagList::AddAtEnd(int32_t appendOffset) [member function]
    cls.add_method('AddAtEnd', 
                   'void', 
                   [param('int32_t', 'appendOffset')])
    ## byte-tag-list.h (module 'network'): void ns3::ByteTagList::AddAtStart(int32_t prependOffset) [member function]
    cls."
6606711,2717414733249273206,18,BigQuery,"
    if verbose:
        print ""Testing correct dispatch of subclass overloading __r<op>__...""

    # This code tests various cases where right-dispatch of a subclass
    # should be preferred over left-dispatch of a base class.

    # Case 1: subclass of int; this tests code in abstract.c::binary_op1()

    class B(int):
        def __floordiv__(self, other):
            return ""B.__floordiv__""
        def __rfloordiv__(self, other):
            return ""B.__rfloordiv__""

    vereq(B(1) // 1,"
2453750,-4090549063588854759,58,BigQuery,"assertTrue(m_sock.close.called)

    @mock.patch('asyncio.base_events.socket')
    def test_create_datagram_endpoint_no_addrinfo(self, m_socket):
        m_socket.getaddrinfo.return_value = []
        m_socket.getaddrinfo._is_coroutine = False

        coro = self.loop.create_datagram_endpoint(
            MyDatagramProto, local_addr=('localhost', 0))
        self.assertRaises(
            OSError, self.loop.run_until_complete, coro)

    def test_"
2179545,3889838027715924996,6,BigQuery,"loop)

        @asyncio.coroutine
        def task():
            yield from fut1
            try:
                yield from fut2
            except asyncio.CancelledError:
                return 42

        t = asyncio.Task(task(), loop=self.loop)
        test_utils.run_briefly(self.loop)
        self.assertIs(t._fut_waiter, fut1)  # White-box test.
        fut1.set_result(None)
        test_utils.run_briefly(self.loop)
        self.assertIs(t._fut_waiter,"
6500097,5475709056845292853,13,BigQuery,"Equal(42.0, f.clean(42))
        self.assertRaisesMessage(ValidationError, ""'Enter a number.'"", f.clean, 'a')
        self.assertEqual(1.0, f.clean('1.0 '))
        self.assertEqual(1.0, f.clean(' 1.0'))
        self.assertEqual(1.0, f.clean(' 1.0 '))
        self.assertRaisesMessage(ValidationError, ""'Enter a number.'"", f.clean, '1.0a')
        self.assertEqual(f.max_value, None)
        self"
2104653,1131227306413539349,17,BigQuery,"playground.html"", ""en"", parent=self.pf, position=""right"")

            self.assertFalse(self.pg.publisher_public)

            # login as master for approval
            self.slave_page = self.slave_page.reload()

            publish_page(self.slave_page, self.user_master, 'en')

            # publish and approve them all
            publish_page(self.pa, self.user_master, 'en')
            publish_page(self.pb, self.user_master, 'en')
            publish_page(self.pc, self.user_master, 'en')"
6030137,-910466606968593241,12,BigQuery,",y+1,options)
        
    def _move(self, dx, dy):
        self.x = self.x + dx
        self.y = self.y + dy
        
    def clone(self):
        other = Point(self.x,self.y)
        other.config = self.config.copy()
        return other
                
    def getX(self): return self.x
    def getY(self): return self.y

class _BBox(GraphicsObject):
    # Internal base class for objects represented by bounding box
    # (opposite corners) Line segment is a degenerate case.
    
"
5181740,1549352675274140687,7,BigQuery,"    if result.ite is not None:
      raise result.ite
    return

  def metricMonitor(self, name, options):
    """"""
    Parameters:
     - name
     - options
    """"""
    self.send_metricMonitor(name, options)
    self.recv_metricMonitor()

  def send_metricMonitor(self, name, options):
    self._oprot.writeMessageBegin('metricMonitor', TMessageType.CALL, self._seqid)
    args = metricMonitor_args()
    args.name = name
    args.options = options
    args.write(self._oprot)
"
7076511,1565167063983578990,23,BigQuery," data
        elif len(data) < self.length:
            self.length -= len(data)
            self.dataCallback(data)
            return b''


    def _dataReceived_FINISHED(self, data):
        raise RuntimeError(
            ""_ChunkedTransferDecoder.dataReceived called after last ""
            ""chunk was processed"")


    def dataReceived(self, data):
        """"""
        Interpret data from a request or response body which uses the
        I{chunked} Transfer-Encoding.
        """"""
        data = self._buffer + data
        self._buffer = b''
"
4741382,-7030467478337186607,8,BigQuery," the batch size.
    softmax_loss_function: Function (labels, logits) -> loss-batch
      to be used instead of the standard softmax (the default if this is None).
      **Note that to avoid confusion, it is required for the function to accept
      named arguments.**
    name: Optional name for this operation, defaults to ""sequence_loss"".

  Returns:
    A scalar float Tensor: The average log-perplexity per symbol (weighted).

  Raises:
    ValueError: If len(logits) is different from len(targets) or len(weights).
  """"""
  with ops.name_"
1104994,-8930557985433765718,16,BigQuery,"
# Most setting_changed receivers are supposed to be added below,
# except for cases where the receiver is related to a contrib app.

# Settings that may not work well when using 'override_settings' (#19031)
COMPLEX_OVERRIDE_SETTINGS = {'DATABASES'}


@receiver(setting_changed)
def clear_cache_handlers(**kwargs):
    if kwargs['setting'] == 'CACHES':
        from django.core.cache import caches
        caches._caches = threading.local()


@receiver(setting_changed)
def update_installed_"
1195095,-5348123537937074628,8,BigQuery,"Produce a :class:`.ColumnClause` object that has the
    :paramref:`.column.is_literal` flag set to True.

    :func:`.literal_column` is similar to :func:`.column`, except that
    it is more often used as a ""standalone"" column expression that renders
    exactly as stated; while :func:`.column` stores a string name that
    will be assumed to be part of a table and may be quoted as such,
    :func:`.literal_column` can be that, or any other arbitrary column-oriented
    expression.

    :param text: the text of the expression; can be"
6788337,-3698624821544383893,36,BigQuery," Meta on the class will use
            the parent's Meta (or the first parent in the MRO if there are
            multiple parent classes).
            """"""
            pass

        self.assertEqual(list(SubCategoryForm.base_fields),
                         ['name','slug', 'url'])

    def test_subclassmeta_form(self):
        class SomeCategoryForm(forms.ModelForm):
            checkbox = forms.BooleanField()

            class Meta:
                model = Category
                fields = '__all__'

        class SubclassMeta(SomeCategoryForm):
            """""" We can also subclass the Meta inner class to change the fields
            "
3437968,2246537226978377563,7,BigQuery,"'useTcp',
        'useUdp',
    ]

    returnables = [
        'answer_default_zones',
        'cache_size',
        'randomize_query_case',
        'route_domain',
        'use_ipv4',
        'use_ipv6',
        'use_tcp',
        'use_udp',
    ]

    updatables = [
        'answer_default_zones',
        'cache_size',
        'randomize_query_case',
        'route_domain',
        'use_ipv4',
        'use_ipv6"
378761,-8589319941177537193,75,BigQuery,\u2a31\u2a32\u2a33\u2a34\u2a35\u2a36\u2a37\u2a38\u2a39\u2a3a\u2a3b\u2a3c\u2a3d\u2a3e\u2a3f\u2a40\u2a41\u2a42\u2a43\u2a44\u2a45\u2a46\u2a47\u2a48\u2a49\u2a4a\u2a4b\u2a4c\
529317,3856431626675034809,6,BigQuery,", ""
            ""set 'fqdn' instead"", DeprecationWarning, 2)
        self.fqdn = value

    @property
    def __peer(self):
        warn(""Access to __peer attribute on SMTPChannel is deprecated, ""
            ""use 'peer' instead"", DeprecationWarning, 2)
        return self.peer
    @__peer.setter
    def __peer(self, value):
        warn(""Setting __peer attribute on SMTPChannel is deprecated, ""
            ""set 'peer' instead"", DeprecationWarning, 2)
        self.peer = value

    @property
    def __conn(self):
"
3483210,-6276151496439991127,6,BigQuery,"        # simulate reading small chunks of the content
        reused_chunks = iter_slices(self._content, chunk_size)

        stream_chunks = generate()

        chunks = reused_chunks if self._content_consumed else stream_chunks

        if decode_unicode:
            chunks = stream_decode_response_unicode(chunks, self)

        return chunks

    def iter_lines(self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=None, delimiter=None):
        """"""Iterates over the response data, one line at a time.  When
        stream=True"
1437522,-1546806430905143163,7,BigQuery,"self.last_printed = None
        self.create_time = None
        self.last_saved_time = None
        self.num_pages = None
        self.num_words = None
        self.num_chars = None
        self.thumbnail = None
        self.creating_application = None
        self.security = None
        # properties from DocumentSummaryInformation stream
        self.codepage_doc = None
        self.category = None
        self.presentation_target = None
        self.bytes = None
        self.lines = None
        self.paragraphs = None
        self.slides = None
        "
1149724,-4723320599989736978,8,BigQuery,"
            while True:
                coeff = _dlog10(c, e, places)
                # assert len(str(abs(coeff)))-p >= 1
                if coeff % (5*10**(len(str(abs(coeff)))-p-1)):
                    break
                places += 3
            ans = _dec_from_triple(int(coeff<0), str(abs(coeff)), -places)

        context = context._shallow_copy()
        rounding = context._set_rounding(ROUND_HALF_EVEN)
        ans = ans._fix(context)
        context.round"
1149724,2776148735232734370,8,BigQuery,"1(x)*(expm1(x)+2)
    #
    # R times to compute the sequence expm1(z/2**R),
    # expm1(z/2**(R-1)),..., exp(z/2), exp(z).

    # Find R such that x/2**R/M <= 2**-L
    R = _nbits((long(x)<<L)//M)

    # Taylor series.  (2**L)**T > M
    T = -int(-10*len(str(M))//(3*L))
    y = _div_nearest(x, T"
6622119,-955596748511891617,25,BigQuery,"None)]


Optional = _Optional(_root=True)


def _gorg(a):
    """"""Return the farthest origin of a generic class (internal helper).""""""
    assert isinstance(a, GenericMeta)
    while a.__origin__ is not None:
        a = a.__origin__
    return a


def _geqv(a, b):
    """"""Return whether two generic classes are equivalent (internal helper).

    The intention is to consider generic class X and any of its
    parameterized forms (X[T], X[int], etc.) as equivalent.

    However, X is not equivalent to a subclass of X.

    "
4665874,4032993544223655967,12,BigQuery,"', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_262', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_263', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_"
3121205,6923734044711348992,91,BigQuery,"[1]
            assert_equal(
                np.ravel_multi_index(coords, shape, order='F'), uncoords)
            assert_equal(coords, np.unravel_index(uncoords, shape, order='F'))

            coords = np.array(
                [[1, 0, 1, 2, 3, 4], [1, 6, 1, 3, 2, 0], [1, 3, 1, 0, 9, 5]],
                dtype=dtype)
            shape = (5, 8, 10)
            uncoords = 10*(8*coords[0]+coords[1])"
1710093,5710611913770687689,7,BigQuery,"def exportAttributes(self, outfile, level, already_processed, namespace_='', name_='MyDouble'):
        if self.MyAttr is not None and 'MyAttr' not in already_processed:
            already_processed.append('MyAttr')
            outfile.write(' MyAttr=%s' % (self.gds_format_string(quote_attrib(self.MyAttr).encode(ExternalEncoding), input_name='MyAttr'), ))
    def exportChildren(self, outfile, level, namespace_='', name_='MyDouble', fromsubclass_=False):
        pass"
3695899,-1707629815208958511,11,BigQuery," can be IPv4 unicast or IPv6 unicast.
              The default type is IPv4 unicast.
        choices: ['ipv4uni','ipv6uni']
'''

EXAMPLES = '''

- name: CloudEngine BGP test
  hosts: cloudengine
  connection: local
  gather_facts: no
  vars:
    cli:
      host: ""{{ inventory_hostname }}""
      port: ""{{ ansible_ssh_port }}""
      username: ""{{ username }}""
      password: ""{{ password }}""
      transport: cli

  tasks:

  - name"
2862961,-5781489759151777217,38,BigQuery,"< ns3::NetDeviceQueue, ns3::empty, ns3::DefaultDeleter< ns3::NetDeviceQueue > > const &', 'o')])
    ## simple-ref-count.h (module 'core'): static void ns3::SimpleRefCount<ns3::NetDeviceQueue, ns3::empty, ns3::DefaultDeleter<ns3::NetDeviceQueue> >::Cleanup() [member function]
    cls.add_method('Cleanup', 
                   'void', 
                   [], 
                   is_static=True)
    return

def register_Ns3SimpleRefCount__Ns3NixVector_Ns"
2619965,311363568804555416,26,BigQuery,"    'tremble',
    'trembleFuzz',
    'tremble_circlenodesnumber',
    'tremble_circlenodesnumber1',
    'tremble_draw',
    'tremble_ellipsenodesnumber',
    'tremble_ellipsenodesnumber1',
    'tremble_hyperbolanodesnumber',
    'tremble_marknodes',
    'tremble_markuniform',
    'tremble_parabolanodesnumber',
    'triangle',
    'triangleAbc',
    'triangleabc',
    'triangulate',
    'trico"
787299,-8955596640337089339,11,BigQuery," debug_strs is not None:
        debug_strs.append('Required field: latitude not set.')
    if (not self.has_longitude_):
      initialized = 0
      if debug_strs is not None:
        debug_strs.append('Required field: longitude not set.')
    return initialized

  def ByteSize(self):
    n = 0
    return n + 18

  def ByteSizePartial(self):
    n = 0
    if (self.has_latitude_):
      n += 9
    if (self.has_longitude_):
      n += 9
    return n

  "
4863921,8333886935347556072,36,BigQuery,"ware, self).setUp()

        @webob.dec.wsgify()
        def fake_app(req):
            self.context = req.environ['nova.context']
            return webob.Response()

        self.context = None
        self.middleware = nova.api.auth.NovaKeystoneContext(fake_app)
        self.request = webob.Request.blank('/')
        self.request.headers['X_TENANT_ID'] = 'testtenantid'
        self.request.headers['X_AUTH_TOKEN'] = 'testauthtoken'
        self.request.headers['"
2210638,-5582020532856721511,12,BigQuery,"Mac::SetExtendedAddress(ns3::Mac64Address address) [member function]
    cls.add_method('SetExtendedAddress', 
                   'void', 
                   [param('ns3::Mac64Address', 'address')])
    ## lr-wpan-mac.h (module 'lr-wpan'): void ns3::LrWpanMac::SetLrWpanMacState(ns3::LrWpanMacState macState) [member function]
    cls.add_method('SetLrWpanMacState', 
                   'void', 
                   [param('ns3::LrWpanMacState',"
3121245,-8689992899297602073,165,BigQuery,", value, **kwargs)

    def delete_cookie(self, key, path='/', domain=None):
        self.set_cookie(key, max_age=0, path=path, domain=domain,
                        expires='Thu, 01-Jan-1970 00:00:00 GMT')

    # Common methods used by subclasses

    def make_bytes(self, value):
        """"""Turn a value into a bytestring encoded in the output charset.""""""
        # Per PEP 3333, this response body must be bytes. To avoid returning
        # an instance of a subclass, this function returns `bytes(value)`.
        # This doesn"
378761,5808302951986023716,75,BigQuery,u6a27\u6a28\u6a29\u6a2a\u6a2b\u6a2c\u6a2d\u6a2e\u6a2f\u6a30\u6a31\u6a32\u6a33\u6a34\u6a35\u6a36\u6a37\u6a38\u6a39\u6a3a\u6a3b\u6a3c\u6a3d\u6a3e\u6a3f\u6a40\u6a41\u6a
378761,1158728196768174860,75,BigQuery,97f8\u97f9\u97fa\u97fb\u97fc\u97fd\u97fe\u97ff\u9800\u9801\u9802\u9803\u9804\u9805\u9806\u9807\u9808\u9809\u980a\u980b\u980c\u980d\u980e\u980f\u9810\u9811\u9812\u9813\u9814\u9815\u9816\u9817\u9818\u9819\u981a\u981b\u981
4802511,-1113974476384199655,8,BigQuery,"ES)
    n_samples, n_features = X.shape
    shape = (n_samples, n_features + 1)
    if sparse.issparse(X):
        if sparse.isspmatrix_coo(X):
            # Shift columns to the right.
            col = X.col + 1
            # Column indices of dummy feature are 0 everywhere.
            col = np.concatenate((np.zeros(n_samples), col))
            # Row indices of dummy feature are 0,..., n_samples-1.
            row = np.concatenate((np.arange(n_samples"
6242540,-4204498723166036590,22,BigQuery," and start from scratch."")

    parser.set_defaults(system_site_packages=False)
    parser.add_option(
        '--no-site-packages',
        dest='system_site_packages',
        action='store_false',
        help=""DEPRECATED. Retained only for backward compatibility. ""
             ""Not having access to global site-packages is now the default behavior."")

    parser.add_option(
        '--system-site-packages',
        dest='system_site_packages',
        action='store_true',
        help=""Give the virtual environment access to the global site-packages."")

    parser.add_option"
5726684,-1393206361125887544,18,BigQuery,"<paragraph>
            Paragraph.
""""""],
[""""""\
1. Item 1.
2. Item 2.
3. Numbered Title
=================

Paragraph.
"""""",
""""""\
<document source=""test data"">
    <enumerated_list enumtype=""arabic"" prefix="""" suffix=""."">
        <list_item>
            <paragraph>
                Item 1.
        <list_item>
            <paragraph>
                Item 2.
    <system_message level=""2"" line=""3"" source=""test data"" type=""WARNING"">
        <paragraph>
            Enumerated list ends without a blank line;"
2210638,-7712575624066062253,12,BigQuery,"IsMatchingType', 
                   'bool', 
                   [param('ns3::Address const &', 'address')], 
                   is_static=True)
    return

def register_Ns3McpsDataConfirmParams_methods(root_module, cls):
    ## lr-wpan-mac.h (module 'lr-wpan'): ns3::McpsDataConfirmParams::McpsDataConfirmParams() [constructor]
    cls.add_constructor([])
    ## lr-wpan-mac.h (module 'lr-wpan'): ns3::McpsDataConfirmPar"
5665919,8370792965328482537,8,BigQuery,"G, **kwargs),
                     msg=msg.format(flow_func.__name__))

def test_white_harary_1():
    # Figure 1b white and harary (2001)
    # # http://eclectic.ss.uci.edu/~drwhite/sm-w23.PDF
    # A graph with high adhesion (edge connectivity) and low cohesion
    # (vertex connectivity)
    G = nx.disjoint_union(nx.complete_graph(4), nx.complete_graph(4))
    G.remove_node(7)
    for i in range(4, 7):
        "
75088,722019085129401847,8,BigQuery,"(list(self.form_class.base_fields), ['name', 'tags'])

        f = self.form_class({'name': 'apple', 'tags': 'green, red, yummy'})
        self.assert_form_renders(f, """"""<tr><th><label for=""id_name"">Name:</label></th><td><input id=""id_name"" type=""text"" name=""name"" value=""apple"" maxlength=""50"" /></td></tr>
<tr><th><label for=""id_tags"">Tags:</label></th><td><input type=""text"" name=""tags"" value=""green, red,"
1179655,-876569468460387461,6,BigQuery,"UTH_STRICT_PARSING and WWW_AUTH_STRICT or WWW_AUTH_RELAXED
            while authenticate:
                # Break off the scheme at the beginning of the line
                if headername == 'authentication-info':
                    (auth_scheme, the_rest) = ('digest', authenticate)
                else:
                    (auth_scheme, the_rest) = authenticate.split("" "", 1)
                # Now loop over all the key value pairs that come after the scheme,
                # being careful not to roll into the next scheme
                match = www_auth.search(the_rest)
                auth"
378175,4894960863046356714,11,BigQuery,"
        self.fields = deepcopy(self.base_fields)

    def __unicode__(self):
        return self.as_table()

    def __iter__(self):
        for name, field in self.fields.items():
            yield BoundField(self, field, name)

    def __getitem__(self, name):
        ""Returns a BoundField with the given name.""
        try:
            field = self.fields[name]
        except KeyError:
            raise KeyError('Key %r not found in Form' % name)
        return BoundField(self, field, name)

    def _get_errors("
5014583,-4143621250294761831,7,BigQuery,"        ('TSNE with AnnoyTransformer', make_pipeline(
            AnnoyTransformer(n_neighbors=n_neighbors, metric='sqeuclidean'),
            TSNE(metric='precomputed', perplexity=perplexity,
                 method=""barnes_hut"", random_state=42, n_iter=n_iter), )),
        ('TSNE with NMSlibTransformer', make_pipeline(
            NMSlibTransformer(n_neighbors=n_neighbors, metric='sqeuclidean'),
            TSNE(metric='precomputed', perplexity="
5666746,-3164432481771045514,66,BigQuery,"', 'arg0')])
    ## data-collector.h (module'stats'): ns3::DataCollector::DataCollector() [constructor]
    cls.add_constructor([])
    ## data-collector.h (module'stats'): void ns3::DataCollector::AddDataCalculator(ns3::Ptr<ns3::DataCalculator> datac) [member function]
    cls.add_method('AddDataCalculator', 
                   'void', 
                   [param('ns3::Ptr< ns3::DataCalculator >', 'datac')])
    ## data-collector."
712130,-2996070510284103446,28,BigQuery,"
        elif dWeekStart == dSchedStart + relativedelta(days=+7):
            sched_obj.add_restdays(
                cr, uid, sched,'restday_ids2', rest_days=nrest_days,
                context=context
            )
        elif dWeekStart == dSchedStart + relativedelta(days=+14):
            sched_obj.add_restdays(
                cr, uid, sched,'restday_ids3', rest_days=nrest_days,
                context=context
            )
        elif dWeekStart == dSchedStart + relativedelta"
1573604,1574129919951631187,7,BigQuery,"(variance, [-1])
        else:
          mean, variance = nn.weighted_moments(inputs, moments_axes,
                                               batch_weights)

      moving_vars_fn = lambda: (moving_mean, moving_variance)
      if updates_collections is None:
        def _force_updates():
          """"""Internal function forces updates moving_vars if is_training.""""""
          update_moving_mean = moving_averages.assign_moving_average(
              moving_mean, mean, decay, zero_debias=zero_debias_moving_mean)
          update_moving_variance"
5666746,-5573273865409813848,66,BigQuery,"], 
                   is_virtual=True)
    ## double-probe.h (module'stats'): void ns3::DoubleProbe::ConnectByPath(std::string path) [member function]
    cls.add_method('ConnectByPath', 
                   'void', 
                   [param('std::string', 'path')], 
                   is_virtual=True)
    ## double-probe.h (module'stats'): static ns3::TypeId ns3::DoubleProbe::GetTypeId() [member function]
    cls.add_method('GetTypeId', 
                   'ns3::TypeId', 
                   "
1786173,7331795490739132318,14,BigQuery,"apHeader(ns3::RadiotapHeader const & arg0) [copy constructor]
    cls.add_constructor([param('ns3::RadiotapHeader const &', 'arg0')])
    ## radiotap-header.h (module 'network'): ns3::RadiotapHeader::RadiotapHeader() [constructor]
    cls.add_constructor([])
    ## radiotap-header.h (module 'network'): uint32_t ns3::RadiotapHeader::Deserialize(ns3::Buffer::Iterator start) [member function]
    cls.add_method('Deserialize', 
                   "
2302519,-8457099812140125721,29,BigQuery,"parsed_meta = soup.find('meta', {'http-equiv': 'Content-type'})
        content = parsed_meta['content']
        self.assertEqual('text/html; charset=x-sjis', content)

        # But that value is actually a ContentMetaAttributeValue object.
        self.assertTrue(isinstance(content, ContentMetaAttributeValue))

        # And it will take on a value that reflects its current
        # encoding.
        self.assertEqual('text/html; charset=utf8', content.encode(""utf8""))

        # For the rest of the story, see TestSubstit"
6652181,-7553764934410720116,18,BigQuery,"AL_SERVER_ERROR = 500
NOT_IMPLEMENTED = 501
BAD_GATEWAY = 502
SERVICE_UNAVAILABLE = 503
GATEWAY_TIMEOUT = 504
HTTP_VERSION_NOT_SUPPORTED = 505
INSUFFICIENT_STORAGE = 507
NOT_EXTENDED = 510

# Mapping status codes to official W3C names
responses = {
    100: 'Continue',
    101: 'Switching Protocols',

    200: 'OK',
    201: 'Created',
    202: 'Accepted',
    203: 'Non-Authoritative Information',
    204:"
378761,5931711588782332209,75,BigQuery,f1\ue9f2\ue9f3\ue9f4\ue9f5\ue9f6\ue9f7\ue9f8\ue9f9\ue9fa\ue9fb\ue9fc\ue9fd\ue9fe\ue9ff\uea00\uea01\uea02\uea03\uea04\uea05\uea06\uea07\uea08\uea09\uea0a\uea0b\uea0c\uea0d\uea0e\uea0f\uea10\uea11\uea12\ue
6151456,-1267966787538714015,9,BigQuery,"assertEqual([c.next(), c.next()], range(2))
        self.assertEqual(list(a), range(100,2000))
        self.assertEqual(list(c), range(2,2000))

        # test values of n
        self.assertRaises(TypeError, tee, 'abc', 'invalid')
        self.assertRaises(ValueError, tee, [], -1)
        for n in xrange(5):
            result = tee('abc', n)
            self.assertEqual(type(result), tuple)
            self.assertEqual(len(result), n)
            self."
3453632,-8894902982072653447,22,BigQuery,"'description': description,
                'entries': entries,
            }


class BiliBiliBangumiIE(InfoExtractor):
    _VALID_URL = r'https?://bangumi\.bilibili\.com/anime/(?P<id>\d+)'

    IE_NAME = 'bangumi.bilibili.com'
    IE_DESC = 'BiliBili番剧'

    _TESTS = [{
        'url': 'http://bangumi.bilibili.com/anime/1869',
        'info_dict': {
            'id': '1869',
            't"
5741400,-2224437102649022794,8,BigQuery,"Token()
            self.index += 1
            ch = self.scanHexEscape('u')
            if not ch or ch == '\\' or not isIdentifierStart(ch[0]):
                self.throwUnexpectedToken()
            d = ch
        while (self.index < self.length):
            ch = self.ccode()
            if not isIdentifierPart(ch):
                break
            self.index += 1
            d += unichr(ch)

            # '\u' (U+005C, U+0075) denotes an escaped character.
            if (ch == 0x5C):
                d ="
1710863,-1764444756718966717,24,BigQuery,"
            if self.xlo is None or v[0] < self.xlo:
                self.xlo = v[0]
            if self.ylo is None or v[1] < self.ylo:
                self.ylo = v[1]
            if self.zlo is None or v[2] < self.zlo:
                self.zlo = v[2]
            if self.xhi is None or v[0] > self.xhi:
                self.xhi = v[0]
            if self.yhi is None or v[1] > self."
4483982,4221974419457904746,20,BigQuery," has just performed an INSERT statement into
        a table that has an auto-incrementing ID, returns the newly created ID.

        This method also receives the table name and the name of the primary-key
        column.
        """"""
        return cursor.lastrowid

    def lookup_cast(self, lookup_type):
        """"""
        Returns the string to use in a query when performing lookups
        (""contains"", ""like"", etc). The resulting string should contain a '%s'
        placeholder for the column being searched against.
        """"""
        return ""%s""

    def max_in_list_size(self):
        """"""
        Returns"
1786173,-4746670380554365177,14,BigQuery,"IntegerValue::Get() const [member function]
    cls.add_method('Get', 
                   'int64_t', 
                   [], 
                   is_const=True)
    ## integer.h (module 'core'): std::string ns3::IntegerValue::SerializeToString(ns3::Ptr<ns3::AttributeChecker const> checker) const [member function]
    cls.add_method('SerializeToString', 
                  'std::string', 
                   [param('ns3::Ptr< ns3::AttributeChecker const >', 'checker')], 
                   is_const=True, is_virtual"
3711394,400668485334718011,20,BigQuery," options for server
        # certificate validation
        return ssl_wrap_socket(
            sock=conn,
            ca_certs=self.ca_certs,
            ca_cert_dir=self.ca_cert_dir,
            ca_cert_data=self.ca_cert_data,
            server_hostname=hostname,
            ssl_context=ssl_context,
        )


def _match_hostname(cert, asserted_hostname):
    try:
        match_hostname(cert, asserted_hostname)
    except CertificateError as e:
        log.warning(
            ""Certificate did not match"
6348158,2232167359357131931,10,BigQuery," the
              normal equation, which improves convergence if the Jacobian is
              rank-deficient [Byrd]_ (eq. 3.4).

    jac_sparsity : {None, array_like, sparse matrix}, optional
        Defines the sparsity structure of the Jacobian matrix for finite
        difference estimation, its shape must be (m, n). If the Jacobian has
        only few non-zero elements in *each* row, providing the sparsity
        structure will greatly speed up the computations [Curtis]_. A zero
        entry means that a corresponding element in the Jacobian is identically
        zero. If provided, forces the use of '"
2104163,7923649998789751608,8,BigQuery," equalities are found.
        lastequality = None  # Always equal to diffs[equalities[-1]][1]
        pointer = 0  # Index of current position.
        pre_ins = False  # Is there an insertion operation before the last equality.
        pre_del = False  # Is there a deletion operation before the last equality.
        post_ins = False  # Is there an insertion operation after the last equality.
        post_del = False  # Is there a deletion operation after the last equality.
        while pointer < len(diffs):
            if diffs[pointer][0] == self.DIFF_EQUAL:  # Equality"
1801124,-6189496635602887560,12,BigQuery," %s cannot be referenced."" % to_field)

        model = self.model
        opts = model._meta
        add = object_id is None

        if add:
            if not self.has_add_permission(request):
                raise PermissionDenied
            obj = None

        else:
            obj = self.get_object(request, unquote(object_id), to_field)

            if not self.has_change_permission(request, obj):
                raise PermissionDenied

            if obj is None:
                raise Http404(_('%(name)s object with primary key %(key)r does not exist"
6272622,-133689987440027594,14,BigQuery,", required=True, store=True, type='char'),
        'parent_id': fields.many2one('res.company', 'Parent Company', select=True),
        'child_ids': fields.one2many('res.company', 'parent_id', 'Child Companies'),
        'partner_id': fields.many2one('res.partner', 'Partner', required=True),
        'rml_header': fields.text('RML Header', required=True),
        'rml_header1': fields.char('Company Tagline', help=""Appears by default on the top right corner of your printed documents (report header).""),
        "
1665264,-682501756196167910,37,BigQuery," it is desired for the tree view to update.
    # This is filed as Apple radar 6588391.
    xccl.SetBuildSetting(_intermediate_var,
                         '$(PROJECT_DERIVED_FILE_DIR)/$(CONFIGURATION)')
    xccl.SetBuildSetting(_shared_intermediate_var,
                         '$(SYMROOT)/DerivedSources/$(CONFIGURATION)')

    # Set user-specified project-wide build settings and config files.  This
    # is intended to be used very sparingly.  Really, almost everything should
    # go into target-specific build settings sections.  The"
3528587,-2411549713825036419,6,BigQuery,"CertificateAtIndex for index 0 to get the leaf.
        # 3. To get the CN, call SecCertificateCopyCommonName and process that
        #    string so that it's of the appropriate type.
        # 4. To get the SAN, we need to do something a bit more complex:
        #    a. Call SecCertificateCopyValues to get the data, requesting
        #       kSecOIDSubjectAltName.
        #    b. Mess about with this dictionary to try to get the SANs out.
        #
        # This is gross. Really gross. It's going to be a few hundred LoC extra
        # just to repeat something that SecureTrans"
1861677,8544921954245221804,11,BigQuery," 0x00e5, 0x00e6, 0x00e7,
  0x00e8, 0x00e9, 0x00ea, 0x00eb, 0x00ec, 0x00ed, 0x00ee, 0x00ef,
  0x00f0, 0x00f1, 0x00f2, 0x00f3, 0x00f4, 0x00f5, 0x00f6, 0x00f7,
  0x00f8, 0x00f9, 0x00fa, 0x00fb, 0x00fc, 0x00fd, 0x00fe"
2528285,-8032442378205058212,7,BigQuery,"level')
        self.assertRaises(psycopg2.InterfaceError,
            cnn.set_isolation_level, 0)
        self.assertRaises(psycopg2.InterfaceError,
            cnn.set_isolation_level, 1)


class ConnectionTwoPhaseTests(ConnectingTestCase):
    def setUp(self):
        ConnectingTestCase.setUp(self)

        self.make_test_table()
        self.clear_test_xacts()

    def tearDown(self):
        self.clear_test_xacts()
        ConnectingTestCase.tearDown(self)

"
1240697,-2833223736198499581,9,BigQuery,"0c0c0L, 0xfefefefeL,
    0x78787878L, 0xcdcdcdcdL, 0x5a5a5a5aL, 0xf4f4f4f4L,
    0x1f1f1f1fL, 0xddddddddL, 0xa8a8a8a8L, 0x33333333L,
    0x88888888L, 0x07070707L, 0xc7c7c7c7L, 0x31313131L,
    0xb1b1b1b1L, 0x12121212L,"
5878534,-1162601432236599783,28,BigQuery,")))

######################################################################
## 2. Example & DocTest
######################################################################
## - An ""example"" is a <source, want> pair, where ""source"" is a
##   fragment of source code, and ""want"" is the expected output for
##   ""source.""  The Example class also includes information about
##   where the example was extracted from.
##
## - A ""doctest"" is a collection of examples, typically extracted from
##   a string (such as an object's docstring).  The DocTest class also
##   includes information about where the string was extracted from.

class Example:
    """"""
    A single"
75900,-5657243753966764173,81,BigQuery,"
...   i56,  i57,  i58,  i59,  i60,  i61,  i62,  i63,  i64,  i65,  i66,
...   i67,  i68,  i69,  i70,  i71,  i72,  i73,  i74,  i75,  i76,  i77,
...   i78,  i79,  i80,  i81,  i82,  i83,  i84,  i85,  i86,  i87,  i88,
...   i89,  i90,  i"
2907961,-8056455328389786728,11,BigQuery,".addCallback(cbRequest)
        self.protocol.dataReceived(
            b""HTTP/1.1 200 OK\r\n""
            b""X-Foo: bar\r\n""
            b""X-Foo: baz\r\n""
            b""\r\n"")
        return d


    def test_receiveResponseBeforeRequestGenerationDone(self):
        """"""
        If response bytes are delivered to L{HTTP11ClientProtocol} before the
        L{Deferred} returned by L{Request.writeTo} fires, those response bytes
        are parsed as part of the response.

        The"
6469670,-2307331685312934819,10,BigQuery," a single array with scores.
        Default is f_classif (see below ""See also""). The default function only
        works with classification tasks.

    percentile : int, optional, default=10
        Percent of features to keep.

    Attributes
    ----------
    scores_ : array-like, shape=(n_features,)
        Scores of features.

    pvalues_ : array-like, shape=(n_features,)
        p-values of feature scores, None if `score_func` returned only scores.

    Examples
    --------
    >>> from sklearn.datasets import load_digits
    >>> from sklearn.feature_selection import Select"
1649997,-5428067914787006594,23,BigQuery,"az'),
        ('/test/test1/foo/', '', 'handle_foo'),
        ('/test/test2/bar/', '', 'handle_bar'),
        ('/test/test2/baz/', '', 'handle_baz'),
        ('/test/test2/foo/', '', 'handle_foo'),
        ('/test/test3/bar/', '', 'handle_bar'),
        ('/test/test3/baz/', '', 'handle_baz'),
        ('/test/test3/foo/', '', 'handle_foo'),
        ('/test/test4/bar/"
5848295,3129040976059084493,46,BigQuery,"

        r53_conn = route53.Route53Connection()
        all_zones = r53_conn.get_zones()

        route53_zones = [ zone for zone in all_zones if zone.name[:-1]
                          not in self.route53_excluded_zones ]

        self.route53_records = {}

        for zone in route53_zones:
            rrsets = r53_conn.get_all_rrsets(zone.id)

            for record_set in rrsets:
                record_name = record_set.name

                if record_name.endswith('.'"
2635228,-2782022167007456391,23,BigQuery," back
        readlifecycle = self.bucket.get_lifecycle_config();
        for rule in readlifecycle:
            if rule.id == ""1"":
                self.assertEqual(rule.prefix, ""1/"")
                self.assertEqual(rule.expiration.days, 1)
            elif rule.id == ""2"":
                self.assertEqual(rule.prefix, ""2/"")
                self.assertEqual(rule.expiration.days, 2)
            elif rule.id == ""3"":
                self.assertEqual(rule.prefix, ""3/"")
                self.assertEqual("
1498335,-4657690102489235281,14,BigQuery,"            \@ifundefined{cmdEpydoc@property@description}{}{
                \par\cmdEpydoc@property@description}
            \@ifundefined{cmdEpydoc@property@type}{}{
                \par\textbf{Type:} \cmdEpydoc@property@type}
            \@ifundefined{cmdEpydoc@property@fget}{}{
                \par\textbf{Get:} \cmdEpydoc@property@fget}
            \@ifundefined{cmdEpydoc@property@fset}{}{
                \par\textbf{Set:} \cmdEpydoc@property@f"
1726148,3047602070033854179,22,BigQuery," division
from __future__ import print_function

from tensorflow.python.framework import ops
from tensorflow.python.ops import array_ops
from tensorflow.python.ops import math_ops

__all__ = [""sparsemax_loss""]


def sparsemax_loss(logits, sparsemax, labels, name=None):
  """"""Computes sparsemax loss function [1].

  [1]: https://arxiv.org/abs/1602.02068

  Args:
    logits: A `Tensor`. Must be one of the following types: `half`, `float32`,
      `float64`.
    "
4363502,-5021544551623490080,15,BigQuery,"# Convert an XML-RPC packet to a Python object.  If the XML-RPC packet
# represents a fault condition, this function raises a Fault exception.
#
# @param data An XML-RPC packet, given as an 8-bit string.
# @return A tuple containing the unpacked data, and the method name
#     (None if not present).
# @see Fault

def loads(data, use_datetime=0):
    """"""data -> unmarshalled data, method name

    Convert an XML-RPC packet to unmarshalled data plus a method
    name (None if not present).

    If the XML-RPC packet represents"
2302519,-8322832073975745851,29,BigQuery,"  </textarea>"")

    def test_nested_inline_elements(self):
        """"""Inline elements can be nested indefinitely.""""""
        b_tag = ""<b>Inside a B tag</b>""
        self.assertSoupEquals(b_tag)

        nested_b_tag = ""<p>A <i>nested <b>tag</b></i></p>""
        self.assertSoupEquals(nested_b_tag)

        double_nested_b_tag = ""<p>A <a>doubly <i>nested <b>tag</b></i></a></p>""
        "
29386,8446632727743934464,6,BigQuery,"    for i in range(5):
        triv = PermutationGroup([Permutation(list(range(i)))])
        assert triv.is_trivial


def test_pointwise_stabilizer():
    S = SymmetricGroup(2)
    stab = S.pointwise_stabilizer([0])
    assert stab.generators == [Permutation(1)]
    S = SymmetricGroup(5)
    points = []
    stab = S
    for point in (2, 0, 3, 4, 1):
        stab = stab.stabilizer(point)
        points.append(point)
"
7394162,7452802854498267108,6,BigQuery,"run([merged, train_step],
                              feed_dict=feed_dict(True),
                              options=run_options,
                              run_metadata=run_metadata)
        train_writer.add_run_metadata(run_metadata,'step%03d' % i)
        train_writer.add_summary(summary, i)
        print('Adding run metadata for', i)
      else:  # Record a summary
        summary, _ = sess.run([merged, train_step], feed_dict=feed_dict(True))
        train_writer.add_summary(summary, i)
  train_writer.close()
  "
2665144,-4717521776186942361,16,BigQuery,"deg] = c
        c = cc

    # warn on rank reduction
    if rank!= order and not full:
        msg = ""The fit may be poorly conditioned""
        warnings.warn(msg, pu.RankWarning, stacklevel=2)

    if full:
        return c, [resids, rank, s, rcond]
    else:
        return c


def lagcompanion(c):
    """"""
    Return the companion matrix of c.

    The usual companion matrix of the Laguerre polynomials is already
    symmetric when `c` is a basis Laguerre polynomial, so no scaling is
"
378761,3775244072497119472,75,BigQuery,\ue4da\ue4db\ue4dc\ue4dd\ue4de\ue4df\ue4e0\ue4e1\ue4e2\ue4e3\ue4e4\ue4e5\ue4e6\ue4e7\ue4e8\ue4e9\ue4ea\ue4eb\ue4ec\ue4ed\ue4ee\ue4ef\ue4f0\ue4f1\ue4f2\ue4f3\ue4f4\ue4f5\ue4f6\ue4f7\ue4f8\ue4f9\ue
4665874,4911705373076823871,12,BigQuery,"2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_331', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_332', 'val_2', 'val_3', 'val_0',"
7167206,3867274773283060480,7,BigQuery,"ellipkinc(self):
        assert_equal(cephes.ellipkinc(0,0),0.0)

    def test_erf(self):
        assert_equal(cephes.erf(0),0.0)

    def test_erfc(self):
        assert_equal(cephes.erfc(0),1.0)

    def test_exp1(self):
        cephes.exp1(1)

    def test_expi(self):
        cephes.expi(1)

    def test_expn(self):
        cephes.expn(1"
166393,-3435505166240190801,19,BigQuery," version 3 of the License, or\n"")
    out_file.write("" * (at your option) any later version.\n"")
    out_file.write("" *\n"")
    out_file.write("" * This program is distributed in the hope that it will be useful,\n"")
    out_file.write("" * but WITHOUT ANY WARRANTY; without even the implied warranty of\n"")
    out_file.write("" * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n"")
    out_file.write("" * GNU General Public License for more details.\n"")
    out_file.write"
5772155,-1993318112452749926,17,BigQuery,"CommandStartedEvent`.

        :Parameters:
          - `event`: An instance of :class:`CommandStartedEvent`.
        """"""
        raise NotImplementedError

    def succeeded(self, event):
        """"""Abstract method to handle a `CommandSucceededEvent`.

        :Parameters:
          - `event`: An instance of :class:`CommandSucceededEvent`.
        """"""
        raise NotImplementedError

    def failed(self, event):
        """"""Abstract method to handle a `CommandFailedEvent`.

        :Parameters:
          - `event`: An instance of :class:`CommandFailedEvent`.
"
559931,-4974056170138718644,7,BigQuery,"': 1,
                   'address': '192.168.1.1',
                   'network_id': 1,
                   'virtual_interface_id': 1,
                   'instance_uuid': '1',
                   'allocated': False,
                   'leased': False,
                  'reserved': False,
                   'host': None,
                   'deleted': False},
                  {'id': 2,
                   'address': '192.168.1.2',
                   'network_id': 1,
                   'virtual_interface_id': 2,
                   'instance_uuid': '2',
                   'allocated': False,
                   'leased"
2999411,-7638765623107082714,18,BigQuery,"        return dict(failed=False, msg='successfully loaded file')

    def _get_network_os(self, task_vars):
        if 'network_os' in self._task.args and self._task.args['network_os']:
            display.vvvv('Getting network OS from task argument')
            network_os = self._task.args['network_os']
        elif self._play_context.network_os:
            display.vvvv('Getting network OS from inventory')
            network_os = self._play_context.network_os
        elif 'network_os' in task_vars.get('ansible_facts"
5756896,-899349704761954818,16,BigQuery,"1D,
	0x9B64C2B0, 0xEC63F226, 0x756AA39C, 0x026D930A,
	0x9C0906A9, 0xEB0E363F, 0x72076785, 0x05005713,
	0x95BF4A82, 0xE2B87A14, 0x7BB12BAE, 0x0CB61B38,
	0x92D28E9B, 0xE5D5BE0D, 0x7CDCEFB7, 0x0BDBDF21,
	0x86D3D2"
3180959,3223188690251432006,8,BigQuery,".pk.name)):
            kwargs['to_field'] = self.remote_field.field_name
        return name, path, args, kwargs

    def to_python(self, value):
        return self.target_field.to_python(value)

    @property
    def target_field(self):
        return self.foreign_related_fields[0]

    def get_reverse_path_info(self, filtered_relation=None):
        """"""Get path from the related model to this field's model.""""""
        opts = self.model._meta
        from_opts = self.remote_field.model._"
2149301,-6773296307287354335,7,BigQuery," matching.
  if options.platform:
    if options.auto_platform:
      parser.error('--platform can not be specified with --auto_platform')
    if not re.match(options.platform, GetNormalizedPlatform()):
      if options.verbose:
        print('The current platform doesn\'t match ""%s"", skipping.' %
              options.platform)
      return 0

  # Set the boto file to /dev/null if we don't need auth.
  if options.no_auth:
    if (set(('http_proxy', 'https_proxy')).intersection(
        env.lower() for env in os.en"
378761,-8420367074251112469,75,BigQuery,\u4a84\u4a85\u4a86\u4a87\u4a88\u4a89\u4a8a\u4a8b\u4a8c\u4a8d\u4a8e\u4a8f\u4a90\u4a91\u4a92\u4a93\u4a94\u4a95\u4a96\u4a97\u4a98\u4a99\u4a9a\u4a9b\u4a9c\u4a9d\u4a9e\u4a
2452802,397782137454615811,7,BigQuery,"settings('settings.py')

    def test_builtin_command(self):
        ""default: manage.py builtin commands succeed when default settings are appropriate""
        args = ['check', 'admin_scripts']
        out, err = self.run_manage(args)
        self.assertNoOutput(err)
        self.assertOutput(out, SYSTEM_CHECK_MSG)

    def test_builtin_with_settings(self):
        ""default: manage.py builtin commands succeed if settings are provided as argument""
        args = ['check', '--settings=test_project.settings', 'admin_scripts']
        out, err ="
6742764,3885413342054048962,14,BigQuery,"            raise F5ModuleError(str(ex))

        if 'code' in response and response['code'] == 400:
            if'message' in response:
                raise F5ModuleError(response['message'])
            else:
                raise F5ModuleError(resp.content)

    def exec_module(self):
        changed = False
        result = dict()
        state = self.want.state

        if state == ""present"":
            changed = self.present()
        elif state == ""absent"":
            changed = self.absent()

        reportable = ReportableChanges(params=self.changes.to_return())
        "
136658,-6331309470566264505,124,BigQuery,"else:
        dict = getstate()
    if dict:
        return _reconstructor, args, dict
    else:
        return _reconstructor, args

# Helper for __reduce_ex__ protocol 2

def __newobj__(cls, *args):
    return cls.__new__(cls, *args)

def _slotnames(cls):
    """"""Return a list of slot names for a given class.

    This needs to find slots defined by the class and its bases, so we
    can't simply return the __slots__ attribute.  We must walk down
    the Method Resolution Order and concaten"
1119347,-8546712286622715749,6,BigQuery,"data_ = 0
      self.data_ = """"

  def has_data(self): return self.has_data_

  def received_from(self):
    if self.received_from_ is None:
      self.lazy_init_lock_.acquire()
      try:
        if self.received_from_ is None: self.received_from_ = AddressPort()
      finally:
        self.lazy_init_lock_.release()
    return self.received_from_

  def mutable_received_from(self): self.has_received_from_ = 1; return self.received_from()

  def clear"
5544771,-4336575553946602422,9,BigQuery," range(level):
            outfile.write('    ')

def quote_xml(inStr):
    if not inStr:
        return ''
    s1 = (isinstance(inStr, basestring) and inStr or
          '%s' % inStr)
    s1 = s1.replace('&', '&amp;')
    s1 = s1.replace('<', '&lt;')
    s1 = s1.replace('>', '&gt;')
    return s1

def quote_attrib(inStr):
    s1 = (isinstance(inStr, basestring) and inStr"
2453726,-984004165843534039,49,BigQuery,"(me_times2,
...                          me_times3),
...                    me_times5):
...         yield i

Print as many of these as you like -- *this* implementation is memory-
efficient.

>>> m235 = LazyList(m235())
>>> for i in range(5):
...     print([m235[j] for j in range(15*i, 15*(i+1))])
[1, 2, 3, 4, 5, 6, 8, 9, 10, 12, 15, 16, 18, 20, 24]
[25, 27, 30, 32, 36, 40, 45, 48, 50,"
1786173,-3356518097793746677,14,BigQuery,"add_method('GetPacketType', 
                   'ns3::NetDevice::PacketType', 
                   [], 
                   is_const=True)
    ## packet-socket.h (module 'network'): uint32_t ns3::PacketSocketTag::GetSerializedSize() const [member function]
    cls.add_method('GetSerializedSize', 
                   'uint32_t', 
                   [], 
                   is_const=True, is_virtual=True)
    ## packet-socket.h (module 'network'): static ns3::TypeId ns3::PacketSocketTag::GetTypeId() [member"
2755633,-245264808607775175,7,BigQuery,"refix)


class test_gettempdir(TC):
    """"""Test gettempdir().""""""

    def test_directory_exists(self):
        # gettempdir returns a directory which exists

        dir = tempfile.gettempdir()
        self.assertTrue(os.path.isabs(dir) or dir == os.curdir,
                     ""%s is not an absolute path"" % dir)
        self.assertTrue(os.path.isdir(dir),
                     ""%s is not a directory"" % dir)

    def test_"
6878521,-1827787525474478027,10,BigQuery,"

class ContextmanagerAssertionMixin(object):
    TEST_EXCEPTION = RuntimeError(""test exception"")

    def assertInWithManagerInvariants(self, mock_manager):
        self.assertTrue(mock_manager.enter_called)
        self.assertFalse(mock_manager.exit_called)
        self.assertEqual(mock_manager.exit_args, None)

    def assertAfterWithManagerInvariants(self, mock_manager, exit_args):
        self.assertTrue(mock_manager.enter_called)
        self."
484103,-6947359531836762235,8,BigQuery,"买一量'
BID_VOLUME_2 = u'买二量'
BID_VOLUME_3 = u'买三量'
BID_VOLUME_4 = u'买四量'
BID_VOLUME_5 = u'买五量'
ASK_VOLUME_1 = u'卖一量'
ASK_VOLUME_2 = u'卖二量'
ASK_VOLUME_3 = u'卖三量'
ASK_VOLUME_4 = u'卖四量'
ASK_"
378761,-5533417892439375321,75,BigQuery,8b6\ud8b7\ud8b8\ud8b9\ud8ba\ud8bb\ud8bc\ud8bd\ud8be\ud8bf\ud8c0\ud8c1\ud8c2\ud8c3\ud8c4\ud8c5\ud8c6\ud8c7\ud8c8\ud8c9\ud8ca\ud8cb\ud8cc\ud8cd\ud8ce\ud8cf\ud8d0\ud8d1\ud8d2\ud8d3\ud8d4\ud8d5\ud8d
377975,-781587690899327946,8,BigQuery,"filename, linenum, 'whitespace/todo', 2,
                'Too many spaces before TODO')

        username = match.group(2)
        if not username:
          error(filename, linenum,'readability/todo', 2,
                'Missing username in TODO; it should look like '
                '""// TODO(my_username): Stuff.""')

        middle_whitespace = match.group(3)
        # Comparisons made explicit for correctness -- pylint: disable=g-explicit-bool-comparison
        if middle_whitespace!='' and middle_whitespace!= '':
          error"
378761,1679161550255333216,75,BigQuery,u668e\u668f\u6690\u6691\u6692\u6693\u6694\u6695\u6696\u6697\u6698\u6699\u669a\u669b\u669c\u669d\u669e\u669f\u66a0\u66a1\u66a2\u66a3\u66a4\u66a5\u66a6\u66a7\u66a8\u66a9\u66aa\u66ab\u66ac\u66ad\u66ae\u66af\u66b0
4228052,-5421229254502676694,80,BigQuery,"/24 + O(x**5)
    assert (x*ln(1 + x)).nseries(
        x, n=5) == x**2 - x**3/2 + x**4/3 + O(x**5)


def test_pow_0():
    assert (x**2).nseries(x, n=5) == x**2
    assert (1/x).nseries(x, n=5) == 1/x
    assert (1/x**2).nseries(x, n=5) == 1/x**2
    assert (x**Rational(2, 3)).nseries(x,"
4620588,-5639825660381760036,14,BigQuery,"
    ## mac-low.h (module 'wifi'): void ns3::MacLow::CreateBlockAckAgreement(ns3::MgtAddBaResponseHeader const * respHdr, ns3::Mac48Address originator, uint16_t startingSeq) [member function]
    cls.add_method('CreateBlockAckAgreement', 
                   'void', 
                   [param('ns3::MgtAddBaResponseHeader const *','respHdr'), param('ns3::Mac48Address', 'originator'), param('uint16_t','startingSeq')])
    ## mac-low.h (module 'wifi'"
7015031,-1582925607690634793,9,BigQuery,"pos(hass, device_reg, entity_reg):
    """"""Test we get the expected triggers from a cover.""""""
    platform = getattr(hass.components, f""test.{DOMAIN}"")
    platform.init()
    ent = platform.ENTITIES[1]

    config_entry = MockConfigEntry(domain=""test"", data={})
    config_entry.add_to_hass(hass)
    device_entry = device_reg.async_get_or_create(
        config_entry_id=config_entry.entry_id,
        connections={(device_registry.CONNECTION_NET"
4544233,-2014436047380433414,7,BigQuery,"        QtGui.QWidget.__init__(self, parent)
        self.ui = Ui_aboutDialog()
        self.ui.setupUi(self)
        self.parent = parent
        self.ui.labelVersion.setText('version'+ shared.softwareVersion)


class regenerateAddressesDialog(QtGui.QDialog):

    def __init__(self, parent):
        QtGui.QWidget.__init__(self, parent)
        self.ui = Ui_regenerateAddressesDialog()
        self.ui.setupUi(self)
        self.parent = parent
        "
5878581,-286975510156207825,34,BigQuery," p2)
        >>> s1.midpoint
        Point2D(2, 3/2)

        """"""
        return Point.midpoint(self.p1, self.p2)

    def distance(self, o):
        """"""
        Finds the shortest distance between a line segment and a point.

        Raises
        ======

        NotImplementedError is raised if o is not a Point

        Examples
        ========

        >>> from sympy import Point, Segment
        >>> p1, p2 = Point(0, 1), Point(3, 4)
        >>> s = Segment(p1, p2)
        >>> s."
3513970,1475351968522004029,10,BigQuery," 301, 302, 303, 307]:
                # Pick out the location header and basically start from the beginning
                # remembering first to strip the ETag header and decrement our 'depth'
                if redirections:
                    if 'location' not in response and response.status!= 300:
                        raise RedirectMissingLocation( _(""Redirected but the response is missing a Location: header.""), response, content)
                    # Fix-up relative redirects (which violate an RFC 2616 MUST)
                    if 'location' in response:
                        location = response['location']
                        (scheme, authority, path, query, fragment) = parse_uri(location)
                        if authority"
3878218,7158475420702603581,10,BigQuery,"parameter_positions: List of indices into args defining the arguments to
      differentiate against.
    args: A list of arguments to the function to be differentiated.

  Returns:
    args, possibly edited in-place.
  """"""
  s = set()
  for (i, t) in enumerate(args):
    if i in parameter_positions:
      tid = ops.tensor_id(t)
      if tid in s:
        args[i] = gen_array_ops.identity(args[i])
      else:
        s.add(tid)
  return args


def val_and_grad_"
5832199,-7420266823199064402,6,BigQuery,".utils.html import escape

from. import PostgreSQLTestCase
from.models import JSONModel, PostgreSQLModel

try:
    from django.contrib.postgres import forms
    from django.contrib.postgres.fields import JSONField
except ImportError:
    pass


class TestSaveLoad(PostgreSQLTestCase):
    def test_null(self):
        instance = JSONModel()
        instance.save()
        loaded = JSONModel.objects.get()
        self.assertIsNone(loaded.field)

    def test_empty_object(self):
        instance = JSONModel(field={})
        instance.save"
2467897,-4407028305017994780,6,BigQuery,":
            ap = a.ravel()
        else:
            ap = a
    else:
        if axis is None:
            ap = a.flatten()
        else:
            ap = a.copy()

    if axis is None:
        axis = 0

    Nx = ap.shape[axis]
    indices = q * (Nx - 1)

    # round fractional indices according to interpolation method
    if interpolation == 'lower':
        indices = floor(indices).astype(intp)
    elif interpolation == 'higher':
        indices = ceil(indices).astype(intp)"
3044509,-185530268837971166,12,BigQuery,"header is None:
            env['CONTENT_TYPE'] = self.headers.type
        else:
            env['CONTENT_TYPE'] = self.headers.typeheader

        length = self.headers.getheader('content-length')
        if length:
            env['CONTENT_LENGTH'] = length

        for h in self.headers.headers:
            k,v = h.split(':',1)
            k=k.replace('-','_').upper(); v=v.strip()
            if k in env:
                continue                    # skip content length, type,etc.
            if 'HTTP_'+k in env:
                "
5666746,-8261939029037278868,66,BigQuery,"TracedValue() [constructor]
    cls.add_constructor([])
    ## traced-value.h (module 'core'): ns3::TracedValue<unsigned int>::TracedValue(ns3::TracedValue<unsigned int> const & o) [copy constructor]
    cls.add_constructor([param('ns3::TracedValue< unsigned int > const &', 'o')])
    ## traced-value.h (module 'core'): ns3::TracedValue<unsigned int>::TracedValue(unsigned int const & v) [constructor]
    cls.add_constructor([param('unsigned int const &"
7213619,-6807036368727350472,31,BigQuery,"[1, 0],
       [1, 0],
       [0, 1],
       [0, 1],
       [1, 0],
       [1, 0],
       [0, 1],
       [0, 1],
       [1, 0],
       [1, 0],
       [0, 1],
       [0, 1]])

>>> d12 = dummy_product(d1, d2)
>>> d12
array([[1, 0, 0, 0, 0, 0],
       [1, 0, 0, 0, 0, 0],
       [0, 1, 0, 0, 0, 0],
"
2119607,-3831063438906763271,12,BigQuery,"TYPE = _descriptor.EnumDescriptor(
  name='BattleType',
  full_name='POGOProtos.Data.Battle.BattleType',
  filename=None,
  file=DESCRIPTOR,
  values=[
    _descriptor.EnumValueDescriptor(
      name='BATTLE_TYPE_UNSET', index=0, number=0,
      options=None,
      type=None),
    _descriptor.EnumValueDescriptor(
      name='BATTLE_TYPE_NORMAL', index=1, number=1,
      options=None,
      type=None),
"
5302965,-8377480949364570168,27,BigQuery,"):
                data = str(data)
            # If the file has an encoding, encode unicode with it.
            if (isinstance(fp, file) and
                    isinstance(data, unicode) and
                    fp.encoding is not None):
                errors = getattr(fp, ""errors"", None)
                if errors is None:
                    errors = ""strict""
                data = data.encode(fp.encoding, errors)
            fp.write(data)
        want_unicode = False
        sep = kwargs.pop(""sep"", None)
        if sep is not None:
            if isinstance(se"
6606711,5530095233023932526,18,BigQuery,"vereq(a * 1, base)
    veris((a / 1).__class__, complex)
    vereq(a / 1, base)

    class madtuple(tuple):
        _rev = None
        def rev(self):
            if self._rev is not None:
                return self._rev
            L = list(self)
            L.reverse()
            self._rev = self.__class__(L)
            return self._rev
    a = madtuple((1,2,3,4,5,6,7,8,9,0))
    vereq(a, (1,2,3,"
4228052,-1079098637240856996,80,BigQuery," test_pole():
    raises(PoleError, lambda: sin(1/x).series(x, 0, 5))
    raises(PoleError, lambda: sin(1 + 1/x).series(x, 0, 5))
    raises(PoleError, lambda: (x*sin(1/x)).series(x, 0, 5))


def test_expsinbug():
    assert exp(sin(x)).series(x, 0, 0) == O(1, x)
    assert exp(sin(x)).series(x, 0, 1) == 1 + O(x)
    assert exp(sin(x"
3121205,6334456430155772002,91,BigQuery,"                                  0.2*np.ones(20, 'd'), 11)


class TestConcatenator(TestCase):
    def test_1d(self):
        assert_array_equal(r_[1, 2, 3, 4, 5, 6], np.array([1, 2, 3, 4, 5, 6]))
        b = np.ones(5)
        c = r_[b, 0, 0, b]
        assert_array_equal(c, [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1])

    def test_mixed_type(self"
5105809,8272063105715563677,7,BigQuery,"0 = 1
        floc = 0
        a, loc, scale = stats.gamma.fit(x, f0=f0, floc=floc)
        assert_equal(a, f0)
        assert_equal(loc, floc)
        assert_almost_equal(scale, x.mean()/a, decimal=8)

        f0 = 2
        floc = 0
        a, loc, scale = stats.gamma.fit(x, f0=f0, floc=floc)
        assert_equal(a, f0)
        assert_equal(loc, floc)
        assert_almost_"
3604698,3408344451277477243,8,BigQuery,"month
                        month += 1
                        if month == 13:
                            month = 1
                            year += 1
                            if year > datetime.MAXYEAR:
                                self._len = total
                                return
                        daysinmonth = calendar.monthrange(year, month)[1]
                    ii.rebuild(year, month)

    def __construct_byset(self, start, byxxx, base):
        """"""
        If a `BYXXX` sequence is passed to the constructor at the same level as
        `FREQ` (e.g. `FREQ=HOURLY,BYHOUR={2,4,7},INTERVAL=3`),"
6348747,2670880168516435201,57,BigQuery,"_warn()
        with self.assertWarns(RuntimeWarning):
            warnings.warn(""foo"", category=RuntimeWarning)
        # Failure when no warning is triggered
        with self.assertRaises(self.failureException):
            with self.assertWarns(RuntimeWarning):
                pass
        # Failure when another warning is triggered
        with warnings.catch_warnings():
            # Force default filter (in case tests are run with -We)
            warnings.simplefilter(""default"", RuntimeWarning)
            with self.assertRaises(self.failureException):
                with self.assertWarns(DeprecationWarning):
                    "
378761,-7669714791826555037,75,BigQuery,42d9\u42da\u42db\u42dc\u42dd\u42de\u42df\u42e0\u42e1\u42e2\u42e3\u42e4\u42e5\u42e6\u42e7\u42e8\u42e9\u42ea\u42eb\u42ec\u42ed\u42ee\u42ef\u42f0\u42f1\u42f2\u42f3\u42f4\u42f5\u42f6\u42f7\u42f8\u42f
2892958,6391224889523817601,15,BigQuery,"ctools.wraps(fn)
    def unique(*args, **kw):
        seen = set()
        for item in fn(*args, **kw):
            if item not in seen:
                seen.add(item)
                yield item
    return unique

# TODO: this goes somewhere besides the wheel module
@_unique
def uninstallation_paths(dist):
    """"""
    Yield all the uninstallation paths for dist based on RECORD-without-.pyc

    Yield paths to all the files in RECORD. For each.py file in RECORD, add
    the.pyc in the same directory.

    UninstallPath"
377975,-3240111695585702722,8,BigQuery,"# Check that access keywords are indented +1 space.  Skip this
        # check if the keywords are not preceded by whitespaces.
        indent = access_match.group(1)
        if (len(indent)!= classinfo.class_indent + 1 and
            Match(r'^\s*$', indent)):
          if classinfo.is_struct:
            parent ='struct'+ classinfo.name
          else:
            parent = 'class'+ classinfo.name
          slots = ''
          if access_match.group(3):
            slots = access_match.group(3)
          error(filename"
5787245,5164138857133296211,16,BigQuery,"def read_windows_registry(self, strict=True):
        """"""
        Load the MIME types database from Windows registry.

        If strict is true, information will be added to
        list of standard types, else to the list of non-standard
        types.
        """"""

        # Windows only
        if not _winreg:
            return

        def enum_types(mimedb):
            i = 0
            while True:
                try:
                    ctype = _winreg.EnumKey(mimedb, i)
                except EnvironmentError:
                    break
                else:
                    if '\0' not in ctype:"
5136130,3515068066284271767,7,BigQuery,"False), param('ns3::CallbackBase const &', 'cb')], 
                   is_pure_virtual=True, is_const=True, is_virtual=True)
    return

def register_Ns3TriangularRandomVariable_methods(root_module, cls):
    ## random-variable-stream.h (module 'core'): static ns3::TypeId ns3::TriangularRandomVariable::GetTypeId() [member function]
    cls.add_method('GetTypeId', 
                   'ns3::TypeId', 
                   [], 
                   is_static=True)
    ## random-variable-stream.h ("
6606711,-7902370467984249229,18,BigQuery,"pass
        else:
            raise TestFailed, ""should fail: cPickle D instance - %s"" % base
        # Give C a nice generic __getstate__ and __setstate__
        class C(base):
            __slots__ = ['a']
            def __getstate__(self):
                try:
                    d = self.__dict__.copy()
                except AttributeError:
                    d = {}
                for cls in self.__class__.__mro__:
                    for sn in cls.__dict__.get('__slots__', ()):
                        try:
                            d[sn] = getattr"
5907801,-8133033833159561416,6,BigQuery,"""""
    if regularizer is None:
      regularizer = self._regularizer
    if caching_device is None:
      caching_device = self._caching_device
    if partitioner is None:
      partitioner = self._partitioner
    if custom_getter is None:
      custom_getter = self._custom_getter

    full_name = self.name + ""/"" + name if self.name else name
    # Variable names only depend on variable_scope (full_name here),
    # not name_scope, so we reset it below for the time of variable creation.
    with ops.name_scope(None):"
4848498,-8718749207460293146,26,BigQuery," 'close'):
            self.file.close()

    def __iter__(self):
        return self

    def next(self):
        data = self.file.read(self.buffer_size)
        if data:
            return data
        raise StopIteration()


def make_line_iter(stream, limit=None, buffer_size=10 * 1024):
    """"""Safely iterates line-based over an input stream.  If the input stream
    is not a :class:`LimitedStream` the `limit` parameter is mandatory.

    This uses the stream's :meth:`~file.read` method internally as opposite
    "
727263,6094810353936396034,82,BigQuery,"cherrypy.org/browser."")
_d.ignore_headers = Tool('before_request_body', cptools.ignore_headers)
_d.referer = Tool('before_request_body', cptools.referer)
_d.basic_auth = Tool('on_start_resource', auth.basic_auth)
_d.digest_auth = Tool('on_start_resource', auth.digest_auth)
_d.trailing_slash = Tool('before_handler', cptools.trailing_slash, priority=60)
_d.flatten = Tool('before_finalize', cptools.flatten"
1301620,7147487801539798933,11,BigQuery,": 'up',
        2: 'down',
        3: 'testing'
        }
    if int_adminstatus in adminstatus_options:
        return adminstatus_options[int_adminstatus]
    else:
        return """"

def lookup_operstatus(int_operstatus):
    operstatus_options = {
        1: 'up',
        2: 'down',
        3: 'testing',
        4: 'unknown',
        5: 'dormant',
        6: 'notPresent',
        7: 'lowerLayerDown'
        }
    if int_operstatus in operstatus_options:
        return oper"
5075938,-1940959918935967109,18,BigQuery,"org/TR/2003/REC-PNG-20031110/#9Filters.""
                  % self.filter)
            if self.interlace not in (0,1):
                raise FormatError(""Unknown interlace method %d,""
                  "" see http://www.w3.org/TR/2003/REC-PNG-20031110/#8InterlaceMethods.""
                  % self.interlace)

            # Derived values
            # http://www.w3.org/TR/PNG/#6Colour-values
            colormap =  bool(self.color_type & 1)
            greyscale = not (self.color_type"
4832721,234739587153634025,7,BigQuery,"'max_length': '255', 'blank': 'True'}),
            'phonenumber': ('django.db.models.fields.CharField', [], {'max_length': '30', 'blank': 'True'}),
           'require_auth': ('django.db.models.fields.BooleanField', [], {'default': 'False'}),
            'twitter': ('django.db.models.fields.CharField', [], {'max_length': '255', 'blank': 'True'}),
            'user': ('django.db.models.fields.related.OneToOneField', [], {"
696906,-2948058263729768269,16,BigQuery,"ged_mode = existing_modes.get(posted_mode.mode_slug, CourseMode())

            merged_mode.course_id = self.id
            merged_mode.mode_slug = posted_mode.mode_slug
            merged_mode.mode_display_name = posted_mode.mode_slug
            merged_mode.min_price = posted_mode.min_price
            merged_mode.currency = posted_mode.currency
            merged_mode.sku = posted_mode.sku
            merged_mode.expiration_datetime = posted_mode.expiration_datetime
            "
6029997,2740690643608706150,9,BigQuery,"'b'    : '&B',
    'c'    : '&C',    'd'    : '&D',    'e'    : '&E',    'f'    : '&F',
    'g'    : '&G',    'h'    : '&H',    'i'    : '&I',    'j'    : '&J',
    'k'    : '&K',    'l'    : '&L',   'm'    : '&M',    'n'    : '&N',
    'o'    : '&O',    'p'    : '&P"
7076043,-2968301198963383746,9,BigQuery,"OL_C_EXIT

# winuser.h line 8594
# constants used with SystemParametersInfo
SPI_GETBEEP = 1
SPI_SETBEEP = 2
SPI_GETMOUSE = 3
SPI_SETMOUSE = 4
SPI_GETBORDER = 5
SPI_SETBORDER = 6
SPI_GETKEYBOARDSPEED = 10
SPI_SETKEYBOARDSPEED = 11
SPI_LANGDRIVER = 12
SPI_ICONHORIZONTALSPACING = 13
SPI_"
1119851,-1258585534706219314,9,BigQuery,".""

  def __init__(self, size, bracket, alignment='l'):
    ""Set the size and symbol for the bracket.""
    self.size = size
    self.original = bracket
    self.alignment = alignment
    self.pieces = None
    if bracket in FormulaConfig.bigbrackets:
      self.pieces = FormulaConfig.bigbrackets[bracket]

  def getpiece(self, index):
    ""Return the nth piece for the bracket.""
    function = getattr(self, 'getpiece' + unicode(len(self.pieces)))
    return function(index)

  def getpiece1(self, index"
378761,7658053463932734483,75,BigQuery,u6d87\u6d88\u6d89\u6d8a\u6d8b\u6d8c\u6d8d\u6d8e\u6d8f\u6d90\u6d91\u6d92\u6d93\u6d94\u6d95\u6d96\u6d97\u6d98\u6d99\u6d9a\u6d9b\u6d9c\u6d9d\u6d9e\u6d9f\u6da0\u6da1\u6da
3361989,-4045129373027304119,6,BigQuery,"moves."" + attr.name] = attr
del attr

_MovedItems._moved_attributes = _moved_attributes

moves = sys.modules[__name__ + "".moves""] = _MovedItems(__name__ + "".moves"")


class Module_six_moves_urllib_parse(_LazyModule):
    """"""Lazy loading of moved objects in six.moves.urllib_parse""""""


_urllib_parse_moved_attributes = [
    MovedAttribute(""ParseResult"", ""urlparse"", ""urllib.parse""),
    MovedAttribute("""
2939015,-4565659608654632097,36,BigQuery,"5ModuleError(
                ""Failed to remove EULA file from device.""
            )
        result = self.reload_license()
        if not result:
            raise F5ModuleError(
                ""Failed to reload the empty license configuration.""
            )

    def remove_license_from_device(self):
        params = dict(
            command='run',
            utilCmdArgs='/config/bigip.license'
        )
        uri = ""https://{0}:{1}/mgmt/tm/util/bash"".format(
            self.client.provider['server'],
            self.client.provider['server_"
7167206,-2432974961813791997,7,BigQuery,"

    def test_ellipkinc(self):
        assert_equal(cephes.ellipkinc(0,0),0.0)

    def test_erf(self):
        assert_equal(cephes.erf(0),0.0)

    def test_erfc(self):
        assert_equal(cephes.erfc(0),1.0)

    def test_exp1(self):
        cephes.exp1(1)

    def test_expi(self):
        cephes.expi(1)

    def test_expn(self):
        cephes"
1179888,2625159537942801010,7,BigQuery,"(FilePathField, self).formfield(**defaults)

    def get_internal_type(self):
        return ""FilePathField""

class FloatField(Field):
    empty_strings_allowed = False
    default_error_messages = {
        'invalid': _(""This value must be a float.""),
    }
    description = _(""Floating point number"")

    def get_prep_value(self, value):
        if value is None:
            return None
        return float(value)

    def get_internal_type(self):
        return ""FloatField""

    def to_python(self, value):"
4575500,-9054733045635605973,50,BigQuery,"    name
      identifying name for the dialect from a DBAPI-neutral point of view
      (i.e.'sqlite')

    driver
      identifying name for the dialect's DBAPI

    positional
      True if the paramstyle for this Dialect is positional.

    paramstyle
      the paramstyle to be used (some DB-APIs support multiple
      paramstyles).

    convert_unicode
      True if Unicode conversion should be applied to all ``str``
      types.

    encoding
      type of encoding to use for unicode, usually defaults to
      'utf-8'.

    statement_compiler
      a :class:`."
7061433,-7517748313291100998,47,BigQuery,"'t match.""),
    }
    new_password1 = forms.CharField(label=_(""New password""),
                                    widget=forms.PasswordInput)
    new_password2 = forms.CharField(label=_(""New password confirmation""),
                                    widget=forms.PasswordInput)

    def __init__(self, user, *args, **kwargs):
        self.user = user
        super(SetPasswordForm, self).__init__(*args, **kwargs)

    def clean_new_password2(self):
        password1 = self.cleaned_data.get('new_password1')
        password2 = self.cle"
2862961,-7312461390696686968,38,BigQuery,"32_t', 'addressIndex')], 
                   is_pure_virtual=True, is_virtual=True)
    ## ipv4.h (module 'internet'): bool ns3::Ipv4::RemoveAddress(uint32_t interface, ns3::Ipv4Address address) [member function]
    cls.add_method('RemoveAddress', 
                   'bool', 
                   [param('uint32_t', 'interface'), param('ns3::Ipv4Address', 'address')], 
                   is_pure_virtual=True, is_virtual=True)
    ## ipv4.h (module 'internet'):"
5862384,3121249258484602273,6,BigQuery,"Null)
        self.assertIsInstance(a_isnull.lhs, Col)
        self.assertIsNone(a_isnull.lhs.alias)
        self.assertEqual(a_isnull.lhs.target, ObjectC._meta.get_field('objecta'))
        b_isnull = where.children[1]
        self.assertIsInstance(b_isnull, RelatedIsNull)
        self.assertIsInstance(b_isnull.lhs, Col)
        self.assertIsNone(b_isnull.lhs.alias)
        self.assertEqual(b_isnull.lhs"
5878534,-397338050947651099,28,BigQuery," traced code is executed.
    """"""
    def __init__(self, out):
        self.__out = out
        self.__debugger_used = False
        pdb.Pdb.__init__(self)

    def set_trace(self):
        self.__debugger_used = True
        pdb.Pdb.set_trace(self)

    def set_continue(self):
        # Calling set_continue unconditionally would break unit test coverage
        # reporting, as Bdb.set_continue calls sys.settrace(None).
        if self.__debugger_used:
            pdb.Pdb.set_"
6591691,-5064065396941774870,45,BigQuery," open(p, ""w"")
        f.write(auto_gen_msg)
        f.write('#include ""asdl.h""\n\n')
        c = ChainOfVisitors(TypeDefVisitor(f),
                            StructVisitor(f),
                            PrototypeVisitor(f),
                            )
        c.visit(mod)
        f.write(""PyObject* PyAST_mod2obj(mod_ty t);\n"")
        f.write(""mod_ty PyAST_obj2mod(PyObject* ast, PyArena* arena, int mode);\n"")
        f.write(""int PyAST_"
4560007,1064110971697534541,17,BigQuery," import configparser


class RegistrationBase(object):
    def __init__(self, module, username=None, password=None):
        self.module = module
        self.username = username
        self.password = password

    def configure(self):
        raise NotImplementedError(""Must be implemented by a sub-class"")

    def enable(self):
        # Remove any existing redhat.repo
        redhat_repo = '/etc/yum.repos.d/redhat.repo'
        if os.path.isfile(redhat_repo):
            os.unlink(redhat_repo)

    "
7303637,6858059706469851467,10,BigQuery,"""""""A Job object.

  Fields:
    configuration: [Required] Describes the job configuration.
    etag: [Output-only] A hash of this resource.
    id: [Output-only] Opaque ID field of the job
    jobReference: [Optional] Reference describing the unique-per-user name of
      the job.
    kind: [Output-only] The type of the resource.
    selfLink: [Output-only] A URL that can be used to access this resource
      again.
    statistics: [Output-only] Information about the job, including starting
      time and ending time of the job.
    status: ["
6637234,-3625713080467137525,40,BigQuery,"encoding)

    def __unicode__(self):
        return self.decode()

    def __str__(self):
        if PY3K:
            return self.decode()
        else:
            return self.encode()

    if PY3K:
        __str__ = __repr__ = __unicode__

    def encode(self, encoding=DEFAULT_OUTPUT_ENCODING,
               indent_level=None, formatter=""minimal"",
               errors=""xmlcharrefreplace""):
        # Turn the data structure into Unicode, then encode the
        # Unicode.
        u = self."
1786173,-2701112760659069778,14,BigQuery,"member function]
    cls.add_method('End', 
                  'std::_List_const_iterator< ns3::Ptr< ns3::PbbAddressTlv > >', 
                   [], 
                   is_const=True)
    ## packetbb.h (module 'network'): std::_List_iterator<ns3::Ptr<ns3::PbbAddressTlv> > ns3::PbbAddressTlvBlock::Erase(std::_List_iterator<ns3::Ptr<ns3::PbbAddressTlv> > position) [member function]
    cls.add_method('Erase', 
                  'std"
6363119,2469171660084122521,8,BigQuery,"pass

        test = TestableTest('testNothing')

        # This shouldn't blow up
        deepcopy(test)

    def testPickle(self):
        # Issue 10326

        # Can't use TestCase classes defined in Test class as
        # pickle does not work with inner classes
        test = unittest.TestCase('run')
        for protocol in range(pickle.HIGHEST_PROTOCOL + 1):

            # blew up prior to fix
            pickled_test = pickle.dumps(test, protocol=protocol)
            unpickled_test = pickle.loads(pickled_test)
            self"
575297,4780444904928822317,9,BigQuery,"ine,'multi-comment'),
             'root'),
            (r'@import', Keyword, 'import'),
            (r'@for', Keyword, 'for'),
            (r'@(debug|warn|if|while)', Keyword, 'value'),
            (r'(@mixin)( [\w-]+)', bygroups(Keyword, Name.Function), 'value'),
            (r'(@include)( [\w-]+)', bygroups(Keyword, Name.Decorator), 'value'),
            (r'@extend', Keyword,'selector'),
            (r'@[a-z0"
4620588,-171396231696716595,14,BigQuery," wifi-remote-station-manager.h (module 'wifi'): void ns3::WifiRemoteStationManager::Reset() [member function]
    cls.add_method('Reset', 
                   'void', 
                   [])
    ## wifi-remote-station-manager.h (module 'wifi'): void ns3::WifiRemoteStationManager::Reset(ns3::Mac48Address address) [member function]
    cls.add_method('Reset', 
                   'void', 
                   [param('ns3::Mac48Address', 'address')])
    ## wifi-remote-station-manager.h (module '"
5878425,-1794869253081338371,19,BigQuery,"(score_macro, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])
    assert_almost_equal(score_samples, [1, 1 / 2, 3 / 4, 1 / 2, 1 / 4])


def test_cross_val_predict():
    boston = load_boston()
    X, y = boston.data, boston.target
    cv = KFold()

    est = Ridge()

    # Naive loop (should be same as cross_val_predict):
    preds2 = np.zeros_like(y)
    for train, test in cv"
1756274,-7597184461521449035,12,BigQuery,"

    self._screen_init()
    self._screen_refresh_size()
    # TODO(cais): Error out if the size of the screen is too small.

    # Initialize some UI component size and locations.
    self._init_layout()

    self._command_history_store = debugger_cli_common.CommandHistory()

    # Active list of command history, used in history navigation.
    # _command_handler_registry holds all the history commands the CLI has
    # received, up to a size limit. _active_command_history is the history
    # currently being navigated in, e.g., using the Up/Down keys"
377975,6419239644160236626,8,BigQuery," in both Match and Search for
  # performance reasons; factoring it out into a separate function turns out
  # to be noticeably expensive.
  if pattern not in _regexp_compile_cache:
    _regexp_compile_cache[pattern] = sre_compile.compile(pattern)
  return _regexp_compile_cache[pattern].match(s)


def ReplaceAll(pattern, rep, s):
  """"""Replaces instances of pattern in a string with a replacement.

  The compiled regex is kept in a cache shared by Match and Search.

  Args:
    pattern: regex pattern
    rep: replacement text"
6637234,-8145464466573818085,40,BigQuery," ""%s""' % token
            recursive_candidate_generator = None
            tag_name = None

            # Each operation corresponds to a checker function, a rule
            # for determining whether a candidate matches the
            # selector. Candidates are generated by the active
            # iterator.
            checker = None

            m = self.attribselect_re.match(token)
            if m is not None:
                # Attribute selector
                tag_name, attribute, operator, value = m.groups()
                checker = self._attribute_checker(operator, attribute, value)

            elif '#' in token:
                # ID selector"
3802102,-2231580888038963449,6,BigQuery,"ValueValuesEnum', 2)
  predefinedDefaultObjectAcl = _messages.EnumField('PredefinedDefaultObjectAclValueValuesEnum', 3)
  project = _messages.StringField(4, required=True)
  projection = _messages.EnumField('ProjectionValueValuesEnum', 5)
  userProject = _messages.StringField(6)


class StorageBucketsListRequest(_messages.Message):
  r""""""A StorageBucketsListRequest object.

  Enums:
    ProjectionValueValuesEnum: Set of properties to return. Defaults to noAcl.

  Fields:"
6485237,-6868448303959068754,21,BigQuery,"ibleFilterError(""|skipped expects a dictionary"")
    skipped = item.get('skipped', False)
    return skipped


@environmentfilter
def do_groupby(environment, value, attribute):
    """"""Overridden groupby filter for jinja2, to address an issue with
    jinja2>=2.9.0,<2.9.5 where a namedtuple was returned which
    has repr that prevents ansible.template.safe_eval.safe_eval from being
    able to parse and eval the data.

    jinja2<2.9.0,>=2.9.5 is not affected, as <2."
1786173,4257606271641966423,14,BigQuery,"::RemoveTrailer(ns3::Trailer & trailer) [member function]
    cls.add_method('RemoveTrailer', 
                   'uint32_t', 
                   [param('ns3::Trailer &', 'trailer')])
    ## packet.h (module 'network'): bool ns3::Packet::ReplacePacketTag(ns3::Tag & tag) [member function]
    cls.add_method('ReplacePacketTag', 
                   'bool', 
                   [param('ns3::Tag &', 'tag')])
    ## packet.h (module 'network'): uint32_t ns3::P"
6545018,6994871073624498008,6,BigQuery,"str = self.netconf_get_config(module=module, conf_str=conf_str)

        result = list()

        if ""<data/>"" in xml_str:
            return result
        else:
            re_find = re.findall(
                r'.*<peerAddr>(.*)</peerAddr>.*\s.*<remoteAs>(.*)</remoteAs>.*', xml_str)

            if re_find:
                return re_find
            else:
                return result

    def get_bgp_del_peer(self, **kwargs):
        """""" get_bgp_del_peer """"""

        module ="
2271380,2841250181883675739,9,BigQuery,"or_requirement, resource_name):
        """"""Is the named resource an existing directory?""""""
        return get_provider(package_or_requirement).resource_isdir(
            resource_name
        )

    def resource_filename(self, package_or_requirement, resource_name):
        """"""Return a true filesystem path for specified resource""""""
        return get_provider(package_or_requirement).get_resource_filename(
            self, resource_name
        )

    def resource_stream(self, package_or_requirement, resource_name):
        """"""Return a readable file-like object for specified resource""""""
        return get"
2863065,467746515235088485,73,BigQuery,"self.tzinfo = tzinfo

    def __repr__(self):
        return ""{}({}, {}, {})"".format(
            self.__class__.__name__, self.lookup, self.lookup_type, self.tzinfo)

    def get_source_expressions(self):
        return [self.col]

    def set_source_expressions(self, exprs):
        self.col, = exprs

    def resolve_expression(self, query=None, allow_joins=True, reuse=None, summarize=False, for_save=False):
        copy = self.copy()
        copy"
378761,-1854592548656523070,75,BigQuery,u827e\u827f\u8280\u8281\u8282\u8283\u8284\u8285\u8286\u8287\u8288\u8289\u828a\u828b\u828c\u828d\u828e\u828f\u8290\u8291\u8292\u8293\u8294\u8295\u8296\u8297\u8298\u8299\u829a\u829b\u829c\u829d\u829e\u829f\
7259029,-2821324828650511967,30,BigQuery," p terminates.
                    p_terminates = 1

                if p_terminates:
                    # symbol n terminates!
                    if not Terminates[n]:
                        Terminates[n] = 1
                        some_change = 1
                    # Don't need to consider any more productions for this n.
                    break

        if not some_change:
            break

    some_error = 0
    for (s,terminates) in Terminates.items():
        if not terminates:
            if not Prodnames.has_key(s) and not Terminals.has_key(s) and s!= 'error':
                # s is used-but"
1786173,-5880342305053133234,14,BigQuery,"s.add_method('MessageClear', 
                   'void', 
                   [])
    ## packetbb.h (module 'network'): bool ns3::PbbPacket::MessageEmpty() const [member function]
    cls.add_method('MessageEmpty', 
                   'bool', 
                   [], 
                   is_const=True)
    ## packetbb.h (module 'network'): std::_List_iterator<ns3::Ptr<ns3::PbbMessage> > ns3::PbbPacket::MessageEnd() [member function]
    cls.add_method('MessageEnd', 
                  'std::_List"
15270,4165193602353531008,85,BigQuery,".rststyle('strong')})
                label = self.find_title_label(node, docutils.nodes.title,
                    'abstract')
                el1.text = label
            elif 'dedication' in node.attributes['classes']:
                el = self.append_p('horizontalline')
                el = self.append_p('centeredtextbody')
                el1 = SubElement(el, 'text:span',
                    attrib={'text:style-name': self.rststyle('strong')})
                label = self.find_title_label(node, docutils.nodes.title,
                    "
4802511,-4664676305141848059,8,BigQuery,"scaler>`.

    Parameters
    ----------
    X : {array-like, sparse matrix}
        The data to center and scale.

    axis : int (0 by default)
        axis used to compute the means and standard deviations along. If 0,
        independently standardize each feature, otherwise (if 1) standardize
        each sample.

    with_mean : boolean, True by default
        If True, center the data before scaling.

    with_std : boolean, True by default
        If True, scale the data to unit variance (or equivalently,
        unit standard deviation).

    copy : boolean, optional, default True
        set to False to"
378761,6607852075568788869,75,BigQuery,ud993\ud994\ud995\ud996\ud997\ud998\ud999\ud99a\ud99b\ud99c\ud99d\ud99e\ud99f\ud9a0\ud9a1\ud9a2\ud9a3\ud9a4\ud9a5\ud9a6\ud9a7\ud9a8\ud9a9\ud9aa\ud9ab\ud9ac\ud9ad\ud9ae\ud9af\ud9b0\ud9b1\ud9b2\ud9b3\ud9b4\ud9b5\ud
2756072,185250077980506254,13,BigQuery,")[1] for path in sys.path
                        if path and os.path.isdir(path)]:
            self.assertIn(entry, dir_set,
                          ""%s from sys.path not found in set returned ""
                          ""by _init_pathinfo(): %s"" % (entry, dir_set))

    def pth_file_tests(self, pth_file):
        """"""Contain common code for testing results of reading a.pth file""""""
        self.assertIn(pth_file.imported, sys.modules,
                      ""%s not in sys.modules"" % pth_file.imported)
        self"
3121205,-2612164863831600887,91,BigQuery,"
        bool_a = [True, False, True, True]
        int_a, = np.nonzero(bool_a)
        assert_equal(np.ix_(bool_a)[0], int_a)

    def test_1d_only(self):
        idx2d = [[1, 2, 3], [4, 5, 6]]
        assert_raises(ValueError, np.ix_, idx2d)

    def test_repeated_input(self):
        length_of_vector = 5
        x = np.arange(length_of_vector)
        out = ix_(x, x"
2968552,-3161830045680650557,10,BigQuery," os.X_OK):
        msg = (_LE(""Volume is not writable. Please broaden the file""
                   "" permissions. Mount: %s"") % mnt_base)
        raise nova_exception.NovaException(msg)


class LibvirtQuobyteVolumeDriver(fs.LibvirtBaseFileSystemVolumeDriver):
    """"""Class implements libvirt part of volume driver for Quobyte.""""""

    def _get_mount_point_base(self):
        return CONF.libvirt.quobyte_mount_point_base

    def get_config(self, connection_info, disk_info):
        conf = super(Libvirt"
2134865,1344103785651963605,22,BigQuery,"is found on the page.
        """"""
        self.wait_until(
            lambda driver: driver.find_element_by_tag_name(tag_name),
            timeout
        )

    def admin_login(self, username, password, login_url='/admin/'):
        """"""
        Helper function to log into the admin.
        """"""
        self.selenium.get('%s%s' % (self.live_server_url, login_url))
        username_input = self.selenium.find_element_by_name('username')
        username_input.send_keys(username)
        password_input = self"
5590233,-5609175790968346134,9,BigQuery,"
            blas_info = get_info('blas')
            #blas_info = {} ## uncomment for testing
            if blas_info:
                dict_append(info, **blas_info)
            else:
                warnings.warn(BlasNotFoundError.__doc__)
                blas_src_info = get_info('blas_src')
                if not blas_src_info:
                    warnings.warn(BlasSrcNotFoundError.__doc__)
                    return
                dict_append(info, libraries=[('fblas_src', blas_src_info)])

        self"
2680064,-9177604046142354171,8,BigQuery," '100',
       'miglog-affinity': 'test_value_101',
       'miglogd-children': '102',
       'multi-factor-authentication': 'optional',
       'multicast-forward': 'enable',
        'ndp-max-entry': '105',
        'per-user-bwl': 'enable',
        'policy-auth-concurrent': '107',
        'post-login-banner': 'disable',
        'pre-login-banner': 'enable',
        'private-data-encryption': 'disable',
        'proxy-auth-lifetime': 'enable',
        "
1179863,43982611636351616,7,BigQuery,"
    (1, TType.STRING, 'type', None, None, ), # 1
    (2, TType.STRING, 'name', None, None, ), # 2
  )

  def __init__(self, type=None, name=None,):
    self.type = type
    self.name = name

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary."
378761,-7710425467997117931,75,BigQuery,21\u9722\u9723\u9724\u9725\u9726\u9727\u9728\u9729\u972a\u972b\u972c\u972d\u972e\u972f\u9730\u9731\u9732\u9733\u9734\u9735\u9736\u9737\u9738\u9739\u973a\u973b\u973c\u973d\u973e\u973f\u9740\u9741\u9742\u9743\
378761,1655344196058549547,75,BigQuery,4\u9da5\u9da6\u9da7\u9da8\u9da9\u9daa\u9dab\u9dac\u9dad\u9dae\u9daf\u9db0\u9db1\u9db2\u9db3\u9db4\u9db5\u9db6\u9db7\u9db8\u9db9\u9dba\u9dbb\u9dbc\u9dbd\u9dbe\u9dbf\u9dc0\u9dc1\u9dc2
1195095,-7435113881167900683,8,BigQuery,"('name', Unicode),
                        column('timestamp', DateTime)
                    )

            for id, name, timestamp in connection.execute(stmt):
                print(id, name, timestamp)

        As a shortcut to the above syntax, keyword arguments referring to
        types alone may be used, if only type conversion is needed::

            stmt = text(""SELECT id, name, timestamp FROM some_table"")
            stmt = stmt.columns(
                        id=Integer,
                        name=Unicode,
                        timestamp=DateTime
                    )

            for id, name, timestamp in connection.execute(stmt):
                print(id, name,"
1588672,6664469130185353686,7,BigQuery,"        # Adding unique constraint on 'ProductRecommendation', fields ['primary','recommendation']
        db.create_unique(u'catalogue_productrecommendation', ['primary_id','recommendation_id'])

        # Adding unique constraint on 'ProductCategory', fields ['product', 'category']
        db.create_unique(u'catalogue_productcategory', ['product_id', 'category_id'])


    def backwards(self, orm):
        # Removing unique constraint on 'ProductCategory', fields ['product', 'category']
        db.delete_unique(u'catalogue_productcategory', ['product_id', 'category_"
832293,-2916587927432157305,9,BigQuery,"copy_method

def _keep_alive(x, memo):
    """"""Keeps a reference to the object x in the memo.

    Because we remember objects by their id, we have
    to assure that possibly temporary objects are kept
    alive by referencing them.
    We store a reference at the id of the memo, which should
    normally not be used unless someone tries to deepcopy
    the memo itself...
    """"""
    try:
        memo[id(memo)].append(x)
    except KeyError:
        # aha, this is the first one :-)"
1786173,-4018285172731532330,14,BigQuery,"
    cls.add_constructor([])
    ## boolean.h (module 'core'): ns3::BooleanValue::BooleanValue(bool value) [constructor]
    cls.add_constructor([param('bool', 'value')])
    ## boolean.h (module 'core'): ns3::Ptr<ns3::AttributeValue> ns3::BooleanValue::Copy() const [member function]
    cls.add_method('Copy', 
                   'ns3::Ptr< ns3::AttributeValue >', 
                   [], 
                   is_const=True, is_virtual=True)
    ## boolean.h ("
5075938,1415996681843541236,18,BigQuery,"pr = self.width * self.planes
        row_bytes = vpr
        if self.bitdepth > 8:
            assert self.bitdepth == 16
            row_bytes *= 2
            fmt = '>%dH' % vpr
            def line():
                return array('H', struct.unpack(fmt, infile.read(row_bytes)))
        else:
            def line():
                scanline = array('B', infile.read(row_bytes))
                return scanline
        for y in range(self.height):
            yield line()

    def array_scanlines(self, pixels):
        "
3090302,-4158840908696589450,10,BigQuery," the index method, to filter the result, along with the optional query string and XML filter described below.

            **Inputs**

            |  ``api version min:`` 2.10
            |  ``api version max:`` None
            |  ``required:`` False
            |  ``default:`` None

             :param completed_at: The date and time this target was completed in the script execution.
             :type completed_at: Array of DateTime

            |  ``api version min:`` 2.10
            |  ``api version max:`` None
            |  ``required:`` False
            |  ``default:`` None

             :param created_"
7500897,4735476120046207597,94,BigQuery,".

    Operator precedence follows Python.
    """"""
    # {% if... %}
    bits = token.split_contents()[1:]
    condition = TemplateIfParser(parser, bits).parse()
    nodelist = parser.parse(('elif', 'else', 'endif'))
    conditions_nodelists = [(condition, nodelist)]
    token = parser.next_token()

    # {% elif... %} (repeatable)
    while token.contents.startswith('elif'):
        bits = token.split_contents()[1:]
        condition = TemplateIfParser(parser,"
1756274,-7940662016709177435,12,BigQuery,"_unwrapped_output:
      # Force refresh screen output.
      self._scroll_output(_SCROLL_REFRESH)

    if not candidates:
      return

    candidates_prefix = ""Candidates: ""
    candidates_line = candidates_prefix + "" "".join(candidates)
    candidates_output = debugger_cli_common.RichTextLines(
        candidates_line,
        font_attr_segs={
            0: [(len(candidates_prefix), len(candidates_line), ""yellow"")]
        })

    candidates_output, _ = debugger_cli_common.wrap_rich_text"
1468552,-1478610679956414299,71,BigQuery,"-+]?\d+)?', Number.Float),
        ],
    }


class CobolFreeformatLexer(CobolLexer):
    """"""
    Lexer for Free format OpenCOBOL code.

    *New in Pygments 1.6.*
    """"""
    name = 'COBOLFree'
    aliases = ['cobolfree']
    filenames = ['*.cbl', '*.CBL']
    mimetypes = []
    flags = re.IGNORECASE | re.MULTILINE

    tokens = {
        'comment': [
            (r'(\*>.*\n"
1710834,2736639627809068830,22,BigQuery,"s_with_annotation(
                self.primaryjoin, annotation)
        )
        if self.secondaryjoin is not None:
            s.update(
                self._gather_columns_with_annotation(
                    self.secondaryjoin, annotation)
            )
        return set([x._deannotate() for x in s])

    def _gather_columns_with_annotation(self, clause, *annotation):
        annotation = set(annotation)
        return set([
            col for col in visitors.iterate(clause, {})
            if annotation.issubset(col._annotations)
        ])"
559731,6131382122638015191,6,BigQuery,"_ID))
        req.method = 'POST'

        self.assertRaises(webob.exc.HTTPBadRequest,
                          self.controller.create, req, fake.VOLUME_ID, body)

    def test_create_no_body(self):
        self._extra_specs_create_bad_body(body=None)

    def test_create_missing_volume(self):
        body = {'foo': {'a': 'b'}}
        self._extra_specs_create_bad_body(body=body)

    def test_create_malformed_entity(self):
        body = {'extra_"
6681883,-3917047993421900556,8,BigQuery,"Set):
            w.as_sql(compiler, connection)
        w.negate()
        self.assertEqual(w.as_sql(compiler, connection), ('', []))
        w = WhereNode(children=[self.DummyNode(), self.DummyNode()])
        self.assertEqual(w.as_sql(compiler, connection), ('(dummy AND dummy)', []))
        w.negate()
        self.assertEqual(w.as_sql(compiler, connection), ('NOT (dummy AND dummy)', []))
        w = WhereNode(children=[NothingNode(), self.DummyNode"
225880,7354260512264214866,6,BigQuery,".assertDictEqual(fake.FC_I_T_MAP, init_targ_map)
        self.assertEqual(4, num_paths)

    @mock.patch.object(na_utils, 'check_flags')
    def test_do_setup_san_configured(self, mock_check_flags):
        self.library.configuration.netapp_lun_ostype = 'windows'
        self.library.configuration.netapp_host_type ='solaris'
        self.library.configuration.netapp_lun_space_reservation = 'disabled'
        self.library"
1194741,3915370675488577098,6,BigQuery,"-uploaded files.
# Example: ""/home/media/media.lawrence.com/media/""
MEDIA_ROOT = ''

# URL that handles the media served from MEDIA_ROOT. Make sure to use a
# trailing slash.
# Examples: ""http://media.lawrence.com/media/"", ""http://example.com/media/""
MEDIA_URL = ''

# Absolute path to the directory static files should be collected to.
# Don't put anything in this directory yourself; store your static files
# in apps' ""static/"" subdirectories and in STATICFILES_DIRS.
# Example: ""/home/media/"
802645,-5430807245977041975,17,BigQuery," = np.sum(sample_model_inliers)
        if (
            # more inliers
            sample_inlier_num > best_inlier_num
            # same number of inliers but less ""error"" in terms of residuals
            or (sample_inlier_num == best_inlier_num
                and sample_model_residuals_sum < best_inlier_residuals_sum)
        ):
            best_model = sample_model
            best_inlier_num = sample_inlier_num
            best_inlier_residuals_sum = sample_model_residuals"
4848498,-5359414762184094547,26,BigQuery,".app(environ, start_response)

        guessed_type = mimetypes.guess_type(real_filename)
        mime_type = guessed_type[0] or self.fallback_mimetype
        f, mtime, file_size = file_loader()

        headers = [('Date', http_date())]
        if self.cache:
            timeout = self.cache_timeout
            etag = self.generate_etag(mtime, file_size, real_filename)
            headers += [
                ('Etag', '""%s""' % etag),
                ('Cache-Control','max"
1149724,8228013215027817651,8,BigQuery,")//n) # initial estimate
            while True:
                q, r = divmod(xc, a**(n-1))
                if a <= q:
                    break
                else:
                    a = (a*(n-1) + q)//n
            if not (a == q and r == 0):
                return None
            xc = a

        # now xc*10**xe is the nth root of the original xc*10**xe
        # compute mth power of xc*10**xe

        # if m > p*100//_log10_lb(xc) then m > p/log10(xc), hence"
1029243,3154997504650554313,8,BigQuery,"('GetAddress', 
                   'ns3::Address', 
                   [], 
                   is_const=True, is_virtual=True)
    ## virtual-net-device.h: ns3::Address ns3::VirtualNetDevice::GetBroadcast() const [member function]
    cls.add_method('GetBroadcast', 
                   'ns3::Address', 
                   [], 
                   is_const=True, is_virtual=True)
    ## virtual-net-device.h: ns3::Ptr<ns3::Channel> ns3::VirtualNetDevice::GetChannel() const [member function]
    cls.add_method"
1149724,-7132126743035774196,8,BigQuery,"int *
        # 10**ideal_exponent.   Apply correction to ensure that
        # abs(remainder) <= abs(other)/2
        if 2*r + (q&1) > op2.int:
            r -= op2.int
            q += 1

        if q >= 10**context.prec:
            return context._raise_error(DivisionImpossible)

        # result has same sign as self unless r is negative
        sign = self._sign
        if r < 0:
            sign = 1-sign
            r = -r

        ans = _dec_from_triple(sign, str(r), ideal"
2862961,-3681426101426248995,38,BigQuery,", is_virtual=True)
    ## ipv4-l3-protocol.h (module 'internet'): uint32_t ns3::Ipv4L3Protocol::GetNAddresses(uint32_t interface) const [member function]
    cls.add_method('GetNAddresses', 
                   'uint32_t', 
                   [param('uint32_t', 'interface')], 
                   is_const=True, is_virtual=True)
    ## ipv4-l3-protocol.h (module 'internet'): uint32_t ns3::Ipv4L3Protocol::GetN"
2741113,-1443938879070717931,18,BigQuery,"g1>'))

    def test_5027_2(self):
        # The xml prefix (as in xml:lang below) is reserved and bound by
        # definition to http://www.w3.org/XML/1998/namespace.  XMLGenerator had
        # a bug whereby a KeyError is raised because this namespace is missing
        # from a dictionary.
        #
        # This test demonstrates the bug by direct manipulation of the
        # XMLGenerator.
        result = self.ioclass()
        gen = XMLGenerator(result)

        gen.startDocument()
        gen.startPrefixMapping('a', '"
378761,8690446080128952443,75,BigQuery,6e03\u6e04\u6e05\u6e06\u6e07\u6e08\u6e09\u6e0a\u6e0b\u6e0c\u6e0d\u6e0e\u6e0f\u6e10\u6e11\u6e12\u6e13\u6e14\u6e15\u6e16\u6e17\u6e18\u6e19\u6e1a\u6e1b\u6e1c\u6e1d\u6e1e\
3635319,863588840031310995,12,BigQuery,"_critical = module.params['subjectAltName_critical']
        self.keyUsage = module.params['keyUsage']
        self.keyUsage_critical = module.params['keyUsage_critical']
        self.extendedKeyUsage = module.params['extendedKeyUsage']
        self.extendedKeyUsage_critical = module.params['extendedKeyUsage_critical']
        self.basicConstraints = module.params['basicConstraints']
        self.basicConstraints_critical = module.params['basicConstraints_critical']
        self.ocspMustStaple = module.params['ocspMustStaple']
        self.oc"
5924146,1774172845581413061,63,BigQuery," 1, the summary value tag is '*tag*/audio'.
  *  If `max_outputs` is greater than 1, the summary value tags are
     generated sequentially as '*tag*/audio/0', '*tag*/audio/1', etc.

  Args:
    tag: A scalar `Tensor` of type `string`. Used to build the `tag`
      of the summary values.
    tensor: A 3-D `float32` `Tensor` of shape `[batch_size, frames, channels]`
      or a 2-D `float32` `Tensor` of shape `[batch_size, frames]`.
    "
4787232,3994922339262902019,7,BigQuery,"    iaxis = normalize_axis_index(iaxis, c.ndim)

    if cnt == 0:
        return c

    c = np.rollaxis(c, iaxis)
    k = list(k) + [0]*(cnt - len(k))
    for i in range(cnt):
        n = len(c)
        c *= scl
        if n == 1 and np.all(c[0] == 0):
            c[0] += k[i]
        else:
            tmp = np.empty((n + 1,) + c.shape[1:], dtype"
4453209,7799164424230268547,6,BigQuery,".""
                      ""Hypervisors"")

    def test_hypervisors_list(self):
        response = self._do_get('os-hypervisors')
        self._verify_response('hypervisors-list-resp', {}, response, 200)

    def test_hypervisors_search(self):
        response = self._do_get('os-hypervisors/fake/search')
        self._verify_response('hypervisors-search-resp', {}, response, 200)

    def test_hypervisors_servers(self):
        response = self._do_get('os-hypervisors/fake/servers')
        self._verify_response"
5075196,-7746318014419287776,6,BigQuery,"tf.float32, name='dropout_prob')

  input_frequency_size = model_settings['fingerprint_width']
  input_time_size = model_settings['spectrogram_length']

  # Validation.
  input_shape = fingerprint_input.get_shape()
  if len(input_shape)!= 2:
    raise ValueError('Inputs to `SVDF` should have rank == 2.')
  if input_shape[-1].value is None:
    raise ValueError('The last dimension of the inputs to `SVDF` '
                    'should be defined. Found `None`.')
  if"
2983912,7678199273632930495,11,BigQuery,"ets,)
        independent term in decision function.

    n_iter_ : int or array-like
        Number of active features across every target.

    Notes
    -----
    Orthogonal matching pursuit was introduced in G. Mallat, Z. Zhang,
    Matching pursuits with time-frequency dictionaries, IEEE Transactions on
    Signal Processing, Vol. 41, No. 12. (December 1993), pp. 3397-3415.
    (http://blanche.polytechnique.fr/~mallat/papiers/MallatPursuit93.pdf)

    This implementation is based on Rubinstein, R., Zibulevsky,"
4060446,-2943204028390705674,8,BigQuery,":
    An apiproxy_stub_map.UserRPC object specialized for this service.
  """"""
  if deadline is not None:
    _ValidateDeadline(deadline)
  return apiproxy_stub_map.UserRPC('taskqueue', deadline, callback)


class TaskRetryOptions(object):
  """"""The options used to decide when a failed Task will be retried.""""""

  __CONSTRUCTOR_KWARGS = frozenset(
      ['min_backoff_seconds','max_backoff_seconds',
       'task_age_limit','max_doublings', 'task_retry"
2862961,7451348924858821149,38,BigQuery,"Ns3Packet_Ns3Empty_Ns3DefaultDeleter__lt__ns3Packet__gt___methods(root_module, root_module['ns3::SimpleRefCount< ns3::Packet, ns3::empty, ns3::DefaultDeleter<ns3::Packet> >'])
    register_Ns3SimpleRefCount__Ns3QueueItem_Ns3Empty_Ns3DefaultDeleter__lt__ns3QueueItem__gt___methods(root_module, root_module['ns3::SimpleRefCount< ns3::QueueItem, ns3::empty, ns3::DefaultDeleter<ns3::QueueItem> >"
377975,8415977999717963632,8,BigQuery,"line, 0, -1)

  # Check last line
  (start_pos, stack) = FindStartOfExpressionInLine(line, pos, [])
  if start_pos > -1:
    return (line, linenum, start_pos)

  # Continue scanning backward
  while stack and linenum > 0:
    linenum -= 1
    line = clean_lines.elided[linenum]
    (start_pos, stack) = FindStartOfExpressionInLine(line, len(line) - 1, stack)
    if start_pos > -1:
      return (line, linenum, start_pos)

  "
3938548,1969995109256266951,6,BigQuery,"self, content, len):
        """"""Creation of a new node containing a CDATA block. """"""
        ret = libxml2mod.xmlNewCDataBlock(self._o, content, len)
        if ret is None:raise treeError('xmlNewCDataBlock() failed')
        __tmp = xmlNode(_obj=ret)
        return __tmp

    def newCharRef(self, name):
        """"""Creation of a new character reference node. """"""
        ret = libxml2mod.xmlNewCharRef(self._o, name)
        if ret is None:raise treeError('xmlNewCharRef() failed')
        __tmp = xmlNode"
2210638,-4505106778666043674,12,BigQuery,"Ns3Packet_Ns3Empty_Ns3DefaultDeleter__lt__ns3Packet__gt___methods(root_module, root_module['ns3::SimpleRefCount< ns3::Packet, ns3::empty, ns3::DefaultDeleter<ns3::Packet> >'])
    register_Ns3SimpleRefCount__Ns3SpectrumSignalParameters_Ns3Empty_Ns3DefaultDeleter__lt__ns3SpectrumSignalParameters__gt___methods(root_module, root_module['ns3::SimpleRefCount< ns3::SpectrumSignalParameters, ns3::empty, ns3::DefaultDele"
5953544,-3489912461556149520,6,BigQuery,"self.groupSuffix = iprot.readString();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    oprot.write"
7076511,1516831331046475406,23,BigQuery," this protocol which came after the terminal
        chunk.

    @ivar length: Counter keeping track of how many more bytes in a chunk there
        are to receive.

    @ivar state: One of C{'CHUNK_LENGTH'}, C{'CRLF'}, C{'TRAILER'},
        C{'BODY'}, or C{'FINISHED'}.  For C{'CHUNK_LENGTH'}, data for the
        chunk length line is currently being read.  For C{'CRLF'}, the CR LF
        pair which follows each chunk is being read. For C{'TRAIL"
7076043,8877930498199286957,9,BigQuery,"
NMPWAIT_NOWAIT = 1
NMPWAIT_USE_DEFAULT_WAIT = 0
OF_READ = 0
OF_WRITE = 1
OF_READWRITE = 2
OF_SHARE_COMPAT = 0
OF_SHARE_EXCLUSIVE = 16
OF_SHARE_DENY_WRITE = 32
OF_SHARE_DENY_READ = 48
OF_SHARE_DENY_NONE = 64
OF_PARSE = 256
OF_DELETE = 512
OF_VERIFY = 1024
OF_CANCEL = 2048
OF"
6272155,4516187003901532812,7,BigQuery,"Bilinear interpolation.](
    https://en.wikipedia.org/wiki/Bilinear_interpolation)
  *   <b>`ResizeMethod.NEAREST_NEIGHBOR`</b>: [Nearest neighbor interpolation.](
    https://en.wikipedia.org/wiki/Nearest-neighbor_interpolation)
  *   <b>`ResizeMethod.BICUBIC`</b>: [Bicubic interpolation.](
    https://en.wikipedia.org/wiki/Bicubic_interpolation)
  *   <b>`ResizeMethod.AREA`</"
4726021,2033128880718036649,8,BigQuery,""", models.AutoField(primary_key=True)),
    ], {""db_table"": ""author_one""})
    author_with_new_db_table_options = ModelState(""testapp"", ""Author"", [
        (""id"", models.AutoField(primary_key=True)),
    ], {""db_table"": ""author_two""})
    author_renamed_with_db_table_options = ModelState(""testapp"", ""NewAuthor"", [
        (""id"", models.AutoField(primary_key=True)),
    ], {""db_table"": ""author_one""})
    author_renamed_with_new_db_table"
2302570,7659921516442005940,37,BigQuery,"['reassemble_fragments'] == 'no':
            return 'disabled'

    @property
    def reset_on_timeout(self):
        if self._values['reset_on_timeout'] == 'yes':
            return 'enabled'
        elif self._values['reset_on_timeout'] == 'no':
            return 'disabled'

    @property
    def rtt_from_client(self):
        if self._values['rtt_from_client'] == 'yes':
            return 'enabled'
        elif self._values['rtt_from_client'] == 'no':
            return 'disabled'

    @property"
2877483,-1496013796673662480,8,BigQuery,"
def expr(s):
    """"""Create an Expr representing a logic expression by parsing the input
    string. Symbols and numbers are automatically converted to Exprs.
    In addition you can use alternative spellings of these operators:
      'x ==> y'   parses as   (x >> y)    # Implication
      'x <== y'   parses as   (x << y)    # Reverse implication
      'x <=> y'   parses as   (x % y)     # Logical equivalence
      'x =/= y'   parses as   (x ^ y)     # Logical disequality (x"
4166736,8402862854302125521,7,BigQuery," 1?
    assert integrate(besselj(1, x), x, meijerg=True) == -besselj(0, x)
    assert integrate(besselj(1, x)**2/x, x, meijerg=True) == \
        -(besselj(0, x)**2 + besselj(1, x)**2)/2
    # TODO more besseli when tables are extended or recursive mellin works
    assert integrate(besselj(0, x)**2/x**2, x, meijerg=True) == \
        -2*x*besselj(0, x)"
6272155,-8412178410659313263,7,BigQuery,") by
  `delta`.  The image is then converted back to RGB.

  `delta` must be in the interval `[-1, 1]`.

  Args:
    image: RGB image or images. Size of the last dimension must be 3.
    delta: float.  How much to add to the hue channel.
    name: A name for this operation (optional).

  Returns:
    Adjusted image(s), same shape and DType as `image`.
  """"""
  with ops.name_scope(name, 'adjust_hue', [image]) as name:
    image = ops.convert_to_tensor(image"
3695899,1734009707201296860,11,BigQuery,"config, ce_argument_spec


SUCCESS = """"""success""""""
FAILED = """"""failed""""""


# get bgp enable
CE_GET_BGP_ENABLE = """"""
    <filter type=""subtree"">
      <bgp xmlns=""http://www.huawei.com/netconf/vrp"" content-version=""1.0"" format-version=""1.0"">
        <bgpcomm>
          <bgpSite>
            <bgpEnable></bgpEnable>
            <asNumber></asNumber>
          </bgpSite>
        </bgpcomm>
      </bgp>
    </filter>"
3362593,4367190492844629032,15,BigQuery," 255
                    )

SSHPASS_AVAILABLE = None


class AnsibleControlPersistBrokenPipeError(AnsibleError):
    ''' ControlPersist broken pipe '''
    pass


def _ssh_retry(func):
    """"""
    Decorator to retry ssh/scp/sftp in the case of a connection failure

    Will retry if:
    * an exception is caught
    * ssh returns 255
    Will not retry if
    * remaining_tries is <2
    * retries limit reached
    """"""
    @wraps(func)
    def wrapped(self, *args"
4879071,3004413297787010166,49,BigQuery,"_check(
            ""<a $><b $=%><c \=/>"",
            [(""starttag"", ""a"", [(""$"", None)]),
             (""starttag"", ""b"", [(""$"", ""%"")]),
             (""starttag"", ""c"", [(""\\"", ""/"")])])

    def test_entityrefs_in_attributes(self):
        self._run_check(
            ""<html foo='&euro;&amp;&#97;&#x61;&unsupported;'>"",
            [(""starttag"", ""html"", [(""foo"", u""\u20AC&aa&unsupported;"")])"
1179655,-351276404989078462,6,BigQuery,"ize the request_uri?
        (scheme, authority, path, query, fragment) = parse_uri(request_uri)
        return (host == self.host) and path.startswith(self.path)

    def request(self, method, request_uri, headers, content):
        """"""Modify the request headers to add the appropriate
        Authorization header. Over-ride this in sub-classes.""""""
        pass

    def response(self, response, content):
        """"""Gives us a chance to update with new nonces
        or such returned from the last authorized response.
        Over-rise this in sub-classes if necessary.

        Return"
377975,-2879913113436410967,8,BigQuery,"matching_punctuation[text[position]])
    position += 1
  if punctuation_stack:
    # Opening punctuations left without matching close-punctuations.
    return None
  # punctuations match.
  return text[start_position:position - 1]


# Patterns for matching call-by-reference parameters.
#
# Supports nested templates up to 2 levels deep using this messy pattern:
#   < (?: < (?: < [^<>]*
#               >
#           |   [^<>] )*
#         >
#     |   [^<>] )*
#   >
_RE_PATTERN"
4620588,6134665081058869757,14,BigQuery,"_const=True, visibility='private', is_virtual=True)
    return

def register_Ns3Object_methods(root_module, cls):
    ## object.h (module 'core'): ns3::Object::Object() [constructor]
    cls.add_constructor([])
    ## object.h (module 'core'): void ns3::Object::AggregateObject(ns3::Ptr<ns3::Object> other) [member function]
    cls.add_method('AggregateObject', 
                   'void', 
                   [param('ns3::Ptr< ns3::Object >', 'other')])
"
408781,-7037651834928595145,25,BigQuery," as e:
            self.assertTrue(e)
            del e
        self.assertNotIn('e', locals())

    def testExceptionCleanupState(self):
        # Make sure exception state is cleaned up as soon as the except
        # block is left. See #2507

        class MyException(Exception):
            def __init__(self, obj):
                self.obj = obj
        class MyObj:
            pass

        def inner_raising_func():
            # Create some references in exception value and traceback
            local_ref = obj
            raise MyException(obj)

        # Qualified ""except"" with ""as""
        "
6106260,3315924744010873867,11,BigQuery,"_var_list = set(decay_var_list) if decay_var_list else False
    return super(DecoupledWeightDecayExtension, self).apply_gradients(
        grads_and_vars, global_step=global_step, name=name)

  def _prepare(self):
    weight_decay = self._weight_decay
    if callable(weight_decay):
      weight_decay = weight_decay()
    self._weight_decay_tensor = ops.convert_to_tensor(
        weight_decay, name=""weight_decay"")
    # Call"
5544771,-7491105179888689348,9,BigQuery,"write('<%s>%f</%s>' %
                (self.name, self.value, self.name))
        elif self.content_type == MixedContainer.TypeDouble:
            outfile.write('<%s>%g</%s>' %
                (self.name, self.value, self.name))
        elif self.content_type == MixedContainer.TypeBase64:
            outfile.write('<%s>%s</%s>' %
                (self.name, base64.b64encode(self.value), self.name))
    def to_etree(self, element):"
3938548,6267240147421162586,6,BigQuery," 3017
XML_SCHEMAP_COS_ST_RESTRICTS_2_3_2_1 = 3018
XML_SCHEMAP_COS_ST_RESTRICTS_2_3_2_2 = 3019
XML_SCHEMAP_COS_ST_RESTRICTS_2_3_2_3 = 3020
XML_SCHEMAP_COS_ST_RESTRICTS_2_3_2_4 = 3021
XML_SCHEMAP_COS_ST_RESTRICTS_2_3_2_5 = 3022
XML"
2393317,7158569768925363148,45,BigQuery,"ob@example.com')

        # Set a one-to-one relation with an object from a different database
        alice_profile = UserProfile.objects.create(user=alice, flavor='chocolate')
        try:
            bob.userprofile = alice_profile
        except ValueError:
            self.fail(""Assignment across primary/replica databases with a common source should be ok"")

        # Database assignments of original objects haven't changed...
        self.assertEqual(alice._state.db, 'default')
        self.assertEqual(alice_profile._state.db, 'default')
        self.assertEqual("
5363455,1445217485520450180,12,BigQuery,"""]
    tower_host:
      description:
        - URL to your Tower instance.
      required: False
      default: null
    tower_username:
        description:
          - Username for your Tower instance.
        required: False
        default: null
    tower_password:
        description:
          - Password for your Tower instance.
        required: False
        default: null
    tower_verify_ssl:
        description:
          - Dis/allow insecure connections to Tower. If C(no), SSL certificates will not be validated.
            This should only be used on personally controlled sites using self-signed certificates.
        required: False
        default:"
332394,-1529234837998300137,8,BigQuery,"_1"":
                name = ""latin_1""
            self.assertEqual(encoding.replace(""_"", ""-""), name.replace(""_"", ""-""))
            (bytes, size) = codecs.getencoder(encoding)(s)
            self.assertEqual(size, len(s), ""%r!= %r (encoding=%r)"" % (size, len(s), encoding))
            (chars, size) = codecs.getdecoder(encoding)(bytes)
            self.assertEqual(chars, s, ""%r!= %r (encoding=%r)"" % (chars, s, encoding))"
5332311,-8300839336555982683,6,BigQuery,".ie.Document:
                return self.ie.Document.setShowToolbar(On)
            else:
                raise PDFWindowError()

        def setView(self, viewMode):
            """"""
            Determines how the page will fit in the current view.
            viewMode possible values:

                ========  ==============================================
                'Fit'     fits whole page within the window both vertically
                          and horizontally.
                'FitH'    fits the width of the page within the window.
                'FitV'    fits the height of the page within the window.
                'FitB'    fits bounding box within the window both vertically
                          and horizontally.
"
6697696,-6772146795959251179,27,BigQuery,"
short_description: Collect facts from Ordnance Virtual Routers over SSH
description:
  - Collects a base set of device facts from an Ordnance Virtual
    router over SSH. This module prepends all of the
    base network fact keys with C(ansible_net_<fact>).  The facts
    module will always collect a base set of facts from the device
    and can enable or disable collection of additional facts.
options:
  gather_subset:
    description:
      - When supplied, this argument will restrict the facts collected
        to a given subset.  Possible values for this argument include
        all, hardware, config, and interfaces.  "
3513970,-232752299006215183,10,BigQuery,"challenges = _parse_www_authenticate(response, 'www-authenticate')
        for cred in self.credentials.iter(host):
            for scheme in AUTH_SCHEME_ORDER:
                if scheme in challenges:
                    yield AUTH_SCHEME_CLASSES[scheme](cred, host, request_uri, headers, response, content, self)

    def add_credentials(self, name, password, domain=""""):
        """"""Add a name and password that will be used
        any time a request requires authentication.""""""
        self.credentials.add(name, password, domain)

    def add"
5408731,3807571226380954901,13,BigQuery,"9""
author:
  - ""Yanis Guenane (@Spredzy)""
  - ""René Moser (@resmo)""
extends_documentation_fragment: vultr
'''

EXAMPLES = r'''
- name: Get Vultr OSes infos
  vultr_os_info:
  register: results

- name: Print the gathered infos
  debug:
    var: results.vultr_os_info
'''

RETURN = r'''
---
vultr_api:
  description: Response from Vultr API with a few additions/modification
"
4665874,-7402264897470934475,12,BigQuery,"_3', 'col_0', 'col_1', 'col_4') VALUES ('row_193', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_297', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1"
3105515,3769529671474848368,17,BigQuery,"sep

    @property
    def yaml_dict(self):
        ''' getter method for yaml_dict '''
        return self.__yaml_dict

    @yaml_dict.setter
    def yaml_dict(self, value):
        ''' setter method for yaml_dict '''
        self.__yaml_dict = value

    @staticmethod
    def parse_key(key, sep='.'):
        '''parse the key allowing the appropriate separator'''
        common_separators = list(Yedit.com_sep - set([sep]))
        return re.findall("
1634747,9032452652858714950,14,BigQuery,"oauth_token=kjerht2309uf&oauth_token_secret=lsdajfh923874&oauth_verifier=w34o8967345'
        >>> oauth_session = OAuth1Session('client-key', client_secret='secret')
        >>> oauth_session.parse_authorization_response(redirect_response)
        {
            'oauth_token: 'kjerht2309u',
            'oauth_token_secret: 'lsdajfh923874',
            'oauth_verifier: 'w34o8967345',
        }
        >>> oauth_session"
7076043,-1575504613098915223,9,BigQuery,"RESERVED3 = 26
SM_RESERVED4 = 27
SM_CXMIN = 28
SM_CYMIN = 29
SM_CXSIZE = 30
SM_CYSIZE = 31
SM_CXFRAME = 32
SM_CYFRAME = 33
SM_CXMINTRACK = 34
SM_CYMINTRACK = 35
SM_CXDOUBLECLK = 36
SM_CYDOUBLECLK = 37
SM_CXICONSPACING = 38
SM_CYICONSPACING = 39
SM"
1861052,-1569266101989583636,6,BigQuery," + '""/>\n')
sc.write('  <property name=""pin6"" type=""string"" value=""' + dPins['6'] + '""/>\n')
sc.write('  <property name=""pin6inv"" type=""bool"" value=""' + dInvert['6'] + '""/>\n')
sc.write('  <property name=""pin7"" type=""string"" value=""' + dPins['7'] + '""/>\n')
sc.write('  <property name=""pin7inv"" type=""bool"" value=""' + dInvert['7'] + '""/>\n')
sc.write('  <property name="""
6847927,-6579296250981147933,7,BigQuery," dry_run=dry_run)

def copy_tree(src, dst, preserve_mode=1, preserve_times=1,
              preserve_symlinks=0, update=0, verbose=1, dry_run=0):
    """"""Copy an entire directory tree'src' to a new location 'dst'.

    Both'src' and 'dst' must be directory names.  If'src' is not a
    directory, raise DistutilsFileError.  If 'dst' does not exist, it is
    created with'mkpath()'.  The end result of the copy is that every
    file in'src' is copied to"
1392113,-4029409010702401018,7,BigQuery,"):
    """"""
    A collection of bigram association measures. Each association measure
    is provided as a function with three arguments::

        bigram_score_fn(n_ii, (n_ix, n_xi), n_xx)

    The arguments constitute the marginals of a contingency table, counting
    the occurrences of particular events in a corpus. The letter i in the
    suffix refers to the appearance of the word in question, while x indicates
    the appearance of any word. Thus, for example:

        n_ii counts (w1, w2), i.e. the bigram being scored
        n_ix counts (w1, *)
        n"
3362764,-2732509012110529163,22,BigQuery,"self):
        ""Make a fresh connection.""
        connection = self.connection_class(**self.connection_kwargs)
        self._connections.append(connection)
        return connection

    def get_connection(self, command_name, *keys, **options):
        """"""
        Get a connection, blocking for ``self.timeout`` until a connection
        is available from the pool.

        If the connection returned is ``None`` then creates a new connection.
        Because we use a last-in first-out queue, the existing connections
        (having been returned to the pool after the initial ``None`` values
        were added) will be returned before ``None``"
7364573,760785315849450151,20,BigQuery," target):
        ""Returns the authority code for the given string target node.""
        return capi.get_auth_code(self.ptr, force_bytes(target))

    def clone(self):
        ""Returns a clone of this SpatialReference object.""
        return SpatialReference(capi.clone_srs(self.ptr))

    def from_esri(self):
        ""Morphs this SpatialReference from ESRI's format to EPSG.""
        capi.morph_from_esri(self.ptr)

    def identify_epsg(self):
        """"""
        This method inspects the WKT of this SpatialReference,"
1786173,8231535115415088702,14,BigQuery,"network'): uint32_t ns3::EthernetHeader::Deserialize(ns3::Buffer::Iterator start) [member function]
    cls.add_method('Deserialize', 
                   'uint32_t', 
                   [param('ns3::Buffer::Iterator','start')], 
                   is_virtual=True)
    ## ethernet-header.h (module 'network'): ns3::Mac48Address ns3::EthernetHeader::GetDestination() const [member function]
    cls.add_method('GetDestination', 
                   'ns3::Mac48Address', 
                   [], 
                   is"
6773063,7458179934908315006,24,BigQuery,"acket >', 'pkt')])
    ## error-model.h (module 'network'): bool ns3::ErrorModel::IsEnabled() const [member function]
    cls.add_method('IsEnabled', 
                   'bool', 
                   [], 
                   is_const=True)
    ## error-model.h (module 'network'): void ns3::ErrorModel::Reset() [member function]
    cls.add_method('Reset', 
                   'void', 
                   [])
    ## error-model.h (module 'network'): bool ns3::ErrorModel::DoCorrupt(ns3::Ptr<"
3741729,-3012781481357385099,14,BigQuery,")
        # mapping = {}
        if node.comment:
            maptyp._yaml_add_comment(node.comment[:2])
            if len(node.comment) > 2:
                maptyp.yaml_end_comment_extend(node.comment[2], clear=True)
        if node.anchor:
            from ruamel.yaml.serializer import templated_id
            if not templated_id(node.anchor):
                maptyp.yaml_set_anchor(node.anchor)
        for key_node, value_node in node.value:
            # keys"
2348168,-1422047540971404540,42,BigQuery,"'])
        assert_equal(a[-1].b, 'bbb')

    def test_recarray_stringtypes(self):
        # Issue #3993
        a = np.array([('abc ', 1), ('abc', 2)],
                     dtype=[('foo', 'S4'), ('bar', int)])
        a = a.view(np.recarray)
        assert_equal(a.foo[0] == a.foo[1], False)

    def test_recarray_returntypes(self):
        qux_fields = {'C': (np.dtype('S5'), 0),"
4393893,-1954144832584931973,38,BigQuery,"        else:
            if not self.addr and not self.mask:
                self.set_ipv6_enable(self.interface)
            else:
                self.set_ipv6_addr(self.interface, self.addr, self.mask)

        self.get_end_state()
        self.results['changed'] = self.changed
        self.results['proposed'] = self.proposed
        self.results['existing'] = self.existing
        self.results['end_state'] = self.end_state
        if self.changed:
            self.results['updates'] = self.updates_cmd
        else"
6773063,2785902075161420788,24,BigQuery," buffer.h (module 'network'): void ns3::Buffer::CopyData(std::ostream * os, uint32_t size) const [member function]
    cls.add_method('CopyData', 
                   'void', 
                   [param('std::ostream *', 'os'), param('uint32_t','size')], 
                   is_const=True)
    ## buffer.h (module 'network'): uint32_t ns3::Buffer::CopyData(uint8_t * buffer, uint32_t size) const [member function]
    cls.add_method('CopyData', 
                   'uint32_"
801835,1109086661025304390,6,BigQuery,"name == 'posix':
    _LockImplPosix(target_file, flags)
  else:
    raise NotImplementedError('%s is not supported' % os.name)


def ReleaseFileLock(target_file):
  """""" Unlock the target file.
  Args:
    target_file: file handle of the file to release the lock.
  """"""
  if os.name == 'nt':
    _UnlockImplWin(target_file)
  elif os.name == 'posix':
    _UnlockImplPosix(target_file)
  else:
    raise NotImplementedError('%s is not"
1513071,7087023824699313505,8,BigQuery,"'),
                        quotas: ('get_default_quota_data',
                                 'get_disabled_quotas',
                                 'tenant_quota_usages',),
                        api.cinder: ('tenant_quota_update',),
                        api.nova: ('tenant_quota_update',)})
    def test_add_project_post(self, neutron=False):
        project = self.tenants.first()
        quota = self.quotas.first()
        default_role = self.roles.first()
        default_domain = self._get_default_domain()
        domain_id"
575297,7167109418225143641,9,BigQuery," Text, Keyword), 'itemtype'),
            (ncname + r':\*', Keyword.Type, 'operator'),
            (qname, Keyword.Type, 'occurrenceindicator'),
        ],
        'kindtest': [
            (r'\(:', Comment, 'comment'),
            (r'{', Punctuation, 'root'),
            (r'(\))([*+?]?)', popstate_kindtest_callback),
            (r'\*', Name, 'closekindtest'),
            (qname, Name, 'closekindtest'),
            (r'(element|schema-element)(\s*)(\"
7076693,-2430101927115554811,53,BigQuery,"    Multiple vector cross-products. Note that the direction of the cross
    product vector is defined by the `right-hand rule`.

    >>> x = np.array([[1,2,3], [4,5,6]])
    >>> y = np.array([[4,5,6], [1,2,3]])
    >>> np.cross(x, y)
    array([[-3,  6, -3],
           [ 3, -6,  3]])

    The orientation of `c` can be changed using the `axisc` keyword.

    >>> np.cross(x, y, axisc=0)
    "
378776,8099268376519756910,83,BigQuery,"(self, method, url, body=None, headers=None,
                encode_multipart=True, multipart_boundary=None,
                **kw):  # Abstract
        raise NotImplemented(""Classes extending RequestMethods must implement ""
                             ""their own ``urlopen`` method."")

    def request(self, method, url, fields=None, headers=None, **urlopen_kw):
        """"""
        Make a request using :meth:`urlopen` with the appropriate encoding of
        ``fields`` based on the ``method`` used.

        This is a convenience method that requires the least amount of manual
        effort. It can"
1922749,8471482431838003487,44,BigQuery,".0.0.0
  secondary_dns:
    description:
    - The secondary DNS server that this block of IPv4 addresses should access.
    default: 0.0.0.0
  ipv6_first_addr:
    description:
    - The first IPv6 address in the IPv6 addresses block.
    - This is the From field in the UCS Manager Add IPv6 Blocks menu.
  ipv6_last_addr:
    description:
    - The last IPv6 address in the IPv6 addresses block.
    - This is the To field in the UCS Manager Add IPv6 Blocks menu.
  ipv6_prefix:
    description"
2438411,5069361430222086415,27,BigQuery,"set = int((utcoffset + 30) // 60) * 60
            dst = int((dst + 30) // 60) * 60
            transition_info.append(memorized_ttinfo(utcoffset, dst, tzname))

        cls = type(zone, (DstTzInfo,), dict(
            zone=zone,
            _utc_transition_times=transitions,
            _transition_info=transition_info))

    return cls()

if __name__ == '__main__':
    import os.path
    from pprint import pprint
    base = os."
6938964,1265657806269005619,8,BigQuery,"et = None
    in_escape = False
    while True:
        waiting = port.inWaiting()
        read_bytes = port.read(1 if waiting == 0 else waiting)
        if read_bytes == b'':
            waiting_for = ""header"" if partial_packet is None else ""content""
            trace_function(""Timed out waiting for packet %s"", waiting_for)
            raise FatalError(""Timed out waiting for packet %s"" % waiting_for)
        trace_function(""Read %d bytes: %s"", len(read_bytes), HexFormatter(read_bytes))
        for b in read_bytes"
3650037,-5657617436943822359,8,BigQuery," = True

try:
	from datetime import datetime
except ImportError:
	print ""[Error] datetime python library is required to be installed!""
else:
	datetimeLibFound = True

try:
	import csv
except ImportError:
	print ""[Error] csv python library is required to be installed!""
else:
	csvLibFound = True

try:
	import xlrd
except ImportError:
	print ""[Error] xlrd python library is required to be installed!""
else:
	xlrdLibFound = True

try:
	import xlwt
except ImportError:
	print ""[Error"
4848498,646937754227437434,26,BigQuery,"resource.
    """"""
    from werkzeug.urls import uri_to_iri, url_fix

    def _as_iri(obj):
        if not isinstance(obj, unicode):
            return uri_to_iri(obj, charset, errors)
        return obj

    def _normalize_netloc(scheme, netloc):
        parts = netloc.split(u'@', 1)[-1].split(u':', 1)
        if len(parts) == 2:
            netloc, port = parts
            if (scheme == u'http' and port == u'80') or \
               "
7076705,2087973807771977096,56,BigQuery,"generator_filelist_paths = {
      'toplevel': toplevel,
      'qualified_out_dir': qualified_out_dir,
  }


def OpenOutput(path, mode='w'):
  """"""Open |path| for writing, creating directories if necessary.""""""
  gyp.common.EnsureDirExists(path)
  return open(path, mode)


def CommandWithWrapper(cmd, wrappers, prog):
  wrapper = wrappers.get(cmd, '')
  if wrapper:
    return wrapper +'' + prog
  return prog


def GetDefaultConcurrentLinks():
  """"""Returns"
3075375,-2692203401273777870,15,BigQuery,"# ensure that the two transactions overlap.
        thread1 = threading.Thread(target=task1)
        thread2 = threading.Thread(target=task2)
        thread1.start()
        thread2.start()
        thread1.join()
        thread2.join()

        # Exactly one of the threads should have failed with
        # TransactionRollbackError:
        self.assertFalse(self.thread1_error and self.thread2_error)
        error = self.thread1_error or self.thread2_error
        self.assertTrue(isinstance(
            error, psycopg2.extensions.TransactionRollbackError"
621467,-6796743332466839205,33,BigQuery,"chemy and is released under
# the MIT License: http://www.opensource.org/licenses/mit-license.php

from. import base, mysqldb, oursql, \
    pyodbc, zxjdbc, mysqlconnector, pymysql,\
    gaerdbms, cymysql

# default dialect
base.dialect = mysqldb.dialect

from.base import \
    BIGINT, BINARY, BIT, BLOB, BOOLEAN, CHAR, DATE, DATETIME, \
    DECIMAL, DOUBLE, ENUM, DECIMAL,\"
5015234,5621077653205433081,21,BigQuery," OUT OF OR IN CONNECTION
# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
#

__revision__ = ""src/engine/SCons/Platform/hpux.py 5357 2011/09/09 21:31:03 bdeegan""

import posix

def generate(env):
    posix.generate(env)
    #Based on HP-UX11i: ARG_MAX=2048000 - 3000 for environment expansion
    env['MAXLINELENGTH']  = 2045000

# Local Variables:
# tab-width:4
# indent-tabs-mode:nil
#"
1179863,-4845206725448207303,7,BigQuery,"4], requestorUserName=None, component=None, serviceName=None, oldAuthorizables=None, newAuthorizables=None,):
    self.protocol_version = protocol_version
    self.requestorUserName = requestorUserName
    self.component = component
    self.serviceName = serviceName
    self.oldAuthorizables = oldAuthorizables
    self.newAuthorizables = newAuthorizables

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport"
1786173,882660119726554133,14,BigQuery,"', 'MIN', 'S', 'MS', 'US', 'NS', 'PS', 'FS', 'LAST'], outer_class=root_module['ns3::Time'], import_from_module='ns.core')
    ## nstime.h (module 'core'): ns3::Time [class]
    root_module['ns3::Time'].implicitly_converts_to(root_module['ns3::int64x64_t'])
    ## trace-source-accessor.h (module 'core'): ns3::TraceSourceAccessor [class]
    module.add_class('TraceSourceAccessor',"
1558362,265004135382957338,7,BigQuery,", Series
        arr = Categorical(list('abc'))
        result = lib.infer_dtype(arr)
        self.assertEqual(result, 'categorical')

        result = lib.infer_dtype(Series(arr))
        self.assertEqual(result, 'categorical')

        arr = Categorical(list('abc'), categories=['cegfab'], ordered=True)
        result = lib.infer_dtype(arr)
        self.assertEqual(result, 'categorical')

        result = lib.infer_dtype(Series(arr))
        self.assertE"
575297,8961412875813017300,9,BigQuery,"?'"", String, '#pop'),
            (r'[^\s>]+', String, '#pop'),
        ],
    }

    def analyse_text(text):
        if looks_like_xml(text):
            return 0.5


class XsltLexer(XmlLexer):
    '''
    A lexer for XSLT.

    *New in Pygments 0.10.*
    '''

    name = 'XSLT'
    aliases = ['xslt']
    filenames = ['*.xsl', '*.xslt', '*.xpl']  # xpl is XProc
    "
2665327,3736470151029024699,27,BigQuery,"        default: no_use
        choices: ['no_use','true','false']
    public_as_only_limited:
        description:
            - Limited use public as number.
        default: no_use
        choices: ['no_use','true','false']
    public_as_only_replace:
        description:
            - Private as replaced by public as number.
        default: no_use
        choices: ['no_use','true','false']
    public_as_only_skip_peer_as:
        description:
            - Public as only skip peer as.
        default: no_use
        choices:"
5136130,-7959106297906684403,7,BigQuery,", double >', 'params'), param('double', 'b')])
    return

def register_Ns3RandomVariableStream_methods(root_module, cls):
    ## random-variable-stream.h (module 'core'): static ns3::TypeId ns3::RandomVariableStream::GetTypeId() [member function]
    cls.add_method('GetTypeId', 
                   'ns3::TypeId', 
                   [], 
                   is_static=True)
    ## random-variable-stream.h (module 'core'): ns3::RandomVariableStream::RandomVariableStream() [constructor]
    cls.add"
817976,-4093931219265623525,129,BigQuery,"
        return decorator

    @setupmethod
    def endpoint(self, endpoint):
        """"""A decorator to register a function as an endpoint.
        Example::

            @app.endpoint('example.endpoint')
            def example():
                return ""example""

        :param endpoint: the name of the endpoint
        """"""
        def decorator(f):
            self.view_functions[endpoint] = f
            return f
        return decorator

    @staticmethod
    def _get_exc_class_and_code(exc_class_or_code):
        """"""Ensure that we register only exceptions as handler keys""""""
        if"
4802137,-7182192923927433215,6,BigQuery,"('GL_COMPRESSED_RGBA_ASTC_5x5_KHR',0x93B2)
GL_COMPRESSED_RGBA_ASTC_6x5_KHR=_C('GL_COMPRESSED_RGBA_ASTC_6x5_KHR',0x93B3)
GL_COMPRESSED_RGBA_ASTC_6x6_KHR=_C('GL_COMPRESSED_RGBA_ASTC_6x6_KHR',0x93B4)
GL_COMPRESSED_RGBA_ASTC_8x5_KHR=_C('GL_COMPRESSED"
6545018,-7516955456006330663,6,BigQuery," module.params['remote_as']
    description = module.params['description']
    fake_as = module.params['fake_as']
    dual_as = module.params['dual_as']
    conventional = module.params['conventional']
    route_refresh = module.params['route_refresh']
    four_byte_as = module.params['four_byte_as']
    is_ignore = module.params['is_ignore']
    local_if_name = module.params['local_if_name']
    ebgp_max_hop = module.params['ebgp_max_hop']
    valid_"
3802102,-6254560130177790278,6,BigQuery," bucket's website configuration, controlling how the
      service behaves when accessing bucket contents as a web site. See the
      Static Website Examples for more information.

  Fields:
    acl: Access controls on the bucket.
    billing: The bucket's billing configuration.
    cors: The bucket's Cross-Origin Resource Sharing (CORS) configuration.
    defaultEventBasedHold: The default value for event-based hold on newly
      created objects in this bucket. Event-based hold is a way to retain
      objects indefinitely until an event occurs, signified by the hold's
      release. After being released, such objects will be subject to bucket-
      level retention (if any)."
1740637,-8391460694170366385,7,BigQuery,"weights_sign_flip)

    assert_matrix_orthogonal(pls.x_weights_)
    assert_matrix_orthogonal(pls.y_weights_)

    assert_matrix_orthogonal(pls._x_scores)
    assert_matrix_orthogonal(pls._y_scores)


def test_convergence_fail():
    # Make sure ConvergenceWarning is raised if max_iter is too small
    d = load_linnerud()
    X = d.data
    Y = d.target
    pls_nipals = PLSCanonical("
7076693,-3611423641494615353,53,BigQuery," 8, e.g.,
        numbers smaller (in absolute value) than 5e-9 are represented as
        zero.

    See Also
    --------
    array2string, array_repr, set_printoptions

    Examples
    --------
    >>> np.array_str(np.arange(3))
    '[0 1 2]'

    """"""
    return array2string(a, max_line_width, precision, suppress_small,'', """", str)


def set_string_function(f, repr=True):
    """"""
    Set a Python function to be used when pretty printing arrays.

    Parameters
    ----------
    "
7455041,-8266726840634697721,17,BigQuery," max_option_string_index:

            # consume any Positionals preceding the next option
            next_option_string_index = min([
                index
                for index in option_string_indices
                if index >= start_index])
            if start_index!= next_option_string_index:
                positionals_end_index = consume_positionals(start_index)

                # only try to parse the next optional if we didn't consume
                # the option string during the positionals parsing
                if positionals_end_index > start_index:
                    start_index = positionals_end_index
                    continue
                else:
                    start_"
6485450,-5591474894672950789,55,BigQuery,"lo = phi_aj
            derphi_lo = derphi_aj
        i += 1
        if (i > maxiter):
            a_star = a_j
            val_star = phi_aj
            valprime_star = None
            break
    return a_star, val_star, valprime_star


#------------------------------------------------------------------------------
# Armijo line and scalar searches
#------------------------------------------------------------------------------

def line_search_armijo(f, xk, pk, gfk, old_fval, args=(), c1=1e-4, alpha0=1):
    """"""Minimize over alpha, the function ``"
5984999,8582683621940508731,62,BigQuery," items:
            f.write(""#define %s_%s %s\n"" % (prefix, k.upper(), v))
    f = open(""sre_constants.h"", ""w"")
    f.write(""""""\
/*
 * Secret Labs' Regular Expression Engine
 *
 * regular expression matching engine
 *
 * NOTE: This file is generated by sre_constants.py.  If you need
 * to change anything in here, edit sre_constants.py and run it.
 *
 * Copyright (c) 1997-2001 by Secret Labs AB.  All rights reserved."
378761,429447847312089550,75,BigQuery,"17\uff41\uff42\uff43\uff44\uff45\uff46\uff47\uff48\uff49\uff4a\uff4b\uff4c\uff4d\uff4e\uff4f\uff50\uff51\uff52\uff53\uff54\uff55\uff56\uff57\uff58\uff59\uff5a'

Lm = u'\u02b0\u02b1\u02b2\u02b3\u02b4\u02b5\u02b6\u02b7\u02b8\u02b9\u02ba\u"
3848688,6101757632673481124,178,BigQuery,":
                raise UnicodeError(""UTF-32 stream does not start with BOM"")
            return (output, consumed)
        return self.decoder(input, self.errors, final)

    def reset(self):
        codecs.BufferedIncrementalDecoder.reset(self)
        self.decoder = None

    def getstate(self):
        # additonal state info from the base class must be None here,
        # as it isn't passed along to the caller
        state = codecs.BufferedIncrementalDecoder.getstate(self)[0]
        # additional state info we pass to the caller:
        # 0: stream"
3044959,1785280698310547387,45,BigQuery,"
            debug.clear_breakpoint_here(filename, lineno)
        except:
            pass

    def clear_file_breaks(self):
        if self.breakpoints:
            text = self.text
            filename = self.io.filename
            if not filename:
                text.bell()
                return
            self.breakpoints = []
            text.tag_remove(""BREAK"", ""1.0"", END)
            try:
                debug = self.flist.pyshell.interp.debugger
                debug.clear_file_breaks(filename)
            except:
                pass

    def store_file_breaks(self"
2968932,105302906741609,24,BigQuery,"']
    firewall_ssh_local_ca_data = data['firewall_ssh_local_ca']
    filtered_data = filter_firewall_ssh_local_ca_data(firewall_ssh_local_ca_data)
    if firewall_ssh_local_ca_data['state'] == ""present"":
        return fos.set('firewall.ssh',
                       'local-ca',
                       data=filtered_data,
                       vdom=vdom)

    elif firewall_ssh_local_ca_data['state'] == ""absent"":
        return fos.delete('firewall.ssh',
                          'local"
2665327,-1213779832270657576,27,BigQuery,"/>"" in recv_xml:
                need_cfg = True
            else:
                re_find = re.findall(
                    r'.*<publicAsOnlyLimited>(.*)</publicAsOnlyLimited>.*', recv_xml)

                if re_find:
                    result[""public_as_only_limited""] = re_find
                    result[""vrf_name""] = vrf_name
                    result[""af_type""] = af_type
                    if re_find[0]!= public_as_only_limited:
                        need_cfg = True
                else:
                    need_cfg = True

        public_as_only_replace ="
4620588,-4354798057932883999,14,BigQuery,"ifiManager_methods(root_module, cls):
    ## arf-wifi-manager.h (module 'wifi'): ns3::ArfWifiManager::ArfWifiManager(ns3::ArfWifiManager const & arg0) [copy constructor]
    cls.add_constructor([param('ns3::ArfWifiManager const &', 'arg0')])
    ## arf-wifi-manager.h (module 'wifi'): ns3::ArfWifiManager::ArfWifiManager() [constructor]
    cls.add_constructor([])
    ## arf-wifi-"
1179655,-2497167674766642142,6,BigQuery,"=None, redirections=DEFAULT_MAX_REDIRECTS, connection_type=None):
        """""" Performs a single HTTP request.

        The 'uri' is the URI of the HTTP resource and can begin with either
        'http' or 'https'. The value of 'uri' must be an absolute URI.

        The'method' is the HTTP method to perform, such as GET, POST, DELETE,
        etc. There is no restriction on the methods allowed.

        The 'body' is the entity body to be sent with the request. It is a
        string object.

        Any extra headers that are to be sent with the request should be
        provided"
5514998,7761362786000644436,20,BigQuery,"'), [1, 2, 3, 4])
        assert_equal(a.ravel(order='F'), [1, 3, 2, 4])

    def test_reshape(self):
        # Tests reshape
        x = arange(4)
        x[0] = masked
        y = x.reshape(2, 2)
        assert_equal(y.shape, (2, 2,))
        assert_equal(y._mask.shape, (2, 2,))
        assert_equal(x.shape, (4,))
        assert_equal(x._mask.shape, (4,))

    def test_sort("
2210638,1805532880237350489,12,BigQuery,"-header.h (module 'lr-wpan'): uint8_t ns3::LrWpanMacHeader::GetKeyIdMode() const [member function]
    cls.add_method('GetKeyIdMode', 
                   'uint8_t', 
                   [], 
                   is_const=True)
    ## lr-wpan-mac-header.h (module 'lr-wpan'): uint32_t ns3::LrWpanMacHeader::GetKeyIdSrc32() const [member function]
    cls.add_method('GetKeyIdSrc32', 
                   'uint32_t', 
"
1786173,-5267739437426721726,14,BigQuery," const [member function]
    cls.add_method('Copy', 
                   'ns3::Ptr< ns3::AttributeValue >', 
                   [], 
                   is_const=True, is_virtual=True)
    ## double.h (module 'core'): bool ns3::DoubleValue::DeserializeFromString(std::string value, ns3::Ptr<ns3::AttributeChecker const> checker) [member function]
    cls.add_method('DeserializeFromString', 
                   'bool', 
                   [param('std::string', 'value'), param('ns3::Ptr< ns3::AttributeChecker const"
3331839,2213941759670137798,6,BigQuery,"return default
    """"""
    __slots__ = ()

    def __new__(cls, *args, **kwds):
        if cls is Generic:
            raise TypeError(""Type Generic cannot be instantiated; ""
                            ""it can be used only as a base class"")
        if super().__new__ is object.__new__ and cls.__init__ is not object.__init__:
            obj = super().__new__(cls)
        else:
            obj = super().__new__(cls, *args, **kwds)
        return obj

    @_tp_cache
    def __class_getitem"
378761,3176467902406062661,75,BigQuery,d\u4f3e\u4f3f\u4f40\u4f41\u4f42\u4f43\u4f44\u4f45\u4f46\u4f47\u4f48\u4f49\u4f4a\u4f4b\u4f4c\u4f4d\u4f4e\u4f4f\u4f50\u4f51\u4f52\u4f53\u4f54\u4f55\u4f56\u4f57\u4f58\u4f59\
378761,3702748406177629507,75,BigQuery,\ue703\ue704\ue705\ue706\ue707\ue708\ue709\ue70a\ue70b\ue70c\ue70d\ue70e\ue70f\ue710\ue711\ue712\ue713\ue714\ue715\ue716\ue717\ue718\ue719\ue71a\ue71b\ue71c\ue71d\ue71e\ue71f\ue720\ue721\ue722\ue723\ue724\ue725\ue726\ue727\ue728\ue729\ue72a\ue72b\ue72
1240697,7875703893727588651,9,BigQuery,"x381c1c24L, 0x57a6a6f1L, 0x73b4b4c7L, 0x97c6c651L,
    0xcbe8e823L, 0xa1dddd7cL, 0xe874749cL, 0x3e1f1f21L,
    0x964b4bddL, 0x61bdbddcL, 0x0d8b8b86L, 0x0f8a8a85L,
    0xe0707090L, 0x7c3e3e42L, 0x71b5b5c4"
1119851,3976471794059116895,9,BigQuery,"'span class=""bigsymbol""',], 
      u'\\boxed':[u'{$1}',u'f0{$1}',u'span class=""boxed""',], 
      u'\\cfrac':[u'[$p!]{$1}{$2}',u'f0{f3{(}f1{$1}f3{)/(}f2{$2}f3{)}}',u'span class=""fullfraction""',u'span class=""numerator align-$p""',u'span class=""denominator""',u'span class=""ignored""',], 
      "
378761,1722523310832056209,75,BigQuery,\uc064\uc065\uc066\uc067\uc068\uc069\uc06a\uc06b\uc06c\uc06d\uc06e\uc06f\uc070\uc071\uc072\uc073\uc074\uc075\uc076\uc077\uc078\uc079\uc07a\uc07b\uc07c\uc07d\uc07e\uc07f\uc080\uc081\uc082\uc083\uc084\uc085\uc086\uc087\uc088\uc089\uc08a
6545815,-1507987467072963414,18,BigQuery,"]

    K = FF(9)

    assert dmp_sqr([[K(3)], [K(4)]], 1, K) == [[K(6)], [K(7)]]


def test_dup_pow():
    assert dup_pow([], 0, ZZ) == [ZZ(1)]
    assert dup_pow([], 0, QQ) == [QQ(1)]

    assert dup_pow([], 1, ZZ) == []
    assert dup_pow([], 7, ZZ) == []

    assert dup_pow([ZZ(1)], 0, ZZ) =="
3741729,6950523231800843320,14,BigQuery," value in slotstate.items():
                setattr(object, key, value)

    def construct_python_object(self, suffix, node):
        # Format:
        #  !!python/object:module.name {... state... }
        instance = self.make_python_instance(suffix, node, newobj=True)
        yield instance
        deep = hasattr(instance, '__setstate__')
        state = self.construct_mapping(node, deep=deep)
        self.set_python_instance_state(instance, state)

    def construct_python_object_apply(self, suffix, node, newobj="
7500897,-8326638527730505902,94,BigQuery,"def find_library(parser, name):
    try:
        return parser.libraries[name]
    except KeyError:
        raise TemplateSyntaxError(
            ""'%s' is not a registered tag library. Must be one of:\n%s"" % (
                name, ""\n"".join(sorted(parser.libraries.keys())),
            ),
        )


def load_from_library(library, label, names):
    """"""
    Return a subset of tags and filters from a library.
    """"""
    subset = Library()
    for name in names:
        found = False
        if name in library.tags:
"
6818328,-8746505201571304133,34,BigQuery,"')
      #  if cw: self.write(iso.ARC_CW)
      #  else: self.write(iso.ARC_CCW)
      #  self.write_preps()
       #if (x!= None):
        #    self.write(iso.X + (self.fmt % x))
        #    self.x = x
        #if (y!= None):
        #    self.write(iso.Y + (self.fmt % y))
        #    self.y = y
        #if (z!= None):
         #   self.write"
3620598,-1019167471992481798,34,BigQuery,"_forcelist:
        A set of integer HTTP status codes that we should force a retry on.
        A retry is initiated if the request method is in ``method_whitelist``
        and the response status code is in ``status_forcelist``.

        By default, this is disabled with ``None``.

    :param float backoff_factor:
        A backoff factor to apply between attempts after the second try
        (most errors are resolved immediately by a second try without a
        delay). urllib3 will sleep for::

            {backoff factor} * (2 ** ({number of total retries} - 1))

        seconds. If the backoff"
2467897,-7553389877997450878,6,BigQuery,"(d)
        y = np.asarray(y)
        ret = add.reduce(d * (y[slice1]+y[slice2])/2.0, axis)
    return ret


#always succeed
def add_newdoc(place, obj, doc):
    """"""
    Adds documentation to obj which is in module place.

    If doc is a string add it to obj as a docstring

    If doc is a tuple, then the first element is interpreted as
       an attribute of obj and the second as the docstring
          (method, docstring)

    If doc is a list, then each element of the list should be a
       "
3589631,-1166607038307555371,12,BigQuery,"_ready())

class TestSoupSelector(TreeTest):

    HTML = """"""
<!DOCTYPE HTML PUBLIC ""-//W3C//DTD HTML 4.01//EN""
""http://www.w3.org/TR/html4/strict.dtd"">
<html>
<head>
<title>The title</title>
<link rel=""stylesheet"" href=""blah.css"" type=""text/css"" id=""l1"">
</head>
<body>
<custom-dashed-tag class=""dashed"" id=""dash1"">Hello there.</custom-dashed-tag>
<div id="""
7273962,2339938679919091490,26,BigQuery," logic and no backups inside.
class SourcesList(object):
    def __init__(self, module):
        self.module = module
        self.files = {}  # group sources by file
        # Repositories that we're adding -- used to implement mode param
        self.new_repos = set()
        self.default_file = self._apt_cfg_file('Dir::Etc::sourcelist')

        # read sources.list if it exists
        if os.path.isfile(self.default_file):
            self.load(self.default_file)

        # read sources.list.d
        for file in glob.iglob"
6121522,8303489970060829666,15,BigQuery," = X.shape
        n_classes = len(self.classes_)

        self.means_ = _class_means(X, y)
        if store_covariance:
            self.covariance_ = _class_cov(X, y, self.priors_)

        Xc = []
        for idx, group in enumerate(self.classes_):
            Xg = X[y == group, :]
            Xc.append(Xg - self.means_[idx])

        self.xbar_ = np.dot(self.priors_, self.means_)

"
2302519,1252121104303665131,29,BigQuery,"_ampersand_in_attribute_value_is_left_alone(self):
        self.assertSoupEquals('<a href=""http://example.org?a=1&amp;b=2;3""></a>')

    def test_entities_in_strings_converted_during_parsing(self):
        # Both XML and HTML entities are converted to Unicode characters
        # during parsing.
        text = ""<p>&lt;&lt;sacr&eacute;&#32;bleu!&gt;&gt;</p>""
        expected = u""<p>&lt;&lt;sacr\"
909044,2716939772859533220,99,BigQuery,"# Import them explicitly to cause an ImportError
# on non-Windows systems
from codecs import mbcs_encode, mbcs_decode
# for IncrementalDecoder, IncrementalEncoder,...
import codecs

### Codec APIs

encode = mbcs_encode

def decode(input, errors='strict'):
    return mbcs_decode(input, errors, True)

class IncrementalEncoder(codecs.IncrementalEncoder):
    def encode(self, input, final=False):
        return mbcs_en"
1726033,-4534332612784795673,16,BigQuery,"# Keep only the first `numtaps` coefficients in `out`, and multiply by
    # the window.
    out = out_full[:numtaps] * wind

    if ftype == 3:
        out[out.size // 2] = 0.0

    return out


def remez(numtaps, bands, desired, weight=None, Hz=1, type='bandpass',
          maxiter=25, grid_density=16):
    """"""
    Calculate the minimax optimal filter using the Remez exchange algorithm.

    Calculate the filter-coefficients for the finite impulse response
    (FIR) filter whose transfer function minim"
1195095,-9153209241634941543,8,BigQuery,"        id, name = column(""id""), column(""name"")
        stmt = select([id, name]).select_from(""user"")

    The above statement would produce SQL like::

        SELECT id, name FROM user

    :class:`.ColumnClause` is the immediate superclass of the schema-specific
    :class:`.Column` object.  While the :class:`.Column` class has all the
    same capabilities as :class:`.ColumnClause`, the :class:`.ColumnClause`
    class is usable by itself in those cases where behavioral requirements
    are limited to simple SQL expression generation.  The object has none of
    the associations with schema-level"
5590233,-8134583079571169993,9,BigQuery," = []
        for a0 in args[0]:
            for a1 in args[1]:
                result.extend(glob(os.path.join(a0, a1)))
    else:
        result = combine_paths(*(combine_paths(args[0], args[1]) + args[2:]))
    verbosity = kws.get('verbosity', 1)
    log.debug('(paths: %s)', ','.join(result))
    return result

language_map = {'c': 0, 'c++': 1, 'f77': 2, 'f90': 3}
inv"
3711130,460152371018052682,11,BigQuery,"x1', tbl.c.data,
                sqlite_where=and_(tbl.c.data > 5, tbl.c.data < 10))

The index will be rendered at create time as::

    CREATE INDEX test_idx1 ON testtbl (data)
    WHERE data > 5 AND data < 10

.. versionadded:: 0.9.9

.. _sqlite_dotted_column_names:

Dotted Column Names
-------------------

Using table or column names that explicitly have periods in them is
**not recommended**.   While this is generally a bad idea for relational
databases in general, as the dot"
5590233,-5359985013284481538,9,BigQuery,"('  libraries %s not found in %s', ','.join(libs),
                     lib_dirs)
        return info

    def check_libs2(self, lib_dirs, libs, opt_libs=[]):
        """"""If static or shared libraries are available then return
        their info dictionary.

        Checks each library for shared or static.
        """"""
        exts = self.library_extensions()
        info = self._check_libs(lib_dirs, libs, opt_libs, exts)
        if not info:
            log.info('  libraries %s not found in %s"
6773063,-368739473581283098,24,BigQuery,"(int64_t stream) [member function]
    cls.add_method('AssignStreams', 
                   'int64_t', 
                   [param('int64_t','stream')])
    ## backoff.h (module 'csma'): ns3::Time ns3::Backoff::GetBackoffTime() [member function]
    cls.add_method('GetBackoffTime', 
                   'ns3::Time', 
                   [])
    ## backoff.h (module 'csma'): void ns3::Backoff::IncrNumRetries() [member function]
    cls.add_method('"
6166978,3636658581716653690,24,BigQuery,"Install', 
                   'ns3::NetDeviceContainer', 
                   [param('ns3::NodeContainer', 'c')], 
                   is_const=True)
    ## waveform-generator-helper.h (module'spectrum'): ns3::NetDeviceContainer ns3::WaveformGeneratorHelper::Install(ns3::Ptr<ns3::Node> node) const [member function]
    cls.add_method('Install', 
                   'ns3::NetDeviceContainer', 
                   [param('ns3::Ptr< ns3::Node >', 'node')], 
                   is_const=True)
    ## waveform-"
5787245,-2668925818324980143,16,BigQuery,"-pn-realaudio',
        '.ram'    : 'application/x-pn-realaudio',
        '.ras'    : 'image/x-cmu-raster',
        '.rdf'    : 'application/xml',
        '.rgb'    : 'image/x-rgb',
        '.roff'   : 'application/x-troff',
        '.rtx'    : 'text/richtext',
        '.sgm'    : 'text/x-sgml',
        '.sgml'   : 'text/x-sgml',
        '.sh'     : 'application/x-sh',
        '.sh"
1149724,3503241639159515292,8,BigQuery,"elif not eng:
            # usual scientific notation: 1 digit on left of the point
            dotplace = 1
        elif self._int == '0':
            # engineering notation, zero
            dotplace = (leftdigits + 1) % 3 - 1
        else:
            # engineering notation, nonzero
            dotplace = (leftdigits - 1) % 3 + 1

        if dotplace <= 0:
            intpart = '0'
            fracpart = '.' + '0'*(-dotplace) + self._int
        elif dotplace >= len(self._int):
            intpart = self._int+'0'*("
1634463,1042559751627560021,9,BigQuery,"])
    return res


def _poly_fjacd(B, x, powers):
    b = B[1:]
    b.shape = (b.shape[0], 1)

    b = b * powers

    return np.sum(b * np.power(x, powers-1), axis=0)


def _exp_fcn(B, x):
    return B[0] + np.exp(B[1] * x)


def _exp_fjd(B, x):
    return B[1] * np.exp(B[1] * x)


def _exp_fjb(B"
2755633,-8660323245431326855,7,BigQuery,".name, dir, pre, suf)
        return file

    def test_basic(self):
        # _mkstemp_inner can create files
        self.do_create().write(""blat"")
        self.do_create(pre=""a"").write(""blat"")
        self.do_create(suf=""b"").write(""blat"")
        self.do_create(pre=""a"", suf=""b"").write(""blat"")
        self.do_create(pre=""aa"", suf="".txt"").write(""blat"")

    def test_basic_many("
3635009,3962294325434680625,8,BigQuery,"ERATING_COMPLETE:
                            # we're done with the child state, so clear it and continue
                            # back to the top of the loop to get the next task
                            state.tasks_child_state = None
                            continue
                else:
                    # First here, we check to see if we've failed anywhere down the chain
                    # of states we have, and if so we move onto the rescue portion. Otherwise,
                    # we check to see if we've moved past the end of the list of tasks. If so,
                    # we move into the always portion of the block, otherwise we get the next
                    # task from the list.
                    if self._check_"
3286740,-5575666430591873772,8,BigQuery," self._reader.poll
            if sys.platform == 'win32':
                self._wlock = None
            else:
                self._wlock = ctx.Lock()

        # Add possiblity to use custom reducers
        self._reducers = reducers

    # Use custom queue set/get state to be able to reduce the custom reducers
    def __getstate__(self):
        assert_spawning(self)
        return (self._reader, self._writer, self._reducers, self._rlock,
                self._wlock)

    def __setstate__(self, state):
        (self._reader, self"
1105025,-7859546582704049827,17,BigQuery,".train_log_dir)
      tf.gfile.DeleteRecursively(FLAGS.train_log_dir)
      tf.gfile.MakeDirs(FLAGS.train_log_dir)
    else:
      logging.info('Use already existing training directory %s',
                   FLAGS.train_log_dir)


def calculate_graph_metrics():
  param_stats = model_analyzer.print_model_analysis(
      tf.get_default_graph(),
      tfprof_options=model_analyzer.TRAINABLE_VARS_PARAMS_STAT_OPTIONS)
  return param_stats"
5393557,7768778624536987900,9,BigQuery,"(x_i) \right ) \right]^2

    Here, :math:`n` represents the number of dimensions and
    :math:`x_i \in [0, \pi]` for :math:`i = 1,..., n`.

    *Global optimum*: :math:`f(x) = 0` for :math:`x_i = 0` for
    :math:`i = 1,..., n`

   .. [1] Jamil, M. & Yang, X.-S. A Literature Survey of Benchmark Functions
    For Global Optimization Problems Int. Journal of Mathematical Modelling
    and Numerical Optim"
5590162,-7268524857834909551,8,BigQuery,"        self.xPost.hide()
        self.xRing.hide()
        self.xDisc.hide()
        self.yPost.hide()
        self.yRing.hide()
        self.yDisc.hide()
        self.zPost.hide()
        self.zRing.hide()
        self.zDisc.hide()

    def showHandle(self, handle):
        if handle == 'x-post':
            self.xPost.show()
        elif handle == 'x-ring':
            self.xRing.show()
        elif handle == 'x-disc':
            self.xDisc.show()
"
2650071,-8892669788940680681,15,BigQuery,"ged_state.commit_all(merged_dict, self.identity_map)

        if new_instance:
            merged_state.manager.dispatch.load(merged_state, None)
        return merged

    @classmethod
    def identity_key(cls, *args, **kwargs):
        return mapperutil.identity_key(*args, **kwargs)

    @classmethod
    def object_session(cls, instance):
        """"""Return the ``Session`` to which an object belongs.""""""

        return object_session(instance)

    def _validate_persistent(self, state):
        if"
2968294,-9001839425241575099,7,BigQuery,"densebasic import dmp_degree_in
from sympy.polys.densebasic import dmp_to_dict
from sympy.polys.densetools import dup_integrate
from sympy.polys.densetools import dmp_integrate
from sympy.polys.densetools import dmp_integrate_in
from sympy.polys.densetools import dup_diff
from sympy.polys.densetools import dmp_diff
from sympy.polys.densetools import dmp_diff_in
from sympy.polys.densetools import dup_"
378761,-4341360612647468318,75,BigQuery,b1\u59b2\u59b3\u59b4\u59b5\u59b6\u59b7\u59b8\u59b9\u59ba\u59bb\u59bc\u59bd\u59be\u59bf\u59c0\u59c1\u59c2\u59c3\u59c4\u59c5\u59c6\u59c7\u59c8\u59c9\u59ca\u59cb\u59cc\u59cd\u59ce\u59cf\u59d0\u59d1
7121576,682031286245881476,13,BigQuery,"k]:
							end = k - 1
							i = k
							break
				# If we just finished processing the star we cared
				# about, we exit the loop early.
				if foundTheRightStar:
					break
			# Move to the next element of the pattern.
			j += 1
			
		# extract the star words from the original, unmutilated input.
		if foundTheRightStar:
			#print string.join(pattern.split()[start:end+1])
			if starType =='star': return string.join(pattern.split()[start:end+1])
			elif starType == 'thatstar': return"
378761,-3612381712483738289,75,BigQuery,\uf6f7\uf6f8\uf6f9\uf6fa\uf6fb\uf6fc\uf6fd\uf6fe\uf6ff\uf700\uf701\uf702\uf703\uf704\uf705\uf706\uf707\uf708\uf709\uf70a\uf70b\uf70c\uf70d\uf70e\uf70f\uf710\uf711\uf712\uf713\uf714\uf715\uf716\uf717\uf718\uf719\uf71a\uf71b\uf71c\uf71d\uf71e\uf
2467897,1647758179988056182,6,BigQuery,"    >>> np.trim_zeros(a, 'b')
    array([0, 0, 0, 1, 2, 3, 0, 2, 1])

    The input data type is preserved, list/tuple in means list/tuple out.

    >>> np.trim_zeros([0, 1, 2, 0])
    [1, 2]

    """"""
    first = 0
    trim = trim.upper()
    if 'F' in trim:
        for i in filt:
            if i!= 0.:
                break
            else:
                first = first + 1
    last = len(filt)
    "
923177,-9213408128602930189,8,BigQuery,".write(b'abc\n')
        f.flush()
        self.assertTrue(f._rolled)
        with f:
            self.assertFalse(f.closed)
        self.assertTrue(f.closed)
        def use_closed():
            with f:
                pass
        self.assertRaises(ValueError, use_closed)

    def test_truncate_with_size_parameter(self):
        # A SpooledTemporaryFile can be truncated to zero size
        f = tempfile.SpooledTemporaryFile(max_size=10)
        f.write(b'abcdefg\"
847903,7365727163158066878,23,BigQuery,"lem for elem in current_val)
            exec(path + '= flattened_val')
        except BaseException:
            pass

    return data


def log_syslogd2_filter(data, fos):
    vdom = data['vdom']
    log_syslogd2_filter_data = data['log_syslogd2_filter']
    flattened_data = flatten_multilists_attributes(log_syslogd2_filter_data)
    filtered_data = filter_log_syslogd2_filter_data(flattened_data)
    return fos.set('log."
333190,-6623470534640548424,74,BigQuery," gettext_module.find('django', path, [to_locale(lang_code)]) is not None:
            return True
    return False
check_for_language = memoize(check_for_language, _checked_languages, 1)

def get_supported_language_variant(lang_code, supported=None, strict=False):
    """"""
    Returns the language-code that's listed in supported languages, possibly
    selecting a more generic variant. Raises LookupError if nothing found.

    If `strict` is False (the default), the function will look for an alternative
    country-specific variant when the currently checked is"
1483169,879056987516883784,12,BigQuery,"        """"""
        reuse = [a for a, j in self.alias_map.items()
                 if (reuse is None or a in reuse) and j == join]
        if reuse:
            self.ref_alias(reuse[0])
            return reuse[0]

        # No reuse is possible, so we need a new alias.
        alias, _ = self.table_alias(join.table_name, create=True)
        if join.join_type:
            if self.alias_map[join.parent_alias].join_type == LOUTER or join.nullable:
                join_type = LOUTER
            "
5984374,-1794888343335359851,9,BigQuery,"')
@dispatch.add_dispatch_support
@doc_controls.do_not_generate_docs
def sin(x):
  """"""Computes sin of x element-wise.

  Args:
      x: Tensor or variable.

  Returns:
      A tensor.
  """"""
  return math_ops.sin(x)


@keras_export('keras.backend.cos')
@dispatch.add_dispatch_support
@doc_controls.do_not_generate_docs
def cos(x):
  """"""Computes cos of x element-wise.

  Args:
      x:"
3331839,9059904203142182054,6,BigQuery,".

At large scale, the structure of the module is following:
* Imports and exports, all public names should be explicitly added to __all__.
* Internal helper functions: these should never be used in code outside this module.
* _SpecialForm and its instances (special forms): Any, NoReturn, ClassVar, Union, Optional
* Two classes whose instances can be type arguments in addition to types: ForwardRef and TypeVar
* The core of internal generics API: _GenericAlias and _VariadicGenericAlias, the latter is
  currently only used by Tuple and Callable. All subscripted types like X[int], Union[int, str],
  etc., are instances"
3453320,-2956196846413473279,11,BigQuery,"because it takes z' as being in standard time rather than the
daylight time we intend here), but returning it gives the real-life ""local
clock repeats an hour"" behavior when mapping the ""unspellable"" UTC hour into
tz.

When the input is 6:MM, z=1:MM and z.d=0, and we stop at once, again with
the 1:MM standard time spelling we want.

So how can this break?  One of the assumptions must be violated.  Two
possibilities:

1) [2] effectively says that y.s is invariant across all y belong to a given
   time zone.  This isn't true"
3257065,7140549304610568872,14,BigQuery,"UpTestData(cls):
        generic = NamedCategory.objects.create(name=""Generic"")
        cls.t1 = Tag.objects.create(name='t1', category=generic)

        n1 = Note.objects.create(note='n1', misc='foo', id=1)
        n2 = Note.objects.create(note='n2', misc='bar', id=2)

        e1 = ExtraInfo.objects.create(info='e1', note=n1)
        e2 = ExtraInfo.objects.create(info='e2', note=n2)

        cls.a1 = Author.objects.create"
408781,-3333086622433899841,25,BigQuery,"WindowsError
            exceptionList.append(
                (WindowsError, (1,'strErrorStr', 'filenameStr'),
                    {'args' : (1,'strErrorStr'),
                    'strerror' :'strErrorStr', 'winerror' : None,
                     'errno' : 1, 'filename' : 'filenameStr'})
            )
        except NameError:
            pass

        for exc, args, expected in exceptionList:
            try:
                e = exc(*args)
            except:
                print(""\nexc=%r, args=%r"" % (exc, args), file=sys.stderr"
4061119,2416834839174312218,36,BigQuery,"
                                 ""'bar' parameter lacking default value""):
        self.call(test, 1)

    def test(foo, *, bar, **bin):
        return foo, bar, bin
    self.assertEqual(self.call(test, 1, bar=2), (1, 2, {}))
    self.assertEqual(self.call(test, foo=1, bar=2), (1, 2, {}))
    self.assertEqual(self.call(test, 1, bar=2, spam='ham'),
                     (1, 2, {'spam': 'ham'}))
    self.assertEqual(self"
6212165,-332086777713352847,11,BigQuery,"')
            self.assertContains(response, '%Y-%m-%d %H:%M:%S')

    def test_disallowed_filtering(self):
        with patch_logger('django.security.DisallowedModelAdminLookup', 'error') as calls:
            response = self.client.get(
                ""%s?owner__email__startswith=fuzzy"" % reverse('admin:admin_views_album_changelist')
            )
            self.assertEqual(response.status_code, 400)
            self.assertEqual(len(calls), 1)

        # Filters"
7409348,-3672217530751062533,7,BigQuery,"
        self.assertEqual(__, isinstance(string, str))

    def test_triple_single_quotes_work_too(self):
        string = '''Bonjour tout le monde!'''
        self.assertEqual(__, isinstance(string, str))

    def test_raw_strings_are_also_strings(self):
        string = r""Konnichi wa, world!""
        self.assertEqual(__, isinstance(string, str))

    def test_use_single_quotes_to_create_string_with_double_quotes(self):
        string = '"
378761,-6041084966803777039,75,BigQuery,\u3f43\u3f44\u3f45\u3f46\u3f47\u3f48\u3f49\u3f4a\u3f4b\u3f4c\u3f4d\u3f4e\u3f4f\u3f50\u3f51\u3f52\u3f53\u3f54\u3f55\u3f56\u3f57\u3f58\u3f59\u3f5a\u3f5b\u3f5c\u3f5d\u3f5
1861984,-771719898144770282,22,BigQuery,"    urls ='regressiontests.urlpatterns_reverse.namespace_urls'

    def test_urlpattern_resolve(self):
        for path, name, app_name, namespace, func, args, kwargs in resolve_test_data:
            # Test legacy support for extracting ""function, args, kwargs""
            match_func, match_args, match_kwargs = resolve(path)
            self.assertEqual(match_func, func)
            self.assertEqual(match_args, args)
            self.assertEqual(match_kwargs, kwargs)

            # Test ResolverMatch capabilities"
3695899,15029694439626552,11,BigQuery,"=module, conf_str=conf_str)

                if state == ""present"":
                    if ""<data/>"" in recv_xml:
                        need_cfg = True
                    else:
                        re_find = re.findall(
                            r'.*<confedIdNumber>(.*)</confedIdNumber>.*', recv_xml)

                        if re_find:
                            result[""confed_id_number""] = re_find
                            if re_find[0]!= confed_id_number:
                                need_cfg = True
                        else:
                            need_cfg = True
                else:
                    if ""<data/>"" in recv"
4848498,-6942321810427372811,26,BigQuery,"Modified', http_date(mtime))
        ))
        start_response('200 OK', headers)
        return wrap_file(environ, f)


class DispatcherMiddleware(object):
    """"""Allows one to mount middlewares or applications in a WSGI application.
    This is useful if you want to combine multiple WSGI applications::

        app = DispatcherMiddleware(app, {
            '/app2':        app2,
            '/app3':        app3
        })
    """"""

    def __init__(self, app, mounts=None):
        self.app = app
        self.mounts = mounts or {}"
5347703,-3081178924980706794,6,BigQuery,".HtmlSelection_GetFromPos(*args, **kwargs)

    def GetToPos(*args, **kwargs):
        """"""GetToPos(self) -> Point""""""
        return _html.HtmlSelection_GetToPos(*args, **kwargs)

    def GetFromPrivPos(*args, **kwargs):
        """"""GetFromPrivPos(self) -> Point""""""
        return _html.HtmlSelection_GetFromPrivPos(*args, **kwargs)

    def GetToPrivPos(*args, **kwargs):
        """"""GetToPrivPos(self) -> Point""""""
        return _html.HtmlSelection_Get"
5938543,-8930458188368875236,9,BigQuery," < len_l and l[i + 3] == sep:
                                # Jan-01-99
                                ymd.append(l[i + 4])
                                i += 2

                            i += 2

                        elif (i + 4 < len_l and l[i + 1] == l[i + 3] =='' and
                              info.pertain(l[i + 2])):
                            # Jan of 01
                            # In this case, 01 is clearly year
                            if l[i + 4].isdigit():
                                # Convert it here to become unambiguous
                                value = int(l[i + 4])
                                year = str(info.con"
2210638,422911756149501125,12,BigQuery,"3::LrWpanPhyEnumeration status) [member function]
    cls.add_method('PdDataConfirm', 
                   'void', 
                   [param('ns3::LrWpanPhyEnumeration','status')])
    ## lr-wpan-mac.h (module 'lr-wpan'): void ns3::LrWpanMac::PdDataIndication(uint32_t psduLength, ns3::Ptr<ns3::Packet> p, uint8_t lqi) [member function]
    cls.add_method('PdDataIndication', 
                   'void"
3407501,-7588880459880196615,6,BigQuery,".multiprocess':    self.multiprocess,
            'wsgi.run_once':        self.run_once
        })
        for key, value in self.headers.to_list(self.charset):
            result['HTTP_%s' % key.upper().replace('-', '_')] = value
        if self.environ_overrides:
            result.update(self.environ_overrides)
        return result

    def get_request(self, cls=None):
        """"""Returns a request with the data.  If the request class is not
        specified :attr:`request_class"
2393317,-4115809149124196432,45,BigQuery,"        # This isn't a real primary/replica database, so restore the original from other
        dive = Book.objects.using('other').get(title='Dive into Python')

        # If you assign a FK object when the base object hasn't
        # been saved yet, you implicitly assign the database for the
        # base object.
        chris = Person(name=""Chris Mills"")
        html5 = Book(title=""Dive into HTML5"", published=datetime.date(2010, 3, 15))
        # initially, no db assigned
        self.assertEqual(chris._state.db, None)
        self.assertEqual("
2528237,-7685388986685029101,6,BigQuery,".
        if self._write_fut is None:  # IDLE -> WRITING
            assert self._buffer is None
            # Pass a copy, except if it's already immutable.
            self._loop_writing(data=bytes(data))
        elif not self._buffer:  # WRITING -> BACKED UP
            # Make a mutable copy which we can extend.
            self._buffer = bytearray(data)
            self._maybe_pause_protocol()
        else:  # BACKED UP
            # Append to buffer (also copies).
            self._buffer.extend(data)
            self._maybe_pause"
4726242,5618826356886201237,11,BigQuery,", url, body, headers):
        body = self.fixtures.load('zone_does_not_exist.json')

        return httplib.OK, body, {}, httplib.responses[httplib.OK]

    def _api_v2_zone_CREATE_ZONE_SUCCESS(self, method,
                                         url, body, headers):
        body = self.fixtures.load('create_zone_success.json')

        return httplib.OK, body, {}, httplib.responses[httplib.OK]

    def _api_v2_zone_CREATE_ZONE_"
7303681,931370128205177190,10,BigQuery," ""AS IS"" BASIS, WITHOUT
# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
# License for the specific language governing permissions and limitations
# under the License.

import re

""""""
Guidelines for writing new hacking checks

 - Use only for Nova specific tests. OpenStack general tests
   should be submitted to the common 'hacking' module.
 - Pick numbers in the range N3xx. Find the current test with
   the highest allocated number and then pick the next value.
 - Keep the test method code in the source file ordered based
   on the N3xx value.
 - List the new rule in the top level HACK"
378761,5118366388989162235,75,BigQuery,37c2\u37c3\u37c4\u37c5\u37c6\u37c7\u37c8\u37c9\u37ca\u37cb\u37cc\u37cd\u37ce\u37cf\u37d0\u37d1\u37d2\u37d3\u37d4\u37d5\u37d6\u37d7\u37d8\u37d9\u37da\u37db\u37dc\u37dd\u37de\u37df\u37e0\u37e1\u37e
2862961,5998127619222232693,38,BigQuery,"h (module 'internet'): int32_t ns3::Ipv4L3Protocol::GetInterfaceForAddress(ns3::Ipv4Address addr) const [member function]
    cls.add_method('GetInterfaceForAddress', 
                   'int32_t', 
                   [param('ns3::Ipv4Address', 'addr')], 
                   is_const=True, is_virtual=True)
    ## ipv4-l3-protocol.h (module 'internet'): int32_t ns3::Ipv4L3Protocol::GetInterfaceForDevice(ns3::Ptr<const ns3::"
6773063,-4335084430011833807,24,BigQuery," 'header'), param('uint32_t','size')])
    ## packet-metadata.h (module 'network'): void ns3::PacketMetadata::AddPaddingAtEnd(uint32_t end) [member function]
    cls.add_method('AddPaddingAtEnd', 
                   'void', 
                   [param('uint32_t', 'end')])
    ## packet-metadata.h (module 'network'): void ns3::PacketMetadata::AddTrailer(ns3::Trailer const & trailer, uint32_t size) [member function]
    cls.add_method('AddTrailer', 
"
7076043,-3642688232944091182,9,BigQuery,"OUND_SYSTEM_MENUPOPUP = 16
CSOUND_SYSTEM = 16
ALERT_SYSTEM_INFORMATIONAL = 1
ALERT_SYSTEM_WARNING = 2
ALERT_SYSTEM_ERROR = 3
ALERT_SYSTEM_QUERY = 4
ALERT_SYSTEM_CRITICAL = 5
CALERT_SYSTEM = 6
WINEVENT_OUTOFCONTEXT = 0
WINEVENT_SKIPOWNTHREAD = 1
WINEVENT_SKIPOWNPROCESS = 2
WINEVENT_INCONTEXT = 4
GUI_CAR"
2210638,8651423064744882592,12,BigQuery,"-wpan'): static ns3::TypeId ns3::LrWpanMacHeader::GetTypeId() [member function]
    cls.add_method('GetTypeId', 
                   'ns3::TypeId', 
                   [], 
                   is_static=True)
    ## lr-wpan-mac-header.h (module 'lr-wpan'): bool ns3::LrWpanMacHeader::IsAckReq() const [member function]
    cls.add_method('IsAckReq', 
                   'bool', 
                   [], 
                   is_const=True)
    ## l"
4909263,2958647386671545091,28,BigQuery,"056d', 0x1d400:u'a', 0x1d401:u'b',
0x1d402:u'c', 0x1d403:u'd', 0x1d404:u'e', 0x1d405:u'f',
0x1d406:u'g', 0x1d407:u'h', 0x1d408:u'i', 0x1d409:u'j',
0x1d40a:u'k', 0x1d40b:u'l', 0x1d40c:u'm', 0x1d40d:u'"
3726547,8005615268589507874,20,BigQuery,""")
            self.assertEquals(slot_update[0].get_shape(), var0.get_shape())
            self.assertFalse(slot_update[0] in variables.trainable_variables())

            slot[1] = adadelta_opt.get_slot(var1, ""accum"")
            self.assertEquals(slot[1].get_shape(), var1.get_shape())
            self.assertFalse(slot[1] in variables.trainable_variables())

            slot_update[1] = adadelta_opt.get_slot(var1, ""accum_update"")
            self.assertEquals"
7394712,1837427813285384748,12,BigQuery,". Note that we use here the bottleneck variant which has an
  extra bottleneck layer.

  When putting together two consecutive ResNet blocks that use this unit, one
  should use stride = 2 in the last unit of the first block.

  Args:
    inputs: A tensor of size [batch, height, width, channels].
    depth: The depth of the ResNet unit output.
    depth_bottleneck: The depth of the bottleneck layers.
    stride: The ResNet unit's stride. Determines the amount of downsampling of
      the units output compared to its input.
    rate: An integer, rate for atrous convolution.
    outputs"
6151456,-1856698051217053639,9,BigQuery,"(range(6), n))
        selectors = chain.from_iterable(repeat((0, 1)))
        self.assertEqual(list(compress(data, selectors)), [1,3,5] * n)
        self.assertRaises(TypeError, compress, None, range(6))      # 1st arg not iterable
        self.assertRaises(TypeError, compress, range(6), None)      # 2nd arg not iterable
        self.assertRaises(TypeError, compress, range(6))            # too few args
        self.assertRaises(TypeError, compress, range(6), None)      #"
6197705,-8824278587834978644,117,BigQuery,"%d/%Y', '%m/%d/%y', # '2006-10-25', '10/25/2006', '10/25/06'
    # '%b %d %Y', '%b %d, %Y',            # 'Oct 25 2006', 'Oct 25, 2006'
    # '%d %b %Y', '%d %b, %Y',            # '25 Oct 2006', '25 Oct, 2006'
    # '%B %d %Y', '%B %d, %Y',            # 'October 25 2006', 'October 25, 2006'
    # '%d %B %Y', '%"
6773063,6520187825928758349,24,BigQuery,"## random-variable-stream.h (module 'core'): uint32_t ns3::NormalRandomVariable::GetInteger(uint32_t mean, uint32_t variance, uint32_t bound) [member function]
    cls.add_method('GetInteger', 
                   'uint32_t', 
                   [param('uint32_t','mean'), param('uint32_t', 'variance'), param('uint32_t', 'bound')])
    ## random-variable-stream.h (module 'core'): double ns3::NormalRandomVariable::GetValue() [member function]
    cls.add_method('GetValue',"
3908989,7557089708519836111,12,BigQuery," = '{0}'.format(hex_string)
            return codecs.decode(hex_string, 'hex_codec')

        def _set_rsa_parameters(dest, src):
            # map OpenSSL parameter names to JsonWebKey property names
            conversion_dict = {
               'modulus': 'n',
                'publicExponent': 'e',
                'privateExponent': 'd',
                'prime1': 'p',
                'prime2': 'q',
                'exponent1': 'dp',
                'exponent2': 'dq',
                'coefficient': 'qi'
            }
"
3257065,1194708517975584962,14,BigQuery,"OUTER and q.alias_refcount[x.table_alias]]),
            1
        )

    def test_ticket17429(self):
        """"""
        Ensure that Meta.ordering=None works the same as Meta.ordering=[]
        """"""
        original_ordering = Tag._meta.ordering
        Tag._meta.ordering = None
        try:
            self.assertQuerysetEqual(
                Tag.objects.all(),
                ['<Tag: t1>', '<Tag: t2>', '<Tag: t3>', '<Tag: t4>', '<Tag: t5>'],
                ordered="
439465,6621368735005454894,52,BigQuery,"database.
        """"""
        fill_cache = self.query.select_related
        if isinstance(fill_cache, dict):
            requested = fill_cache
        else:
            requested = None
        max_depth = self.query.max_depth

        extra_select = self.query.extra_select.keys()
        aggregate_select = self.query.aggregate_select.keys()

        only_load = self.query.get_loaded_field_names()
        if not fill_cache:
            fields = self.model._meta.fields
            pk_idx = self.model._meta.pk_index"
2271380,6015621496854146737,9,BigQuery,"1.11.0.dev0_2329eae). These distributions will not be
        parsed properly
        downstream by Distribution and safe_version, so
        take an extra step and try to get the version number from
        the metadata file itself instead of the filename.
        """"""
        md_version = _version_from_file(self._get_metadata(self.PKG_INFO))
        if md_version:
            self._version = md_version
        return self


class DistInfoDistribution(Distribution):
    """"""
    Wrap an actual or potential sys.path entry
    w/metadata,.dist-info style.
"
4348622,103778154024463271,37,BigQuery,"dirs', [])
  library_dirs = _FixPaths(library_dirs)
  return library_dirs


def _GetLibraries(spec):
  """"""Returns the list of libraries for this configuration.

  Arguments:
    spec: The target dictionary containing the properties of the target.
  Returns:
    The list of directory paths.
  """"""
  libraries = spec.get('libraries', [])
  # Strip out -l, as it is not used on windows (but is needed so we can pass
  # in libraries that are assumed to be in the default library path).
  # Also remove duplicate entries, leaving only the last"
2467897,4599789150335466986,6,BigQuery,"
    normed : bool, optional
        This keyword is deprecated in NumPy 1.6.0 due to confusing/buggy
        behavior. It will be removed in NumPy 2.0.0. Use the ``density``
        keyword instead. If ``False``, the result will contain the
        number of samples in each bin. If ``True``, the result is the
        value of the probability *density* function at the bin,
        normalized such that the *integral* over the range is 1. Note
        that this latter behavior is known to be buggy with unequal bin
        widths; use ``density`` instead.
    weights : array_like, optional"
378761,-7048894231597095194,75,BigQuery,\u426a\u426b\u426c\u426d\u426e\u426f\u4270\u4271\u4272\u4273\u4274\u4275\u4276\u4277\u4278\u4279\u427a\u427b\u427c\u427d\u427e\u427f\u4280\u4281\u4282\u4283\u4284\u4285\u4286\u4287\u4288\u4289\u428a\u428b\u428c\u428d\u428e\u
5695703,6184586183114397256,6,BigQuery,"UT_JOYSTICK_BUTTON_C',
    'GLUT_JOYSTICK_BUTTON_D',
    'GLUT_JOYSTICK_POLL_RATE',
    'GLUT_KEY_DOWN',
    'GLUT_KEY_END',
    'GLUT_KEY_F1',
    'GLUT_KEY_F10',
    'GLUT_KEY_F11',
    'GLUT_KEY_F12',
    'GLUT_KEY_F2',
    'GLUT_KEY_F3',
    'GLUT_KEY_F4',
    'GLUT_KEY_"
1149724,3172816030981574461,8,BigQuery,"
    30
    >>> with localcontext(ExtendedContext):
   ...     print getcontext().prec
   ...
    9
    >>> print getcontext().prec
    28
    """"""
    if ctx is None: ctx = getcontext()
    return _ContextManager(ctx)


##### Decimal class #######################################################

class Decimal(object):
    """"""Floating point class for decimal arithmetic.""""""

    __slots__ = ('_exp','_int','_sign', '_is_special')
    # Generally, the value of the Decimal instance is given by
    #  (-1)**_sign * _"
2863065,-1043701582188460608,73,BigQuery,"ions.append(sql)
        expression_params.extend(params)
        sql, params = self.compile(self.rhs, compiler, connection)
        expressions.append(sql)
        expression_params.extend(params)
        # order of precedence
        expression_wrapper = '(%s)'
        sql = connection.ops.combine_duration_expression(self.connector, expressions)
        return expression_wrapper % sql, expression_params


class F(Combinable):
    """"""
    An object capable of resolving references to existing query objects.
    """"""
    def __init__(self, name):
        """"""
"
6166494,5912611620040538620,9,BigQuery,"=True, positive=True)
    pw = Piecewise((1, And(S(1) <= n, n <= nmax)), (0, True))
    assert Sum(pw, (n, 1, nmax)).doit() == Sum(pw, (n, 1, nmax))


def test_Product_doit():
    assert Product(n*Integral(a**2), (n, 1, 3)).doit() == 2 * a**9 / 9
    assert Product(n*Integral(a**2), (n, 1, 3)).doit(deep=False) == \
        6*Integral(a**"
4483664,3776776952748452406,11,BigQuery,"packages(module, packages):
    """"""Run emerge command against given list of atoms.""""""
    p = module.params

    if p['noreplace'] and not (p['update'] or p['state'] == 'latest'):
        for package in packages:
            if p['noreplace'] and not query_package(module, package, 'emerge'):
                break
        else:
            module.exit_json(changed=False, msg='Packages already present.')
        if module.check_mode:
            module.exit_json(changed=True, msg='Packages would be installed.')

    args = []
    emerge_"
5106590,2582648529958726937,29,BigQuery,"_path in self.symlinked_files:
            return self.log(""Skipping '%s' (already linked earlier)"" % path)
        # Delete the target file if needed or break
        if not self.delete_file(path, prefixed_path, source_storage):
            return
        # The full path of the source file
        source_path = source_storage.path(path)
        # Finally link the file
        if self.dry_run:
            self.log(""Pretending to link '%s'"" % source_path, level=1)
        else:
            self.log(""Linking '%s'"""
378761,6664523081830329581,75,BigQuery,\u5e73\u5e74\u5e75\u5e76\u5e77\u5e78\u5e79\u5e7a\u5e7b\u5e7c\u5e7d\u5e7e\u5e7f\u5e80\u5e81\u5e82\u5e83\u5e84\u5e85\u5e86\u5e87\u5e88\u5e89\u5e8a\u5e8b\u5e8c\u5e8d\u5e8
7561126,7969073601447711960,12,BigQuery,"i_paymentline.get('note',False),
            'journal':      ui_paymentline['journal_id'],
        }

    def create_from_ui(self, cr, uid, orders, context=None):
        # Keep only new orders
        submitted_references = [o['data']['name'] for o in orders]
        existing_order_ids = self.search(cr, uid, [('pos_reference', 'in', submitted_references)], context=context)
        existing_orders = self.read(cr, uid, existing_order_ids, ['pos_reference'], context=context)
        "
4303541,-8375457485791129787,309,BigQuery,"##############################################################################
#
#    OpenERP, Open Source Management Solution
#    Copyright (C) 2004-2009 Tiny SPRL (<http://tiny.be>).
#    Copyright (C) 2010-TODAY OpenERP S.A. (http://www.openerp.com)
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will"
6393730,8567551452642988995,10,BigQuery,"_inherit = 'account.invoice'

    def invoice_validate(self, cr, uid, ids, context=None):
        res = super(account_invoice, self).invoice_validate(cr, uid, ids, context=context)
        purchase_order_obj = self.pool.get('purchase.order')
        # read access on purchase.order object is not required
        if not purchase_order_obj.check_access_rights(cr, uid,'read', raise_exception=False):
            user_id = SUPERUSER_ID
        else:
            user_id = uid"
6515695,1863966051500582433,37,BigQuery,":
        onerror(os.listdir, path, sys.exc_info())
    for name in names:
        fullname = os.path.join(path, name)
        try:
            mode = os.lstat(fullname).st_mode
        except os.error:
            mode = 0
        if stat.S_ISDIR(mode):
            rmtree(fullname, ignore_errors, onerror)
        else:
            try:
                os.remove(fullname)
            except os.error:
                onerror(os.remove, fullname, sys.exc_info())
    try:
        os."
1573604,-6545334092022344607,7,BigQuery,"ables_collections=None,
                outputs_collections=None,
                trainable=True,
                scope=None):
  """"""Adds an N-D convolution followed by an optional batch_norm layer.

  It is required that 1 <= N <= 3.

  `convolution` creates a variable called `weights`, representing the
  convolutional kernel, that is convolved (actually cross-correlated) with the
  `inputs` to produce a `Tensor` of activations. If a `normalizer_fn` is
  provided (such as `batch_norm`), it is then applied. Otherwise, if
  `normalizer_fn`"
408781,-3862304576786496789,25,BigQuery,"def inner_raising_func():
            # Create some references in exception value and traceback
            local_ref = obj
            raise MyException(obj)

        # Qualified ""except"" with ""as""
        obj = MyObj()
        wr = weakref.ref(obj)
        try:
            inner_raising_func()
        except MyException as e:
            pass
        obj = None
        obj = wr()
        self.assertTrue(obj is None, ""%s"" % obj)

        # Qualified ""except"" without ""as""
        obj = MyObj()
        wr = weakref.ref(obj)
        try:
"
5211997,-5196310803547861675,9,BigQuery,"='str'), kms_key_name=dict(type='str'))),
            source_snapshot=dict(),
            source_snapshot_encryption_key=dict(type='dict', options=dict(raw_key=dict(type='str'), kms_key_name=dict(type='str'), sha256=dict(type='str'))),
        )
    )

    if not module.params['scopes']:
        module.params['scopes'] = ['https://www.googleapis.com/auth/compute']

    state = module.params['state']
    kind = 'compute#disk'

    "
2089204,3552395865795604991,13,BigQuery,"ango-users| replace:: :ref:`django-users <django-users-mailing-list>`
.. |django-core-mentorship| replace:: :ref:`django-core-mentorship <django-core-mentorship-mailing-list>`
.. |django-developers| replace:: :ref:`django-developers <django-developers-mailing-list>`
.. |django-announce| replace:: :ref:`django-announce <django-announce-mailing-list>`
.. |django-updates| replace:: :ref:`django-up"
6606711,-6580529415993564396,18,BigQuery,"amethod = lambda self: 3
    vereq(dir(a), astuff + ['adata', 'amethod'])

    # The same, but with new-style classes.  Since these have object as a
    # base class, a lot more gets sucked in.
    def interesting(strings):
        return [s for s in strings if not s.startswith('_')]

    class C(object):
        Cdata = 1
        def Cmethod(self): pass

    cstuff = ['Cdata', 'Cmethod']
    vereq(interesting(dir(C)), cstuff)

    c = C()
    vereq(interesting"
756545,7912157465793170160,6,BigQuery,"__(self, parser, parent=None, invokingState=-1):
            super(AutolevParser.EqualsContext, self).__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return AutolevParser.RULE_equals

        def enterRule(self, listener):
            if hasattr(listener, ""enterEquals""):
                listener.enterEquals(self)

        def exitRule(self, listener):
            if hasattr(listener, ""exitEquals""):
                listener.exitEquals(self)




    def equals(self):"
3135529,3375518263224503025,7,BigQuery," forwarded
                  to the backends configured with this forwarding rule.
                - You may specify a maximum of up to 5 ports.
            returned: success
            type: list
        subnetwork:
            description:
                - A reference to a subnetwork.
                - For internal load balancing, this field identifies the subnetwork that the load
                  balanced IP should belong to for this Forwarding Rule.
                - If the network specified is in auto subnet mode, this field is optional. However,
                  if the network is in custom subnet mode, a subnetwork must be specified.
                - This field is not used for external load balancing.
            returned: success
            type:"
1512768,1487637874597116582,6,BigQuery,"Plugin.ImageFileDirectory_v1`
        instance with the same data as is contained in the original
        :py:class:`~PIL.TiffImagePlugin.ImageFileDirectory_v2`
        instance.

        :returns: :py:class:`~PIL.TiffImagePlugin.ImageFileDirectory_v1`

        """"""

        ifd = cls(prefix=original.prefix)
        ifd._tagdata = original._tagdata
        ifd.tagtype = original.tagtype
        ifd.next = original.next  # an indicator for multipage tiffs
        return ifd

    def to_v2"
378761,-1237803005075659831,75,BigQuery,bd\u69be\u69bf\u69c0\u69c1\u69c2\u69c3\u69c4\u69c5\u69c6\u69c7\u69c8\u69c9\u69ca\u69cb\u69cc\u69cd\u69ce\u69cf\u69d0\u69d1\u69d2\u69d3\u69d4\u69d5\u69d6\u69d7\u69d8\u69d9\u69da\u69db\u69dc\u69dd\
377975,771389767903239130,8,BigQuery," NestingState instance which maintains information about
                   the current stack of nested blocks being parsed.
    error: A callable to which errors are reported, which takes 4 arguments:
           filename, line number, error level, and message
  """"""

  # Remove comments from the line, but leave in strings for now.
  line = clean_lines.lines[linenum]

  if Search(r'printf\s*\(.*"".*%[-+ ]?\d*q', line):
    error(filename, linenum, 'runtime/printf_format', 3,
          '%q in format strings is deprecated.  Use %ll instead.')

  if Search("
1786173,-2054759206762807561,14,BigQuery,"module 'network'): std::_List_iterator<ns3::Ptr<ns3::PbbTlv> > ns3::PbbMessage::TlvEnd() [member function]
    cls.add_method('TlvEnd', 
                  'std::_List_iterator< ns3::Ptr< ns3::PbbTlv > >', 
                   [])
    ## packetbb.h (module 'network'): std::_List_const_iterator<ns3::Ptr<ns3::PbbTlv> > ns3::PbbMessage::TlvEnd() const [member function]
    cls.add_method('TlvEnd',"
5197502,-6642703980942335408,31,BigQuery," 'Asia/Tbilisi',
 'Asia/Tehran',
 'Asia/Thimphu',
 'Asia/Tokyo',
 'Asia/Tomsk',
 'Asia/Ulaanbaatar',
 'Asia/Urumqi',
 'Asia/Ust-Nera',
 'Asia/Vientiane',
 'Asia/Vladivostok',
 'Asia/Yakutsk',
 'Asia/Yangon',
 'Asia/Yekaterinburg',
 'Asia/Yerevan',
 'Atlantic/Azores',
 'Atlantic/Bermuda',
 'Atlantic/Canary',
 'Atlantic/Cape_Verde',"
4620588,-4467590774947656659,14,BigQuery,"## minstrel-wifi-manager.h (module 'wifi'): ns3::MinstrelWifiManager::MinstrelWifiManager() [constructor]
    cls.add_constructor([])
    ## minstrel-wifi-manager.h (module 'wifi'): int64_t ns3::MinstrelWifiManager::AssignStreams(int64_t stream) [member function]
    cls.add_method('AssignStreams', 
                   'int64_t', 
                   [param('int64_t','stream')])
    ## minstrel-wifi-manager.h"
1119851,6798086121860628236,9,BigQuery," maths construction inside the FormulaProcessor.""

  def process(self, contents, index):
    ""Process an element inside a formula.""
    Trace.error('Unimplemented process() in'+ unicode(self))

  def __unicode__(self):
    ""Return a printable description.""
    return 'Maths processor'+ self.__class__.__name__

class FormulaProcessor(object):
  ""A processor specifically for formulas.""

  processors = []

  def process(self, bit):
    ""Process the contents of every formula bit, recursively.""
    self.processcontents(bit)
    self.process"
6773063,-6111220176002422828,24,BigQuery," nstime.h (module 'core'): static ns3::Time::Unit ns3::Time::GetResolution() [member function]
    cls.add_method('GetResolution', 
                   'ns3::Time::Unit', 
                   [], 
                   is_static=True)
    ## nstime.h (module 'core'): double ns3::Time::GetSeconds() const [member function]
    cls.add_method('GetSeconds', 
                   'double', 
                   [], 
                   is_const=True)
    ## nstime.h (module 'core'): int64_t ns"
6137113,-4291733818916069514,101,BigQuery,"Base64(self, binary):
    self.context.write()
    self.trans.write(QUOTE)
    self.trans.write(base64.b64encode(binary))
    self.trans.write(QUOTE)

  def writeJSONObjectStart(self):
    self.context.write()
    self.trans.write(LBRACE)
    self.pushContext(JSONPairContext(self))

  def writeJSONObjectEnd(self):
    self.popContext()
    self.trans.write(RBRACE)

  def writeJSONArrayStart(self):
    self.context.write()
    self."
6317932,8438774008143296211,23,BigQuery,"%s)' % _name, bygroups(Operator, Name.Function), '#pop'),
            (r'#', Name.Builtin, ('#pop','system-constant')),
            # System functions
            (words((
                'child', 'children', 'elder', 'eldest', 'glk', 'indirect','metaclass',
                'parent', 'random','sibling', 'younger', 'youngest'), suffix=r'\b'),
             Name.Builtin, '#pop'),
            # Metaclasses
            (r'(?i)(Class|Object|Routine|String)\b', Name."
439465,8561002770791776000,52,BigQuery,".e. has an order_by()
        clause or a default ordering on the model.
        """"""
        if self.query.extra_order_by or self.query.order_by:
            return True
        elif self.query.default_ordering and self.query.model._meta.ordering:
            return True
        else:
            return False
    ordered = property(ordered)

    @property
    def db(self):
        ""Return the database that will be used if this query is executed now""
        if self._for_write:
            return self._db or router.db_for_write(self.model)
        return"
1906625,-3570095793939114016,6,BigQuery,"get('seuser', None)
        serole    = params.get('serole', None)
        setype    = params.get('setype', None)
        selevel   = params.get('selevel', None)
        secontext = [seuser, serole, setype]

        if self.selinux_mls_enabled():
            secontext.append(selevel)

        default_secontext = self.selinux_default_context(path)
        for i in range(len(default_secontext)):
            if i is not None and secontext[i] == '_default':
                secontext[i"
2741113,-3928943812883984532,18,BigQuery," Python 2.5, an attempt to move the ""xml""
    # package implementation to a new package (""xmlcore"") proved painful.
    # The goal of this change was to allow applications to be able to
    # obtain and rely on behavior in the standard library implementation
    # of the XML support without needing to be concerned about the
    # availability of the PyXML implementation.
    #
    # While the existing import hackery in Lib/xml/__init__.py can cause
    # PyXML's _xmlpus package to supplant the ""xml"" package, that only
    # works because either implementation uses the ""xml"" package name for
    # imports."
5590233,-8507952237734573127,9,BigQuery,"        f = os.environ['HOME']
    except KeyError:
        pass
    else:
        user_file = os.path.join(f, fname)
        if os.path.isfile(user_file):
            filenames.append(user_file)

    # Local file
    if os.path.isfile(fname):
        filenames.append(os.path.abspath(fname))

    return filenames


def get_info(name, notfound_action=0):
    """"""
    notfound_action:
      0 - do nothing
      1 - display warning message
      "
1710093,6340985502109451570,7,BigQuery,"if ""'"" in s1:
            s1 = '""%s""' % s1.replace('""', ""&quot;"")
        else:
            s1 = ""'%s'"" % s1
    else:
        s1 = '""%s""' % s1
    return s1

def quote_python(inStr):
    s1 = inStr
    if s1.find(""'"") == -1:
        if s1.find('\n') == -1:
            return ""'%s'"" % s1
        else:
            return ""'''%s'''"" % s1
    else:
        if s"
5302511,5008099066583031136,10,BigQuery,"other.indices, dtype=idx_dtype),
           other.data,
           indptr, indices, data)

        A = self.__class__((data, indices, indptr), shape=self.shape)
        A.prune()

        return A

    def _divide_sparse(self, other):
        """"""
        Divide this matrix by a second sparse matrix.
        """"""
        if other.shape!= self.shape:
            raise ValueError('inconsistent shapes')

        r = self._binopt(other, '_eldiv_')

        if np.issubdtype(r.dtype,"
378761,4444254770346695728,75,BigQuery,5f74\u5f75\u5f76\u5f77\u5f78\u5f79\u5f7a\u5f7b\u5f7c\u5f7d\u5f7e\u5f7f\u5f80\u5f81\u5f82\u5f83\u5f84\u5f85\u5f86\u5f87\u5f88\u5f89\u5f8a\u5f8b\u5f8c\u5f8d\u5f8e\u5f8f
6030671,-4575131521411154243,81,BigQuery," relevant on Unix.)

        'export_symbols' is a list of symbols that the shared library will
        export.  (This appears to be relevant only on Windows.)

        'debug' is as for 'compile()' and 'create_static_lib()', with the
        slight distinction that it actually matters on most platforms (as
        opposed to 'create_static_lib()', which includes a 'debug' flag
        mostly for form's sake).

        'extra_preargs' and 'extra_postargs' are as for 'compile()' (except
        of course that they supply command-line arguments for the
        particular linker being"
5848295,-4011369546981915470,46,BigQuery,"itecture
 - ec2_association
 - ec2_attachTime
 - ec2_attachment
 - ec2_attachmentId
 - ec2_client_token
 - ec2_deleteOnTermination
 - ec2_description
 - ec2_deviceIndex
 - ec2_dns_name
 - ec2_eventsSet
 - ec2_group_name
 - ec2_hypervisor
 - ec2_id
 - ec2_image_id
 - ec2_instanceState
 - ec2_instance_type
 - ec2_ipOwnerId
 - ec2_ip_address
 - ec2_item
 - ec2_kernel
"
3498910,7820759924285303690,16,BigQuery,"ipud(a),b)
        a = [[0,1,2],
             [3,4,5]]
        b = [[3,4,5],
             [0,1,2]]
        assert_equal(flipud(a),b)

class TestRot90(TestCase):
    def test_basic(self):
        self.assertRaises(ValueError, rot90, ones(4))

        a = [[0,1,2],
             [3,4,5]]
        b1 = [[2,5],
              [1,4],
              [0,3]]
        b2 = [[5,"
3362764,-927645344551332971,22,BigQuery,"raising a ``:py:class: ~redis.exceptions.ConnectionError`` (as the default
    ``:py:class: ~redis.connection.ConnectionPool`` implementation does), it
    makes the client wait (""blocks"") for a specified number of seconds until
    a connection becomes available.

    Use ``max_connections`` to increase / decrease the pool size::

        >>> pool = BlockingConnectionPool(max_connections=10)

    Use ``timeout`` to tell it either how many seconds to wait for a connection
    to become available, or to block forever:

        # Block forever.
        >>> pool = BlockingConnectionPool(timeout=None)

"
923785,1474609704717480814,26,BigQuery," but TensorShape([32, 784]) is not compatible with
    TensorShape([4, 4]).

    Args:
      other: Another TensorShape.

    Returns:
      True iff `self` is compatible with `other`.

    """"""
    other = as_shape(other)
    if self._dims is not None and other.dims is not None:
      if self.ndims!= other.ndims:
        return False
      for x_dim, y_dim in zip(self._dims, other.dims):
        if not x_dim.is_compatible_with(y_dim):
          return False
"
1271180,9107872150248183920,264,BigQuery,"uE005'
    RETURN = '\uE006'
    ENTER = '\uE007'
    SHIFT = '\uE008'
    LEFT_SHIFT = '\uE008'
    CONTROL = '\uE009'
    LEFT_CONTROL = '\uE009'
    ALT = '\uE00A'
    LEFT_ALT = '\uE00A'
    PAUSE = '\uE00B'
    ESCAPE = '\uE00C'
    SPACE = '\uE00D'
    PAGE_UP = '\u"
7455041,-6813748951681471414,17,BigQuery,"_index

            # if we consumed all the positionals we could and we're not
            # at the index of an option string, there were extra arguments
            if start_index not in option_string_indices:
                strings = arg_strings[start_index:next_option_string_index]
                extras.extend(strings)
                start_index = next_option_string_index

            # consume the next optional and any arguments for it
            start_index = consume_optional(start_index)

        # consume any positionals following the last Optional
        stop_index = consume_positionals(start_index)

        # if we didn't"
1589403,-6054170984777705061,30,BigQuery,"
                raise TokenError(""EOF in multi-line string"", strstart)
            endmatch = endprog.match(line)
            if endmatch:
                pos = end = endmatch.end(0)
                yield (STRING, contstr + line[:end],
                       strstart, (lnum, end), contline + line)
                contstr, needcont = '', 0
                contline = None
            elif needcont and line[-2:]!= '\\\n' and line[-3:]!= '\\\r\n':
                yield (ERRORTOKEN, contstr + line,
                           strstart"
5544771,-4380081293178747491,9,BigQuery,":
            self.identifier = identifier
        if contribute is None:
            self.contribute = []
        else:
            self.contribute = contribute
        if metadataSchema is None:
            self.metadataSchema = []
        else:
            self.metadataSchema = metadataSchema
        self.language = language
    def factory(*args_, **kwargs_):
        if metaMetadata.subclass:
            return metaMetadata.subclass(*args_, **kwargs_)
        else:
            return metaMetadata(*args_, **kwargs_)
    factory = staticmethod(factory)
    def"
544871,6019097152145239217,9,BigQuery,"t' * ntabs +'' * nspaces + line[raw:]
        self.set_region(head, tail, chars, lines)

    def untabify_region_event(self, event):
        head, tail, chars, lines = self.get_region()
        tabwidth = self._asktabwidth()
        if tabwidth is None: return
        for pos in range(len(lines)):
            lines[pos] = lines[pos].expandtabs(tabwidth)
        self.set_region(head, tail, chars, lines)

    def toggle_tabs_event(self, event):
        if self"
877435,5261971440839593661,6,BigQuery,"    def test_clearsessions_command(self):
        """"""
        Test clearsessions command for clearing expired sessions.
        """"""
        self.assertEqual(0, self.model.objects.count())

        # One object in the future
        self.session['foo'] = 'bar'
        self.session.set_expiry(3600)
        self.session.save()

        # One object in the past
        other_session = self.backend()
        other_session['foo'] = 'bar'
        other_session.set_expiry(-3600)
        other_session.save()

        # Two sessions are"
4757059,-2470350576614382344,12,BigQuery,"ango import template

register = template.Library()

def prepopulated_fields_js(context):
    """"""
    Creates a list of prepopulated_fields that should render Javascript for
    the prepopulated fields for both the admin form and inlines.
    """"""
    prepopulated_fields = []
    if context['add'] and 'adminform' in context:
        prepopulated_fields.extend(context['adminform'].prepopulated_fields)
    if 'inline_admin_formsets' in context:
        for inline_admin_formset in context['inline"
7243417,1407723537948058740,10,BigQuery,"order_by = self.connection.ops.force_no_ordering()
                result.append('GROUP BY %s' % ', '.join(grouping))

            if having:
                result.append('HAVING %s' % having)
                params.extend(h_params)

            if order_by:
                ordering = []
                for _, (o_sql, o_params, _) in order_by:
                    ordering.append(o_sql)
                    params.extend(o_params)
                result.append('ORDER BY %s' % ', '.join(ordering))

            if with_limits:
                if self"
6818115,2724386548418913596,17,BigQuery," 200)
        self.assertEqual(list(res.context[""object_list""]), list(Author.objects.all()))
        self.assertIs(res.context[""author_list""], res.context[""object_list""])
        self.assertTemplateUsed(res, ""generic_views/list.html"")

    def test_template_name_suffix(self):
        res = self.client.get(""/list/authors/template_name_suffix/"")
        self.assertEqual(res.status_code, 200)
        self.assertEqual(list(res.context[""object_list""]), list(Author.objects"
1786173,1243181432345269419,14,BigQuery," (module 'network'): ns3::PacketTagList [class]
    module.add_class('PacketTagList')
    ## packet-tag-list.h (module 'network'): ns3::PacketTagList::TagData [struct]
    module.add_class('TagData', outer_class=root_module['ns3::PacketTagList'])
    ## packet-tag-list.h (module 'network'): ns3::PacketTagList::TagData::TagData_e [enumeration]
    module.add_enum('TagData_e', ['MAX_SIZE'], outer_class=root_module['ns"
378761,7683872517104541061,75,BigQuery,4\uc6e5\uc6e6\uc6e7\uc6e8\uc6e9\uc6ea\uc6eb\uc6ec\uc6ed\uc6ee\uc6ef\uc6f0\uc6f1\uc6f2\uc6f3\uc6f4\uc6f5\uc6f6\uc6f7\uc6f8\uc6f9\uc6fa\uc6fb\uc6fc\uc6fd\uc6fe\uc6ff\uc700\uc701\uc702\uc703\uc704\uc705\uc706\uc707\uc
6606711,-1525870900409859472,18,BigQuery,"(0), 0L)
    coerce(0, L(0))
    coerce(0L, L(0))
    class F(float): pass
    coerce(F(0), 0)
    coerce(F(0), 0L)
    coerce(F(0), 0.)
    coerce(0, F(0))
    coerce(0L, F(0))
    coerce(0., F(0))
    class C(complex): pass
    coerce(C(0), 0)
    coerce(C(0), 0L)
    coer"
771885,-5114220714488498072,8,BigQuery,".u1)

        # Request a page that requires a login
        response = self.client.get('/login_protected_view/')
        self.assertEqual(response.status_code, 200)
        self.assertEqual(response.context['user'].username, 'testclient')

    def test_view_with_method_login(self):
        ""Request a page that is protected with a @login_required method""

        # Get the page without logging in. Should result in 302.
        response = self.client.get('/login_protected_method_view/')
        self.assertRedirects(response, '/accounts"
3803133,-87131106702949322,71,BigQuery,"dist)
                        skipped[dist] = 1
                    continue

                if dist in req and (dist.precedence<=SOURCE_DIST or not source):
                    return dist

        if force_scan:
            self.prescan()
            self.find_packages(requirement)
            dist = find(requirement)

        if local_index is not None:
            dist = dist or find(requirement, local_index)

        if dist is None and self.to_scan is not None:
            self.prescan()
            dist = find(requirement)

        if dist is None and not force_scan:
            self.find_"
3695899,-6633007865322170096,11,BigQuery,"""
            cmds.append(cmd)

        confed_id_number = module.params['confed_id_number']
        if confed_id_number:
            conf_str += ""<confedIdNumber>%s</confedIdNumber>"" % confed_id_number

            cmd = ""confederation id %s"" % confed_id_number
            cmds.append(cmd)

        confed_nonstanded = module.params['confed_nonstanded']
        if confed_nonstanded!= 'no_use':
            conf_str += ""<confedNonstanded>%s</confedNonstanded"
4559385,8338232356387326929,7,BigQuery," Chebyshev series coefficients representing their product.

    See Also
    --------
    chebadd, chebsub, chebdiv, chebpow

    Notes
    -----
    In general, the (polynomial) product of two C-series results in terms
    that are not in the Chebyshev polynomial basis set.  Thus, to express
    the product as a C-series, it is typically necessary to ""reproject""
    the product onto said basis set, which typically produces
    ""unintuitive live"" (but correct) results; see Examples section below.

    Examples
    --------
    >>> from numpy.polynomial import chebys"
378761,2610684205330423044,75,BigQuery,u4d53\u4d54\u4d55\u4d56\u4d57\u4d58\u4d59\u4d5a\u4d5b\u4d5c\u4d5d\u4d5e\u4d5f\u4d60\u4d61\u4d62\u4d63\u4d64\u4d65\u4d66\u4d67\u4d68\u4d69\u4d6a\u4d6b\u4d6c\u4d6d\u4d6e
1588672,4829073821234402809,7,BigQuery,"', [], {'related_name': ""'images'"", 'to': u""orm['catalogue.Product']""})
        },
        u'catalogue.productrecommendation': {
            'Meta': {'unique_together': ""(('primary','recommendation'),)"", 'object_name': 'ProductRecommendation'},
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
            'primary': ('django.db.models.fields.related.ForeignKey', [], {'related_name': ""'primary_recommendations'"", 'to': u""orm['"
6454003,-477332812819567801,6,BigQuery,"def match(self, ttype, values, regex=False):
        """"""Checks whether the token matches the given arguments.

        *ttype* is a token type. If this token doesn't match the given token
        type.
        *values* is a list of possible values for this token. The values
        are OR'ed together so if only one of the values matches ``True``
        is returned. Except for keyword tokens the comparsion is
        case-sensitive. For convenience it's ok to pass in a single string.
        If *regex* is ``True`` (default is ``False``) the given values are
        treated as regular expressions.
        """""""
5590233,2248711654224519482,9,BigQuery,"                  default_include_dirs=default_include_dirs,
                  verbosity=1,
                  ):
        self.__class__.info = {}
        self.local_prefixes = []
        defaults = {}
        defaults['libraries'] = ''
        defaults['library_dirs'] = os.pathsep.join(default_lib_dirs)
        defaults['include_dirs'] = os.pathsep.join(default_include_dirs)
        defaults['src_dirs'] = os.pathsep.join(default_src_dirs)
        defaults['search_static_first"
787380,-628640199848362885,13,BigQuery,"
    assert x.is_imaginary is False
    assert x.is_noninteger is False
    assert x.is_number is False


def test_zero():
    z = Integer(0)
    assert z.is_commutative is True
    assert z.is_integer is True
    assert z.is_rational is True
    assert z.is_algebraic is True
    assert z.is_transcendental is False
    assert z.is_real is True
    assert z.is_complex is True
    assert z.is_noninteger is False
    assert z.is_irrational is False
    assert z."
1680404,-905579477473342798,46,BigQuery,"()
            self._writers = set()

        def register(self, fileobj, events, data=None):
            key = super(SelectSelector, self).register(fileobj, events, data)
            if events & EVENT_READ:
                self._readers.add(key.fd)
            if events & EVENT_WRITE:
                self._writers.add(key.fd)
            return key

        def unregister(self, fileobj):
            key = super(SelectSelector, self).unregister(fileobj)
            self._readers.discard(key.fd)
            self._writers.discard(key.fd)"
5211629,-5548217775120995651,6,BigQuery," Jacobian
        by finite differences (required for this solver).
    t0 : float
        Initial time.
    y0 : array_like, shape (n,)
        Initial state.
    t_bound : float
        Boundary time - the integration won't continue beyond it. It also
        determines the direction of the integration.
    max_step : float, optional
        Maximum allowed step size. Default is np.inf, i.e. the step size is not
        bounded and determined solely by the solver.
    rtol, atol : float and array_like, optional
        Relative and absolute tolerances. The solver keeps the local"
4347963,2681394836402306626,8,BigQuery," position if necessary
        tps = np.r_[0, tps]
        fps = np.r_[0, fps]
        thresholds = np.r_[thresholds[0] + 1, thresholds]

    if fps[-1] <= 0:
        warnings.warn(""No negative samples in y_true, ""
                      ""false positive value should be meaningless"",
                      UndefinedMetricWarning)
        fpr = np.repeat(np.nan, fps.shape)
    else:
        fpr = fps / fps[-1]

    if tps[-1] <= 0:
        warnings.warn("""
998662,-4817560737115140326,7,BigQuery,"self.assertHTMLEqual(
            w.render('test', 'test'),
            '<select multiple=""multiple"" name=""test"" class=""selectfilterstacked"" '
            'data-field-name=""test\\"" data-is-stacked=""1"">\n</select>'
        )


class AdminDateWidgetTest(SimpleTestCase):
    def test_attrs(self):
        w = widgets.AdminDateWidget()
        self.assertHTMLEqual(
            w.render('test', datetime(2007, 12, 1, 9, 30)),
            '<input value=""2007-12-01"" type=""text"""
2862961,-4969919950469405358,38,BigQuery,"
    ## event-id.h (module 'core'): ns3::EventId [class]
    module.add_class('EventId', import_from_module='ns.core')
    ## hash.h (module 'core'): ns3::Hasher [class]
    module.add_class('Hasher', import_from_module='ns.core')
    ## inet6-socket-address.h (module 'network'): ns3::Inet6SocketAddress [class]
    module.add_class('Inet6SocketAddress', import_from_module='ns.network')
    ## inet6-socket-address.h ("
4832654,5130176483403329733,7,BigQuery," the file given as f's argument, one
or more test names per line.  Whitespace is ignored.  Blank lines and
lines beginning with '#' are ignored.  This is especially useful for
whittling down failures involving interactions among tests.

-L causes the leaks(1) command to be run just before exit if it exists.
leaks(1) is available on Mac OS X and presumably on some other
FreeBSD-derived systems.

-R runs each test several times and examines sys.gettotalrefcount() to
see if the test appears to be leaking references.  The argument should
be of the form stab:run:fname where'stab'"
7409348,6288750958288989255,7,BigQuery,"quotes(self):
        string = ""Don't""
        self.assertEqual(__, string)
    
    def test_use_backslash_for_escaping_quotes_in_strings(self):
        a = ""He said, \""Don't\""""
        b = 'He said, ""Don\'t""'
        self.assertEqual(__, (a == b))

    def test_use_backslash_at_the_end_of_a_line_to_continue_onto_the_next_line(self):
        string = ""It was the best of times,\n\
It was the worst"
892724,-1674333486296152804,6,BigQuery,"decimals = 9

        rs = self.dtc.convert(
            Timestamp('2012-1-1 01:02:03', tz='UTC'), None, None)
        xp = converter.dates.date2num(Timestamp('2012-1-1 01:02:03', tz='UTC'))
        tm.assert_almost_equal(rs, xp, decimals)

        rs = self.dtc.convert(
            Timestamp('2012-1-1 09:02:03', tz='Asia/Hong_Kong'), None, None)
        tm.assert_almost_equal(rs, xp, decim"
453645,-1055391968851839308,7,BigQuery," hereby granted, free of charge, to any person obtaining a copy of
this software and associated documentation files (the ""Software""), to deal in
the Software without restriction, including without limitation the rights to
use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies
of the Software, and to permit persons to whom the Software is furnished to do
so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED ""AS IS"", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF"
3634768,-1368509803209308922,6,BigQuery,"])
    ## global-router-interface.h: ns3::GlobalRoutingLSA::GlobalRoutingLSA(ns3::GlobalRoutingLSA & lsa) [constructor]
    cls.add_constructor([param('ns3::GlobalRoutingLSA &', 'lsa')])
    ## global-router-interface.h: uint32_t ns3::GlobalRoutingLSA::AddAttachedRouter(ns3::Ipv4Address addr) [member function]
    cls.add_method('AddAttachedRouter', 
                   'uint32_t', 
                   [param('ns3::I"
1060270,3815390828501203252,11,BigQuery," may adversely affect ARPACK convergence')
    n = A.shape[0]

    if k <= 0 or k >= n:
        raise ValueError(""k=%d must be between 1 and ndim(A)-1=%d""
                         % (k, n - 1))

    if sigma is None:
        matvec = _aslinearoperator_with_dtype(A).matvec

        if OPinv is not None:
            raise ValueError(""OPinv should not be specified ""
                             ""with sigma = None."")
        if OPpart is not None:
            raise ValueError(""OPpart should not be specified with ""
                             ""sigma"
377975,4013380857830024902,8,BigQuery,"
        not Match(r'[A-Z_]+$', function_name)):
      starting_func = True

  if starting_func:
    body_found = False
    for start_linenum in xrange(linenum, clean_lines.NumLines()):
      start_line = lines[start_linenum]
      joined_line +='' + start_line.lstrip()
      if Search(r'(;|})', start_line):  # Declarations and trivial functions
        body_found = True
        break                              #... ignore
      elif Search(r'{', start_line):
        body_found ="
2332647,-6949302436183597663,13,BigQuery," in self._writes])


    def getReaders(self):
        """"""
        Implement L{IReactorFDSet.getReaders}.
        """"""
        return [self._selectables[fd] for fd in self._reads]


    def getWriters(self):
        """"""
        Implement L{IReactorFDSet.getWriters}.
        """"""
        return [self._selectables[fd] for fd in self._writes]


    def doKEvent(self, timeout):
        """"""
        Poll the kqueue for new events.
        """"""
        if timeout is None:
            timeout ="
3105626,-5374475605725409251,22,BigQuery,"_to_datafile(""_lstm_ops.so""))

LayerRNNCell = rnn_cell_impl.LayerRNNCell  # pylint: disable=invalid-name


# pylint: disable=invalid-name
def _lstm_block_cell(x,
                     cs_prev,
                     h_prev,
                     w,
                     b,
                     wci=None,
                     wcf=None,
                     wco=None,
                     forget_bias=None,
                     cell_clip=None,
                     use_peephole=None,
                     name=None):
  r"""""""
2453726,-4706470210702374984,49,BigQuery," jo in offsets
                                     if 0 <= i+io < m and
                                        0 <= j+jo < n]
                succs.append(s)

    # Generate solutions.
    def solve(self):
        self._init_board()
        for x in conjoin(self.squaregenerators):
            yield x

    def printsolution(self, x):
        m, n = self.m, self.n
        assert len(x) == m*n
        w = len(str(m*n))
        format = ""%"" + str(w) + ""d""

        squares = [[None] * n for i"
3332938,-1394549090111212287,227,BigQuery," & Langford, J.C. A global geometric
           framework for nonlinear dimensionality reduction. Science 290 (5500)
    """"""

    def __init__(self, n_neighbors=5, n_components=2, eigen_solver='auto',
                 tol=0, max_iter=None, path_method='auto',
                 neighbors_algorithm='auto'):

        self.n_neighbors = n_neighbors
        self.n_components = n_components
        self.eigen_solver = eigen_solver
        self.tol = tol
        self.max"
1119851,-5874105717256348506,9,BigQuery,"�', u'\\TH':u'Þ', 
      u'\\aa':u'å', u'\\ae':u'æ', u'\\alpha':u'α', u'\\beta':u'β', 
      u'\\delta':u'δ', u'\\dh':u'ð', u'\\digamma':u'ϝ', u'\\epsilon':u'ϵ', 
      u'\\eta':u'η', u'\\eth':u'ð', u'\\gamma':u'γ', u'\\i':u'ı', 
      u'\\imath':u'"
271691,-2201108807704914495,7,BigQuery,"/or modify
# it under the terms of the GNU Affero General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Booktype is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU Affero General Public License for more details.
#
# You should have received a copy of the GNU Affero General Public License
# along with Booktype.  If not, see <http://www.gnu.org/lic"
2348168,6836823894601688798,42,BigQuery,"le.loads(pickle.dumps(a)))
        assert_equal(a[0], pickle.loads(pickle.dumps(a[0])))

    def test_objview_record(self):
        # https://github.com/numpy/numpy/issues/2599
        dt = np.dtype([('foo', 'i8'), ('bar', 'O')])
        r = np.zeros((1,3), dtype=dt).view(np.recarray)
        r.foo = np.array([1, 2, 3])  # TypeError?

        # https://github.com/numpy"
5544759,340294704730093193,9,BigQuery," [
i(0,0,'WET'),
i(3600,0,'CET'),
i(7200,3600,'CEST'),
i(3600,0,'CET'),
i(7200,3600,'CEST'),
i(3600,0,'CET'),
i(7200,3600,'CEST'),
i(3600,0,'CET'),
i(7200,3600,'CEST'),
i(3600,0,'CET'),
i(7200,3600,'CEST'),
i(3600,0,'CET'),
i(7200,3600,'CEST'),
"
5878534,9016169891442503390,28,BigQuery," keyword arg ""verbose"" prints lots of stuff if true, prints
    only failures if false; by default, it's true iff ""-v"" is in sys.argv.

    Optional keyword arg ""report"" prints a summary at the end when true,
    else prints nothing at the end.  In verbose mode, the summary is
    detailed, else very brief (in fact, empty if all tests passed).

    Optional keyword arg ""optionflags"" or's together module constants,
    and defaults to 0.  Possible values (see the docs for details):

        DONT_ACCEPT_TRUE_FOR_1
        DONT_ACCEPT_BLANKLINE"
2862961,-3265491935606637496,38,BigQuery,"    cls.add_binary_comparison_operator('!=')
    cls.add_output_stream_operator()
    cls.add_binary_comparison_operator('==')
    ## ipv4-interface-address.h (module 'internet'): ns3::Ipv4InterfaceAddress::Ipv4InterfaceAddress() [constructor]
    cls.add_constructor([])
    ## ipv4-interface-address.h (module 'internet'): ns3::Ipv4InterfaceAddress::Ipv4InterfaceAddress(ns3::Ipv4Address local, ns3::Ipv4Mask"
2665327,-4378715109031734623,27,BigQuery,"    if public_as_only_replace!= 'no_use':
        proposed[""public_as_only_replace""] = public_as_only_replace
    if public_as_only_skip_peer_as!= 'no_use':
        proposed[""public_as_only_skip_peer_as""] = public_as_only_skip_peer_as
    if route_limit:
        proposed[""route_limit""] = route_limit
    if route_limit_percent:
        proposed[""route_limit_percent""] = route_limit_percent
    if route_limit_type:
        proposed[""route_limit_type"
7137437,6583895602163059504,56,BigQuery,"            return False
        else:
            return self.setup_instance(instance)

    def setup_instance(self, instance, state=None):
        self._adapted.initialize_instance_dict(self.class_, instance)

        if state is None:
            state = self._state_constructor(instance, self)

        # the given instance is assumed to have no state
        self._adapted.install_state(self.class_, instance, state)
        return state

    def teardown_instance(self, instance):
        self._adapted.remove_state(self.class_, instance)

    def has_state("
1240710,-7915748339443565120,9,BigQuery,"2_out.values, values)
      self.assertAllEqual(sp2_out.dense_shape, shape)

  def testFeedSparsePlaceholderConstantShape(self):
    with session.Session() as s:
      indices = np.array([[3, 2, 0], [4, 5, 1]]).astype(np.int64)
      values = np.array([1.0, 2.0]).astype(np.float32)
      shape = np.array([7, 9, 2]).astype(np.int64)
      sp = array_ops.sparse_placeholder(dtype=np"
3226380,-5208550171606002007,6,BigQuery," a specific IP address
    if args.source:
        source = args.source
        socket.socket = bound_socket

    if not args.simple:
        print_('Retrieving speedtest.net configuration...')
    try:
        config = getConfig()
    except URLError:
        print_('Cannot retrieve speedtest configuration')
        sys.exit(1)

    if not args.simple:
        print_('Retrieving speedtest.net server list...')
    if args.list or args.server:
        servers = closestServers(config['client'], True)
        if args.list:
            serverList ="
5181833,-2518773907783279050,8,BigQuery,"True))

    def namesAndDescriptions(self, all=False):
        """"""Return attribute names and descriptions defined by interface.""""""
        if not all:
            return self.__attrs.items()

        r = {}
        for base in self.__bases__[::-1]:
            r.update(dict(base.namesAndDescriptions(all)))

        r.update(self.__attrs)

        return r.items()

    def getDescriptionFor(self, name):
        """"""Return the attribute description for the given name.""""""
        r = self.get(name)
        if r is not None:
            return"
1664238,-3383104423420260643,6,BigQuery,"1] == rgb[2]:
            return [rgb[0], Op.setgray_stroke]
        else:
            return list(rgb[:3]) + [Op.setrgb_stroke]

    def fillcolor_cmd(self, rgb):
        if rgb is None or rcParams['pdf.inheritcolor']:
            return []
        elif rgb[0] == rgb[1] == rgb[2]:
            return [rgb[0], Op.setgray_nonstroke]
        else:
            return list(rgb[:3]) + [Op.setrgb_nonstroke]

    def push(self"
2923768,-5629366288713450906,43,BigQuery,"), measure,
                            err_msg=""%s is not representation invariant ""
                                    ""with mix list and np-array-column""
                                    % name)

        assert_almost_equal(metric(y1_column, y2_list), measure,
                            err_msg=""%s is not representation invariant ""
                                    ""with mix list and np-array-column""
                                    % name)

        # These mix representations aren't allowed
        assert_raises(ValueError, metric, y1_1d, y2_row)
        assert_raises(ValueError, metric, y1_row, y2_1d)
"
667015,-5995816596968914880,118,BigQuery," gettext_noop


def gettext(message):
    return _trans.gettext(message)


def ngettext(singular, plural, number):
    return _trans.ngettext(singular, plural, number)


def ugettext(message):
    return _trans.ugettext(message)


def ungettext(singular, plural, number):
    return _trans.ungettext(singular, plural, number)


def pgettext(context, message):
    return _trans.pgettext(context, message)


def npgettext(context, singular, plural, number):
    return"
2665144,-7563368202422772102,16,BigQuery,"esian product.

Calculus
--------
- `lagder` -- differentiate a Laguerre series.
- `lagint` -- integrate a Laguerre series.

Misc Functions
--------------
- `lagfromroots` -- create a Laguerre series with specified roots.
- `lagroots` -- find the roots of a Laguerre series.
- `lagvander` -- Vandermonde-like matrix for Laguerre polynomials.
- `lagvander2d` -- Vandermonde-like matrix for 2D power series.
- `lagvander3d` -- Vandermonde-like matrix for 3D power series"
2862553,866726828001421298,12,BigQuery,"(tan(z), 5) + '
        '7*sin(x)*pow(cos(y), 6) + '
        '42*sin(x)*pow(cos(y), 5)*tan(z) + '
        '105*sin(x)*pow(cos(y), 4)*pow(tan(z), 2) + '
        '140*sin(x)*pow(cos(y), 3)*pow(tan(z), 3) + '
        '105*sin(x)*pow(cos(y), 2)*pow(tan(z), 4) + '
        '42*sin(x)*cos"
5136130,-6257120471629965823,7,BigQuery,"ulator-impl.h (module 'core'): bool ns3::RealtimeSimulatorImpl::IsFinished() const [member function]
    cls.add_method('IsFinished', 
                   'bool', 
                   [], 
                   is_const=True, is_virtual=True)
    ## realtime-simulator-impl.h (module 'core'): ns3::Time ns3::RealtimeSimulatorImpl::Now() const [member function]
    cls.add_method('Now', 
                   'ns3::Time', 
                   [], 
                   is_const=True, is_virtual=True)
    ##"
3711130,8060768934395567582,11,BigQuery,"ite""), primary_key=True)
    )

Another is to use a subclass of :class:`.BigInteger` that overrides its DDL name
to be ``INTEGER`` when compiled against SQLite::

    from sqlalchemy import BigInteger
    from sqlalchemy.ext.compiler import compiles

    class SLBigInteger(BigInteger):
        pass

    @compiles(SLBigInteger,'sqlite')
    def bi_c(element, compiler, **kw):
        return ""INTEGER""

    @compiles(SLBigInteger)
    def bi_c(element, compiler, **kw):
        return compiler.vis"
378761,3759267044791584683,75,BigQuery,\ue9b6\ue9b7\ue9b8\ue9b9\ue9ba\ue9bb\ue9bc\ue9bd\ue9be\ue9bf\ue9c0\ue9c1\ue9c2\ue9c3\ue9c4\ue9c5\ue9c6\ue9c7\ue9c8\ue9c9\ue9ca\ue9cb\ue9cc\ue9cd\ue9ce\ue9cf\ue9d0\ue9d1\ue9d2\ue9d3\ue9d4\ue9d5\ue
6151163,-3464752507245136073,7,BigQuery," script, do not edit!'''
from OpenGL import platform as _p, arrays
# Code generation uses this
from OpenGL.raw.GL import _types as _cs
# End users want this...
from OpenGL.raw.GL._types import *
from OpenGL.raw.GL import _errors
from OpenGL.constant import Constant as _C

import ctypes
_EXTENSION_NAME = 'GL_NV_occlusion_query'
def _f( function ):
    return _p.createFunction( function,_p.PLATFORM.GL,'GL_NV_occlusion_query',error_checker=_errors._error_checker)
GL"
4483819,1318385255521412020,14,BigQuery,"""OnBuild"": null,
            ""OpenStdin"": false,
            ""StdinOnce"": false,
            ""Tty"": false,
            ""User"": """",
            ""Volumes"": {
                ""/tmp/lnmp/nginx-sites/logs/"": {}
            },
           ...
    }'
'''

import traceback

try:
    from docker.errors import DockerException
except ImportError:
    # missing Docker SDK for Python handled in ansible.module_utils.docker.common
    pass

from ansible.module_utils.docker.common import (
    AnsibleDockerClient,
    "
1922155,7393273331756534120,9,BigQuery,"ension=False, extension_scope=None,
      options=None),
    _descriptor.FieldDescriptor(
      name='value', full_name='protobuf_test_messages.proto2.TestAllTypesProto2.MapSfixed64Sfixed64Entry.value', index=1,
      number=2, type=16, cpp_type=2, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None"
7106040,-5296529070190794205,7,BigQuery,".utils.encoding import force_bytes, force_text
from django.utils.functional import LazyObject
from django.utils.six import iteritems
from django.utils.six.moves.urllib.parse import (
    unquote, urldefrag, urlsplit, urlunsplit,
)


class StaticFilesStorage(FileSystemStorage):
    """"""
    Standard file system storage for static files.

    The defaults for ``location`` and ``base_url`` are
    ``STATIC_ROOT`` and ``STATIC_URL``.
    """"""
    def __init__(self, location=None, base"
6272155,-4457966042552717387,7,BigQuery,"])

  return padded


def crop_to_bounding_box(image, offset_height, offset_width, target_height,
                         target_width):
  """"""Crops an image to a specified bounding box.

  This op cuts a rectangular part out of `image`. The top-left corner of the
  returned image is at `offset_height, offset_width` in `image`, and its
  lower-right corner is at
  `offset_height + target_height, offset_width + target_width`.

  Args:
    image: 4-D Tensor of shape `[batch, height, width, channels]` or
           3"
3802102,6490744078869155981,6,BigQuery,"
    private = 1
    projectPrivate = 2
    publicRead = 3
    publicReadWrite = 4

  class PredefinedDefaultObjectAclValueValuesEnum(_messages.Enum):
    r""""""Apply a predefined set of default object access controls to this
    bucket.

    Values:
      authenticatedRead: Object owner gets OWNER access, and
        allAuthenticatedUsers get READER access.
      bucketOwnerFullControl: Object owner gets OWNER access, and project team
        owners get OWNER access.
      bucketOwnerRead: Object owner gets OWNER access, and project team owners
        get READER access.
      private: Object"
181333,-8698377938194486798,15,BigQuery,"timespec),
            ('st_size', c_off_t),
            ('st_blocks', c_int64),
            ('st_blksize', c_int32)]
 
elif _system == 'Linux':
    ENOTSUP = 95
    c_dev_t = c_ulonglong
    c_fsblkcnt_t = c_ulonglong
    c_fsfilcnt_t = c_ulonglong
    c_gid_t = c_uint
    c_mode_t = c_uint
    c_off_t = c_longlong
    c_pid_t = c"
378761,1381649865792591455,75,BigQuery,b76\u6b77\u6b78\u6b79\u6b7a\u6b7b\u6b7c\u6b7d\u6b7e\u6b7f\u6b80\u6b81\u6b82\u6b83\u6b84\u6b85\u6b86\u6b87\u6b88\u6b89\u6b8a\u6b8b\u6b8c\u6b8d\u6b8e\u6b8f\u6b90\u6b91\
6044744,3094274355666108922,6,BigQuery,".log_utils.log_debug(traceback.format_exc())
                    raise

        self.__resolvers = []  # No resolvers.
        self._valid_url = False
        return False

    def valid_url(self):
        '''
        Returns True if the ``HostedMediaFile`` can be resolved.

       .. note::

            The following are exactly equivalent::

                if HostedMediaFile('http://youtube.com/watch?v=ABC123XYZ').valid_url():
                    print'resolvable!'

                if HostedMediaFile('http://youtube.com/watch?v=ABC123XYZ'):"
4378042,-5466734928285614297,6,BigQuery,"key, charset)] = smart_str(value, charset)
            return result
        else:
            return tuple([smart_str(p, self.charset, True) for p in params])

    def execute(self, sql, params=()):
        try:
            return self.cursor.execute(smart_str(sql, self.charset), self.format_params(params))
        except Database.IntegrityError, e:
            raise utils.IntegrityError, utils.IntegrityError(*tuple(e)), sys.exc_info()[2]
        except Database.DatabaseError, e:
            raise ut"
5801705,-5726172410022809237,6,BigQuery,"del__ = lambda self : None;
modbus_req_report_slaveid_swigregister = _TCPModbusClient.modbus_req_report_slaveid_swigregister
modbus_req_report_slaveid_swigregister(modbus_req_report_slaveid)

class modbus_reply_read_reg(_object):
    __swig_setmethods__ = {}
    __setattr__ = lambda self, name, value: _swig_setattr(self, modbus_reply_read_reg, name, value)
    __swig_getmethods__ = {}
    __getattr__ = lambda"
15142,-6899320389318509989,36,BigQuery," value. A non-\nempty format string typically modifies the result.\n\nThe general form of a *standard format specifier* is:\n\n   format_spec ::= [[fill]align][sign][#][0][width][,][.precision][type]\n   fill        ::= <a character other than \'{\' or \'}\'>\n   align       ::= ""<"" | "">"" | ""="" | ""^""\n   sign        ::= ""+"" | ""-"" | "" ""\n   width       ::= integer\n   precision   ::= integer\n   type        ::= ""b"" | ""c"" | ""d"""
5636068,4971927466786650081,17,BigQuery," 0


def callback(dt):
    global counter
    counter += 1


class ClockTestCase(unittest.TestCase):

    def setUp(self):
        from kivy.clock import Clock
        global counter
        counter = 0
        Clock._events = [[] for i in range(256)]

    def test_schedule_once(self):
        from kivy.clock import Clock
        Clock.schedule_once(callback)
        Clock.tick()
        self.assertEqual(counter, 1)

    def test_schedule_once_twice(self):
        from kivy.clock import Clock
"
6500097,2447309355853927652,13,BigQuery," a valid URL.'"", f.clean, 'com.')
        self.assertRaisesMessage(ValidationError, ""'Enter a valid URL.'"", f.clean, '.')
        self.assertRaisesMessage(ValidationError, ""'Enter a valid URL.'"", f.clean, 'http://.com')
        self.assertRaisesMessage(ValidationError, ""'Enter a valid URL.'"", f.clean, 'http://invalid-.com')
        self.assertRaisesMessage(ValidationError, ""'Enter a valid URL.'"", f.clean, 'http://-invalid.com')
        self.assertRaisesMessage(ValidationError, ""'Enter"
6363119,-2063380054354399341,8,BigQuery,"revised_sample_text = """"""\
http://www.python.org/doc/2.4.1/lib/module-unittest.html
test case
    A test case is the smallest unit of testing. [...] You may provide your
    own implementation that does not subclass from TestCase, of course.
""""""
        sample_text_error = """"""\
- http://www.python.org/doc/2.3/lib/module-unittest.html
?                             ^
+ http://www.python.org/doc/2.4.1/lib/module-unittest.html
?                             ^^^
  test case
-     "
1437522,1093298280949569027,7,BigQuery,"self.isectStart,
            sizeLow,
            sizeHigh
        ) = struct.unpack(_OleDirectoryEntry.STRUCT_DIRENTRY, entry)
        if self.entry_type not in [STGTY_ROOT, STGTY_STORAGE, STGTY_STREAM, STGTY_EMPTY]:
            olefile._raise_defect(DEFECT_INCORRECT, 'unhandled OLE storage type')
        # only first directory entry can (and should) be root:
        if self.entry_type == STGTY_ROOT and sid!= 0:
            olefile._raise_defect(DEFECT_"
6773063,6062344115188909758,24,BigQuery,"')])
    ## nix-vector.h (module 'network'): uint32_t ns3::NixVector::BitCount(uint32_t numberOfNeighbors) const [member function]
    cls.add_method('BitCount', 
                   'uint32_t', 
                   [param('uint32_t', 'numberOfNeighbors')], 
                   is_const=True)
    ## nix-vector.h (module 'network'): ns3::Ptr<ns3::NixVector> ns3::NixVector::Copy() const [member function]
    cls.add_method('Copy', 
                   'ns"
1392638,2709686646076793064,17,BigQuery,",
            'newline': newline,
            'prompt_retry_check': prompt_retry_check,
            'check_all': check_all
        }

        if prompt is not None:
            if isinstance(prompt, list):
                kwargs['prompt'] = [to_bytes(p) for p in prompt]
            else:
                kwargs['prompt'] = to_bytes(prompt)
        if answer is not None:
            if isinstance(answer, list):
                kwargs['answer'] = [to_bytes(p) for p in answer]
            else:
                kwargs['answer']"
1861677,-6582558596321240327,11,BigQuery,"x2014, 0x2013, 0x0192, 0x2044,
  0x2039, 0x203a, 0x2212, 0x2030, 0x201e, 0x201c, 0x201d, 0x2018,
  0x2019, 0x201a, 0x2122, 0xfb01, 0xfb02, 0x0141, 0x0152, 0x0160,
  0x0178, 0x017d, 0x0131, 0x0142, 0x0153, 0x0161, 0x017e, 0x0000,
  0x20ac, 0x00"
7167206,-744093468597725418,7,BigQuery,"sici(self):
        cephes.sici(1)

        s, c = cephes.sici(np.inf)
        assert_almost_equal(s, np.pi * 0.5)
        assert_almost_equal(c, 0)

        s, c = cephes.sici(-np.inf)
        assert_almost_equal(s, -np.pi * 0.5)
        assert_(np.isnan(c), ""cosine integral(-inf) is not nan"")

    def test_sindg(self):
        assert_equal(cephes.sindg(90),1."
5469562,5592849041946858688,14,BigQuery," fields.selection([('all','Display all Accounts'), ('movement','Display Accounts with movements'),
                                            ('not_zero','Display Account where balance is not equal to 0'),
                                            ],'Display Accounts', required=True),

    }
    _defaults = {
        'display_account':'movement',
    }

    def pre_print_report(self, cr, uid, ids, data, context=None):
        if context is None:
            context = {}
        data['form'].update(self.read(cr, uid, ids, ['display_account'], context=context)[0])
        return data

"
4453209,-5331216664010229263,6,BigQuery,"openstack.compute.contrib"" + \
        "".server_start_stop.Server_start_stop""

    def _test_server_action(self, uuid, action):
        response = self._do_post('servers/%s/action' % uuid,
                                'server_start_stop',
                                 {'action': action})
        self.assertEqual(response.status, 202)
        self.assertEqual(response.read(), """")

    def test_server_start(self):
        uuid = self._post_server()
        self._test_server_action(uuid, 'os-"
378761,6318630787095681975,75,BigQuery,40cb\u40cc\u40cd\u40ce\u40cf\u40d0\u40d1\u40d2\u40d3\u40d4\u40d5\u40d6\u40d7\u40d8\u40d9\u40da\u40db\u40dc\u40dd\u40de\u40df\u40e0\u40e1\u40e2\u40e3\u40e4\u40e5\u40e6\u40e7\u40e8\u40e9\u40ea\u40eb
4498131,4153592919936963906,6,BigQuery,"start_magic)):
                #print'match',`input`  # dbg
                print 'Executing:',input,
                self.shell.runlines(input)
                return
        print 'No previous input matching `%s` found.' % start

    
    def magic_bookmark(self, parameter_s=''):
        """"""Manage IPython's bookmark system.

        %bookmark <name>       - set bookmark to current dir
        %bookmark <name> <dir> - set bookmark to <dir>
        %bookmark -l           - list all bookmarks
        %bookmark -d <name>    - remove bookmark
        "
893455,4557554043239733692,17,BigQuery,"reporter.update(i, self)

            if monitor is not None:
                early_stopping = monitor(i, self, locals())
                if early_stopping:
                    break
        return i + 1

    def _make_estimator(self, append=True):
        # we don't need _make_estimator
        raise NotImplementedError()

    def _init_decision_function(self, X):
        """"""Check input and compute prediction of ``init``. """"""
        self._check_initialized()
        X = self.estimators_[0, 0]._validate_X_predict(X,"
6788190,328064013828983692,21,BigQuery," x):
        """"""Sets value at each (i, j) to x

        Here (i,j) index major and minor respectively.
        """"""
        M, N = self._swap(self.shape)

        def check_bounds(indices, bound):
            idx = indices.max()
            if idx >= bound:
                raise IndexError('index (%d) out of range (>= %d)' %
                                 (idx, bound))
            idx = indices.min()
            if idx < -bound:
                raise IndexError('index (%d) out of range (< -%d)' %
                                 (idx"
5393413,-6447407234167683121,7,BigQuery,"print('Reading took %.2f seconds. Calculating took %.2f seconds' % ((t2 - t1), (t4 - t3)),
          file=sys.stderr)

#
#  Copyright (c) 2013, Novartis Institutes for BioMedical Research Inc.
#  All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are
# met:
#
#     * Redistributions of source code must retain the above copyright
#       notice, this list of conditions and the following disclaimer.
#     * Redistributions in binary form"
6212165,235020778104298793,11,BigQuery,"s.superuser = User.objects.create_superuser(username='super', password='secret', email='super@example.com')
        cls.s1 = Section.objects.create(name='Test section')
        cls.a1 = Article.objects.create(
            content='<p>Middle content</p>', date=datetime.datetime(2008, 3, 18, 11, 54, 58), section=cls.s1
        )
        cls.a2 = Article.objects.create(
            content='<p>Oldest content</p>', date=datetime.datetime(2000, 3, 18, 11,"
3029884,5619663136958096349,49,BigQuery,"if not module.check_mode:
                client.sudorule_del(name)

    return changed, client.sudorule_find(name)


def main():
    module = AnsibleModule(
        argument_spec=dict(
            cmd=dict(type='list', required=False),
            cmdcategory=dict(type='str', required=False, choices=['all']),
            cn=dict(type='str', required=True, aliases=['name']),
            description=dict(type='str', required=False),
            host=dict(type='list', required=False),
            hostcategory=dict(type='"
1710674,-8475381308986748950,14,BigQuery,"Col"", self.getLabelCol())
        _java_obj.set(""predictionCol"", self.getPredictionCol())
        return _java_obj


if __name__ == ""__main__"":
    import doctest
    import pyspark.ml.classification
    from pyspark.sql import SparkSession
    globs = pyspark.ml.classification.__dict__.copy()
    # The small batch size here ensures that we see multiple batches,
    # even in these small test examples:
    spark = SparkSession.builder\
       .master(""local[2]"")\
       .appName(""ml.classification"
4121030,3447136678853373346,6,BigQuery," Exception()
                            host = host.encode('utf-8')

                            sources.append({'source': host, 'quality': quality2, 'language': 'en', 'url': url, 'info': info, 'direct': False, 'debridonly': True})
                        except:
                            pass
                    
                except:
                    pass


            return sources
        except:
            return sources


    def resolve(self, url):
        try:
            b = urlparse.urlparse(url).netloc
            b = re.compile('([\w]+[.][\w]+)$').findall(b)[0]

            if"
3953899,-6683929335605105027,9,BigQuery,"': ('sentry.db.models.fields.bounded.BoundedBigAutoField', [], {'primary_key': 'True'}),
            'key': ('django.db.models.fields.CharField', [], {'unique': 'True','max_length': '32'}),
            'label': ('django.db.models.fields.CharField', [], {'default': ""'Default'"",'max_length': '64', 'blank': 'True'}),
            'organization': ('sentry.db.models.fields.foreignkey.FlexibleForeignKey', [], {'related_name': ""'key_"
3863663,-2137802868072517610,51,BigQuery,"_vms) + "" VMs using it."")

    if not module.check_mode:
        client.call('image.delete', image.id)
        wait_for_delete(module, image)

    return {'changed': True}


def get_connection_info(module):

    url = module.params.get('api_url')
    username = module.params.get('api_username')
    password = module.params.get('api_password')

    if not url:
        url = os.environ.get('ONE_URL')

    if not username:
        username = os.environ.get('ONE_USER"
3121124,132326818172374746,45,BigQuery," a copy of a title, with references, images, etc. removed.""""""
        visitor = ContentsFilter(self.document)
        node.walkabout(visitor)
        return visitor.get_entry_text()


class ContentsFilter(nodes.TreeCopyVisitor):

    def get_entry_text(self):
        return self.get_tree_copy().children

    def visit_citation_reference(self, node):
        raise nodes.SkipNode

    def visit_footnote_reference(self, node):
        raise nodes.SkipNode

    def visit_image(self, node):
        if node.hasattr('alt'):"
1998035,3260552532444399259,9,BigQuery,"gm((year, month, day, hour, minute, second,
                            0, 0, 0))

def posixtime_to_sigtime(what):
    return time.strftime('%Y%m%d%H%M%S', time.gmtime(what))

class RRSIG(dns.rdata.Rdata):
    """"""RRSIG record

    @ivar type_covered: the rdata type this signature covers
    @type type_covered: int
    @ivar algorithm: the algorithm used for the sig
    @type algorithm: int
    @ivar labels: number of labels
    @type labels: int"
2467999,-2452693993154911522,7,BigQuery,"state=rng)

    Y = np.vstack((y, y)).T
    n_features = X.shape[1]

    reg = LinearRegression()
    reg.fit((X), Y)
    assert reg.coef_.shape == (2, n_features)
    Y_pred = reg.predict(X)
    reg.fit(X, y)
    y_pred = reg.predict(X)
    assert_array_almost_equal(np.vstack((y_pred, y_pred)).T, Y_pred, decimal=3)


def test_linear_regression_sparse_multiple_"
1512768,-6659267094976283581,6,BigQuery," (1,), None),
    ""L"": (""L"", II, 1, 1, (8,), None),
    ""LA"": (""LA"", II, 1, 1, (8, 8), 2),
    ""P"": (""P"", II, 3, 1, (8,), None),
    ""PA"": (""PA"", II, 3, 1, (8, 8), 2),
    ""I"": (""I;32S"", II, 1, 2, (32,), None),
    ""I;16"": (""I;16"", II, 1, 1, (16,), None),
    ""I;16S"": (""I;16S"", II"
6303129,-1576982417895695233,62,BigQuery," _ = gyp.common.ParseQualifiedTarget(target_list[0])
  make_global_settings_dict = data[build_file].get('make_global_settings', {})
  for key, value in make_global_settings_dict:
    if key in ['CC', 'CXX']:
      return value

  # Check to see if the compiler was specified as an environment variable.
  for key in ['CC_target', 'CC', 'CXX']:
    compiler = os.environ.get(key)
    if compiler:
      return compiler

  return 'gcc'


def GetAllDefines(target"
5197502,9033476809180772630,31,BigQuery,"Indian/Kerguelen',
 'Indian/Mahe',
 'Indian/Maldives',
 'Indian/Mauritius',
 'Indian/Mayotte',
 'Indian/Reunion',
 'Pacific/Apia',
 'Pacific/Auckland',
 'Pacific/Bougainville',
 'Pacific/Chatham',
 'Pacific/Chuuk',
 'Pacific/Easter',
 'Pacific/Efate',
 'Pacific/Enderbury',
 'Pacific/Fakaofo',
 'Pacific/Fiji',
 'Pacific/Funafuti',
 'Pacific/Galapagos',
 'Pacific/Gambier',
 'Pacific/"
378761,-2444899485921362736,75,BigQuery,\u3abb\u3abc\u3abd\u3abe\u3abf\u3ac0\u3ac1\u3ac2\u3ac3\u3ac4\u3ac5\u3ac6\u3ac7\u3ac8\u3ac9\u3aca\u3acb\u3acc\u3acd\u3ace\u3acf\u3ad0\u3ad1\u3ad2\u3ad3\u3ad4\u3ad5\u3ad6\u3ad7\u3ad8\u3ad9\
6242110,4367179482870222166,9,BigQuery,">')

    # TODO: use the new HTML5 element <aside>? (Also for footnote text)
    def visit_topic(self, node):
        self.body.append(self.starttag(node, 'div', CLASS='topic'))
        self.topic_classes = node['classes']
        # TODO: replace with ::
        #   self.in_contents = 'contents' in node['classes']

    def depart_topic(self, node):
        self.body.append('</div>\n')
        self.topic_classes = []
        # TODO self.in_contents = False

    def"
1664999,-6646970737646343070,17,BigQuery,"RC_SCLOSED = 0x0a
NRC_CMDCAN = 0x0b
NRC_DUPNAME = 0x0d
NRC_NAMTFUL = 0x0e
NRC_ACTSES = 0x0f
NRC_LOCTFUL = 0x11
NRC_REMTFUL = 0x12
NRC_ILLNN = 0x13
NRC_NOCALL = 0x14
NRC_NOWILD = 0x15
NRC_INUSE = 0x16
NRC_NAMERR = 0x17
NRC_SABORT = 0x18
NRC_N"
5136130,4534350378331992688,7,BigQuery,"h (module 'core'): ns3::Scheduler::EventKey::m_uid [variable]
    cls.add_instance_attribute('m_uid', 'uint32_t', is_const=False)
    return

def register_Ns3SequentialRandomVariable_methods(root_module, cls):
    ## random-variable-stream.h (module 'core'): static ns3::TypeId ns3::SequentialRandomVariable::GetTypeId() [member function]
    cls.add_method('GetTypeId', 
                   'ns3::TypeId', 
                   [], 
                   is_static=True)"
3924086,-3770128156880967281,9,BigQuery,". Setting this to C(0) makes yum try forever.
  s3_enabled:
    required: false
    choices: ['yes', 'no']
    default: 'no'
    description:
      - Enables support for S3 repositories.
      - This option only works if the YUM S3 plugin is installed.
  skip_if_unavailable:
    required: false
    choices: ['yes', 'no']
    default: 'no'
    description:
      - If set to C(yes) yum will continue running if this repository cannot be
        contacted for any reason. This should be set carefully as all repos are"
4620588,6519174238868146991,14,BigQuery,", visibility='protected')
    ## mobility-model.h (module'mobility'): int64_t ns3::MobilityModel::DoAssignStreams(int64_t start) [member function]
    cls.add_method('DoAssignStreams', 
                   'int64_t', 
                   [param('int64_t','start')], 
                   visibility='private', is_virtual=True)
    ## mobility-model.h (module'mobility'): ns3::Vector ns3::MobilityModel::DoGetPosition() const [member function]
    cls.add_method('DoGetPosition', 
"
3453320,-456241730527647090,11,BigQuery,":
            carry, second = divmod(second, 60)
            minute += carry
        if not 0 <= minute <= 59:
            carry, minute = divmod(minute, 60)
            hour += carry
        if not 0 <= hour <= 23:
            carry, hour = divmod(hour, 24)
            day += carry

        # That was easy.  Now it gets muddy:  the proper range for day
        # can't be determined without knowing the correct month and year,
        # but if day is, e.g., plus or minus a million, the current month
        # and year values make no sense (and may also be out of bounds
        #"
1179863,-67017471709950155,7,BigQuery,".add(_elem47)
          iprot.readSetEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
      return
    op"
5332889,-7486753348588558988,11,BigQuery,"
#'

# Super directory utilities.
# (Inspired by Eric Raymond; the doc strings are mostly his)

def makedirs(name, mode=0777):
    """"""makedirs(path [, mode=0777])

    Super-mkdir; create a leaf directory and all intermediate ones.
    Works like mkdir, except that any intermediate path segment (not
    just the rightmost) will be created if it does not exist.  This is
    recursive.

    """"""
    head, tail = path.split(name)
    if not tail:
        head, tail = path.split(head)
    if head and tail and"
90327,-730561786793553841,11,BigQuery,")
    >>> np.tile(a, (2, 1, 2))
    array([[[0, 1, 2, 0, 1, 2]],
           [[0, 1, 2, 0, 1, 2]]])

    >>> b = np.array([[1, 2], [3, 4]])
    >>> np.tile(b, 2)
    array([[1, 2, 1, 2],
           [3, 4, 3, 4]])
    >>> np.tile(b, (2, 1))
    array([[1, 2],
           [3, 4],
           [1, 2],
           [3, 4"
499263,3996663337627916023,8,BigQuery," 1
        self.gotonext()
        return EMPTYSTRING.join(aslist) + self.getdomain()

    def getdomain(self):
        """"""Get the complete domain name from an address.""""""
        sdlist = []
        while self.pos < len(self.field):
            if self.field[self.pos] in self.LWS:
                self.pos += 1
            elif self.field[self.pos] == '(':
                self.commentlist.append(self.getcomment())
            elif self.field[self.pos] == '[':
                sdlist.append(self.getdomainliteral())"
2892542,716293023959876953,8,BigQuery,"res = {}
        for line in self.browse(cr, uid, ids, context=context):
            res[line.id] = float(line.quantity) * line.amount * line.rate / 100
        return res

    _columns = {
       'slip_id':fields.many2one('hr.payslip', 'Pay Slip', required=True, ondelete='cascade'),
       'salary_rule_id':fields.many2one('hr.salary.rule', 'Rule', required=True),
        'employee_id':fields.many2one('hr.employee', 'Employee', required="
7167206,1480432823533735786,7,BigQuery,"
             -6.7812944460, -7.9401786892, -9.0195833588], rtol=1e-10)


class TestAssocLaguerre(TestCase):
    def test_assoc_laguerre(self):
        a1 = special.genlaguerre(11,1)
        a2 = special.assoc_laguerre(.2,11,1)
        assert_array_almost_equal(a2,a1(.2),8)
        a2 = special.assoc_laguerre(1,11,1)
        assert_"
6166978,-8730207277053202669,24,BigQuery,"ter > > >', 'ns3::SpectrumConverterMap_t')
    typehandlers.add_type_alias('std::map< unsigned int, ns3::SpectrumConverter, std::less< unsigned int >, std::allocator< std::pair< unsigned int const, ns3::SpectrumConverter > > >*', 'ns3::SpectrumConverterMap_t*')
    typehandlers.add_type_alias('std::map< unsigned int, ns3::SpectrumConverter, std::less< unsigned int >, std::allocator< std::pair< unsigned int const, ns3::SpectrumConver"
378761,2915937776732087637,75,BigQuery,u6722\u6723\u6724\u6725\u6726\u6727\u6728\u6729\u672a\u672b\u672c\u672d\u672e\u672f\u6730\u6731\u6732\u6733\u6734\u6735\u6736\u6737\u6738\u6739\u673a\u673b\u673c\u673d\u673e\u673f\u6740\u6741\u6742\u6743\u6744\u6745\u6746\u67
14636,115567748532358495,11,BigQuery," msg, hdrs)

        return response

    https_response = http_response

class HttpCase(TransactionCase):
    """""" Transactional HTTP TestCase with url_open and phantomjs helpers.
    """"""

    def __init__(self, methodName='runTest'):
        super(HttpCase, self).__init__(methodName)
        # v8 api with correct xmlrpc exception handling.
        self.xmlrpc_url = url_8 = 'http://%s:%d/xmlrpc/2/' % (HOST, PORT)
        self.xmlrpc_common = xmlrpclib."
6151456,-1151849052622595130,9,BigQuery,"ids))

    def test_iziplongest(self):
        for args in [
                ['abc', range(6)],
                [range(6), 'abc'],
                [range(1000), range(2000,2100), range(3000,3050)],
                [range(1000), range(0), range(3000,3050), range(1200), range(1500)],
                [range(1000), range(0), range(3000,3050), range(1200), range(1500), range(0)],
            ]:
            # target = map(None, *args) <- this raises a py3k warning
            # this is the"
7167206,5272887072375870900,7,BigQuery,"0)

    def test_beip(self):
        assert_equal(cephes.beip(0),0.0)

    def test_ber(self):
        assert_equal(cephes.ber(0),1.0)

    def test_berp(self):
        assert_equal(cephes.berp(0),0.0)

    def test_besselpoly(self):
        assert_equal(cephes.besselpoly(0,0,0),1.0)

    def test_beta(self):
        assert_equal(cephes.beta(1,1),1.0"
3909499,-2363160632161335718,61,BigQuery," whence!= 0:
            raise ValueError(""unsupported whence (%r)"" % (whence,))
        if cookie < 0:
            raise ValueError(""negative seek position %r"" % (cookie,))
        self.flush()

        # The strategy of seek() is to go back to the safe start point
        # and replay the effect of read(chars_to_skip) from there.
        start_pos, dec_flags, bytes_to_feed, need_eof, chars_to_skip = \
            self._unpack_cookie(cookie)

        # Seek back to the safe start point.
        self.buffer.seek(start_pos"
3711130,502686483940052109,11,BigQuery,"
            self._broken_fk_pragma_quotes = (
                self.dbapi.sqlite_version_info < (3, 6, 14))

    _isolation_lookup = {
        'READ UNCOMMITTED': 1,
        'SERIALIZABLE': 0,
    }

    def set_isolation_level(self, connection, level):
        try:
            isolation_level = self._isolation_lookup[level.replace('_','')]
        except KeyError:
            raise exc.ArgumentError(
                ""Invalid value '%s' for isolation_level. ""
                """
1786173,-5246366583205328512,14,BigQuery,"True)
    ## basic-data-calculators.h (module'stats'): double ns3::MinMaxAvgTotalCalculator<unsigned int>::getMin() const [member function]
    cls.add_method('getMin', 
                   'double', 
                   [], 
                   is_const=True, is_virtual=True)
    ## basic-data-calculators.h (module'stats'): double ns3::MinMaxAvgTotalCalculator<unsigned int>::getSqrSum() const [member function]
    cls.add_method('getSqrSum', 
                   'double', 
"
378761,-4779497925816067292,75,BigQuery,a\uf93b\uf93c\uf93d\uf93e\uf93f\uf940\uf941\uf942\uf943\uf944\uf945\uf946\uf947\uf948\uf949\uf94a\uf94b\uf94c\uf94d\uf94e\uf94f\uf950\uf951\uf952\uf953\uf954\uf955\uf956\uf957\uf958\uf959\uf95a\uf95b\uf95c\uf95d\uf95e\uf95f\uf960\uf961\uf
3863000,5235225060679321212,9,BigQuery,"ote_re.match(s, end)
            elif s[end] == '""':         # slurp doubly-quoted string
                m = _dquote_re.match(s, end)
            else:
                raise RuntimeError(""this can't happen (bad char '%c')"" % s[end])

            if m is None:
                raise ValueError(""bad string (mismatched %s quotes?)"" % s[end])

            (beg, end) = m.span()
            s = s[:beg] + s[beg+1:end-1] + s[end:]
            pos = m."
4651056,-8416651038183589395,25,BigQuery,"Model

ONE_CHAR_PROB = 0.5

class UTF8Prober(CharSetProber):
    def __init__(self):
        CharSetProber.__init__(self)
        self._mCodingSM = CodingStateMachine(UTF8SMModel)
        self.reset()

    def reset(self):
        CharSetProber.reset(self)
        self._mCodingSM.reset()
        self._mNumOfMBChar = 0

    def get_charset_name(self):
        return ""utf-"
378761,-7547240492107848106,75,BigQuery,1610\u1611\u1612\u1613\u1614\u1615\u1616\u1617\u1618\u1619\u161a\u161b\u161c\u161d\u161e\u161f\u1620\u1621\u1622\u1623\u1624\u1625\u1626\u1627\u1628\u1629\u162a\u162b\u162c\u162d\u162e\u162f\u1630\u1631\u1632\u1633\u1634\u1635
4680409,4296399886096948724,6,BigQuery,"        self._cached_lines = []

        self.options = options
        self.texture = None
        self.resolve_font_name()

    @staticmethod
    def register(name, fn_regular, fn_italic=None, fn_bold=None,
                 fn_bolditalic=None):
        '''Register an alias for a Font.

       .. versionadded:: 1.1.0

        If you're using a ttf directly, you might not be able to use the
        bold/italic properties of
        the ttf version. If the font is delivered in multiple files
        (one regular, one italic and one bold),"
5137002,6069400097431580777,50,BigQuery,", a.__iadd__, ""bad"")

    def test_mul(self):
        a = 5*array.array(self.typecode, self.example)
        self.assertEqual(
            a,
            array.array(self.typecode, 5*self.example)
        )

        a = array.array(self.typecode, self.example)*5
        self.assertEqual(
            a,
            array.array(self.typecode, self.example*5)
        )

        a = 0*array.array(self.typecode, self.example)
        self.assertEqual(
"
5408229,-151546009452523716,7,BigQuery,".is_fully_defined.
      if bshape == x.shape:
        return x
      # Use the built in broadcasting of addition.
      zeros = array_ops.zeros(shape=special_shape, dtype=self.dtype)
      return x + zeros

    # Dynamic broadcast:
    #   Always add to an array of zeros, rather than using a ""cond"", since a
    #   cond would require copying data from GPU --> CPU.
    special_shape = array_ops.concat((self.batch_shape_tensor(), [1, 1]), 0)
    zeros = array_ops.zeros(shape="
136658,-3691443446548086122,124,BigQuery,"raise TypeError, ""can't pickle %s objects"" % base.__name__
        state = base(self)
    args = (self.__class__, base, state)
    try:
        getstate = self.__getstate__
    except AttributeError:
        if getattr(self, ""__slots__"", None):
            raise TypeError(""a class that defines __slots__ without ""
                            ""defining __getstate__ cannot be pickled"")
        try:
            dict = self.__dict__
        except AttributeError:
            dict = None
    else:
        dict = getstate()
    if dict"
1876344,465072625565389469,6,BigQuery,"  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=1510,
  serialized_end=1593,
)


_EQUIPBADGEMESSAGE = _descriptor.Descriptor(
  name='EquipBadgeMessage',
  full_name='POGOProtos.Networking.Requests.Messages.EquipBadgeMessage',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='"
3257388,-3586109427824185447,38,BigQuery,"
            a1 = np.insert(a, row, u, 0)
            check_qr(q1, r1, a1, self.rtol, self.atol)
    
    def test_sqr_p_row(self):
        # sqr + rows --> fat always
        a, q, r, u = self.generate('sqr', which='row', p=3)
        for row in range(r.shape[0] + 1):
            q1, r1 = qr_insert(q, r, u, row)
            a1 = np.insert(a, row*np.ones(3"
4620588,-1072425365481553527,14,BigQuery,"## wifi-remote-station-manager.h (module 'wifi'): ns3::WifiRemoteStationState::m_shortGuardInterval [variable]
    cls.add_instance_attribute('m_shortGuardInterval', 'bool', is_const=False)
    ## wifi-remote-station-manager.h (module 'wifi'): ns3::WifiRemoteStationState::m_stbc [variable]
    cls.add_instance_attribute('m_stbc', 'bool', is_const=False)
    ## wifi-remote-station-manager.h (module 'wifi'): ns3::WifiRemoteStationState::"
4650818,-1005826349291128587,13,BigQuery,"from tensorflow.python.ops.distributions import transformed_distribution
from tensorflow.python.util import deprecation


class _VectorStudentT(transformed_distribution.TransformedDistribution):
  """"""A vector version of Student's t-distribution on `R^k`.

  #### Mathematical details

  The probability density function (pdf) is,

  ```none
  pdf(x; df, mu, Sigma) = (1 + ||y||**2 / df)**(-0.5 (df + 1)) / Z
  where,
  y = inv(Sigma) (x - mu)
  Z = abs("
4121562,-8575657156480919568,12,BigQuery," - izip throws StopIteration
            # when an iter throws it. So if the first iter throws it, the second
            # is *not* consumed. We rely on this, so don't change the order
            # without changing the logic.
            for val, field in izip(args, fields_iter):
                setattr(self, field.attname, val)
        else:
            # Slower, kwargs-ready version.
            for val, field in izip(args, fields_iter):
                setattr(self, field.attname, val)
                kwargs.pop(field.name, None)
                #"
4106047,3764048442082954489,8,BigQuery,"):
        obj = config.LibvirtConfigGuestCPU()
        obj.model = ""Penryn""
        obj.vendor = ""Intel""
        obj.arch = ""x86_64""
        obj.mode = ""custom""

        obj.add_feature(config.LibvirtConfigGuestCPUFeature(""mtrr""))
        obj.add_feature(config.LibvirtConfigGuestCPUFeature(""apic""))

        xml = obj.to_xml()
        self.assertXmlEqual(xml, """"""
            <cpu mode=""custom"" match=""exact"">
              <arch>x86_64</arch>
              <model>Penryn"
4483982,4497629545703780288,20,BigQuery," None

    def fetch_returned_insert_id(self, cursor):
        """"""
        Given a cursor object that has just performed an INSERT...RETURNING
        statement into a table that has an auto-incrementing ID, returns the
        newly created ID.
        """"""
        return cursor.fetchone()[0]

    def field_cast_sql(self, db_type):
        """"""
        Given a column type (e.g. 'BLOB', 'VARCHAR'), returns the SQL necessary
        to cast it before using it in a WHERE statement. Note that the
        resulting string should contain a '%s' placeholder for the"
666471,-76869133423701691,12,BigQuery,"AlmostEqual(c1[0] * xfac, c2[0], tol)
                        self.assertAlmostEqual(c1[1] * yfac, c2[1], tol)

    @no_mysql
    @no_oracle
    @no_spatialite
    def test_snap_to_grid(self):
        ""Testing GeoQuerySet.snap_to_grid().""
        # Let's try and break snap_to_grid() with bad combinations of arguments.
        for bad_args in ((), range(3), range(5)):
            self.assertRaises(ValueError, Country.objects."
877576,-1402567196500580245,7,BigQuery," module,
# you may extend this exception to your version of the module, but you
# are not obliged to do so. If you do not wish to do so, delete this
# exception statement from your version.

""""""
Porter Stemmer

This is the Porter stemming algorithm, ported to Python from the
version coded up in ANSI C by the author. It follows the algorithm
presented in

Porter, M. ""An algorithm for suffix stripping."" Program 14.3 (1980): 130-137.

only differing from it at the points marked --DEPARTURE-- and --NEW--
below.

For a more faithful version of the Porter algorithm, see

"
1922745,8082386616009245193,42,BigQuery," HostGroup(module, zbx)

    group_ids = []
    group_list = []
    if host_groups:
        group_ids, group_list = hostGroup.get_group_ids(host_groups)

    if state == ""absent"":
        # delete host groups
        if group_ids:
            delete_group_names = []
            hostGroup.delete_host_group(group_ids)
            for group in group_list:
                delete_group_names.append(group['name'])
            module.exit_json(changed=True,
                             result=""Successfully deleted host group(s): %s."""
5137002,-7643706866007053412,50,BigQuery,"a = array.array(self.typecode, self.example)
        b = copy.copy(a)
        self.assertNotEqual(id(a), id(b))
        self.assertEqual(a, b)

    def test_deepcopy(self):
        import copy
        a = array.array(self.typecode, self.example)
        b = copy.deepcopy(a)
        self.assertNotEqual(id(a), id(b))
        self.assertEqual(a, b)

    def test_reduce_ex(self):
        a = array.array(self.typecode,"
3528587,2502893600274984250,6,BigQuery," not None:
            wrapped_socket._exception = e
        return SecurityConst.errSSLInternal


# We need to keep these two objects references alive: if they get GC'd while
# in use then SecureTransport could attempt to call a function that is in freed
# memory. That would be...uh...bad. Yeah, that's the word. Bad.
_read_callback_pointer = Security.SSLReadFunc(_read_callback)
_write_callback_pointer = Security.SSLWriteFunc(_write_callback)


class WrappedSocket(object):
    """"""
    API-compatibility wrapper for Python's OpenSSL wrapped socket object.

    Note"
5137002,-889878808474246688,50,BigQuery,".assertTrue(a.itemsize>=self.minitemsize)
        self.assertRaises(TypeError, array.array, self.typecode, None)

    def test_len(self):
        a = array.array(self.typecode)
        a.append(self.example[0])
        self.assertEqual(len(a), 1)

        a = array.array(self.typecode, self.example)
        self.assertEqual(len(a), len(self.example))

    def test_buffer_info(self):
        a = array.array(self.typecode, self.example)
"
2771735,-6704036774439427031,38,BigQuery,"add_class('CallbackBase', import_from_module='ns.core')
    ## device-energy-model-container.h (module 'energy'): ns3::DeviceEnergyModelContainer [class]
    module.add_class('DeviceEnergyModelContainer')
    ## energy-model-helper.h (module 'energy'): ns3::DeviceEnergyModelHelper [class]
    module.add_class('DeviceEnergyModelHelper', allow_subclassing=True)
    ## energy-harvester-helper.h (module 'energy'): ns3::EnergyHarvesterHelper [class]
    module.add_class('EnergyHarvesterHelper',"
6182468,-1864246880860405741,81,BigQuery," self.execute_command(cmd)

    def group_mod(self, **kwargs):
        cmd = [self.module.get_bin_path('groupmod', True)]
        info = self.group_info()
        cmd_len = len(cmd)
        if self.gid is not None and int(self.gid)!= info[2]:
            cmd.append('-g')
            cmd.append('%d' % int(self.gid))
        if len(cmd) == 1:
            return (None, '', '')
        if self.module.check_mode:
            return (0, '', ''"
5136130,5163870038890380888,7,BigQuery," visibility='protected')
    return

def register_Ns3CommandLine_methods(root_module, cls):
    ## command-line.h (module 'core'): ns3::CommandLine::CommandLine() [constructor]
    cls.add_constructor([])
    ## command-line.h (module 'core'): ns3::CommandLine::CommandLine(ns3::CommandLine const & cmd) [copy constructor]
    cls.add_constructor([param('ns3::CommandLine const &', 'cmd')])
    ## command-line.h (module 'core'): void ns3::CommandLine::AddValue(std"
2892247,-3633772108766521484,6,BigQuery,"uum""):
        test_vacuum_config = {
            ""value_template"": ""{{ states('input_select.state') }}"",
            ""fan_speed_template"": ""{{ states('input_select.fan_speed') }}"",
            ""start"": {
                ""service"": ""input_select.select_option"",
                ""data"": {""entity_id"": _STATE_INPUT_SELECT, ""option"": STATE_CLEANING},
            },
            ""pause"": {
                ""service"": ""input_select.select_option"",
                ""data"": {""entity_id"": _STATE_INPUT_SELECT, ""option"": STATE_PA"
6938964,-3466270267772429148,8,BigQuery," in %.1fs' % (time.time() - t))


def erase_region(esp, args):
    print('Erasing region (may be slow depending on size)...')
    t = time.time()
    esp.erase_region(args.address, args.size)
    print('Erase completed successfully in %.1f seconds.' % (time.time() - t))


def run(esp, args):
    esp.run()


def flash_id(esp, args):
    flash_id = esp.flash_id()
    print('Manufacturer: %02x' % (flash_id & 0xff))
"
1287015,-4733508412869246632,30,BigQuery,"_alpha(0.5)
    splot.add_artist(ell)
    splot.set_xticks(())
    splot.set_yticks(())


def plot_lda_cov(lda, splot):
    plot_ellipse(splot, lda.means_[0], lda.covariance_,'red')
    plot_ellipse(splot, lda.means_[1], lda.covariance_, 'blue')


def plot_qda_cov(qda, splot):
    plot_ellipse(splot, qda."
5514998,-7837689875955343349,20,BigQuery,"b', float)])
        a_pickled = pickle.loads(a.dumps())
        assert_equal(a_pickled._mask, a._mask)
        assert_equal(a_pickled, a)

    def test_pickling_keepalignment(self):
        # Tests pickling w/ F_CONTIGUOUS arrays
        a = arange(10)
        a.shape = (-1, 2)
        b = a.T
        test = pickle.loads(pickle.dumps(b))
        assert_equal(test, b)

    def test_single_element_subscript(self):
"
4620588,3026359491733033370,14,BigQuery,"
    cls.add_method('SetPhy', 
                   'void', 
                   [param('ns3::Ptr< ns3::WifiPhy >', 'phy')])
    ## wifi-net-device.h (module 'wifi'): void ns3::WifiNetDevice::SetPromiscReceiveCallback(ns3::Callback<bool, ns3::Ptr<ns3::NetDevice>, ns3::Ptr<ns3::Packet const>, unsigned short, ns3::Address const&, ns3::Address const&, ns3::NetDevice::PacketType, ns3::empty, ns3::empty, ns3::empty> cb"
4076082,8917027404332958352,19,BigQuery,"mm.precisions_),
                                      gmm.covariances_)
        else:
            assert_array_almost_equal(gmm.precisions_, 1. / gmm.covariances_)


def test_sample():
    rng = np.random.RandomState(0)
    rand_data = RandomData(rng, scale=7, n_components=3)
    n_features, n_components = rand_data.n_features, rand_data.n_components

    for covar_type in COVARIANCE_TYPE:
        X = rand_data.X["
439465,-4019904010328624697,52,BigQuery,"):
        """"""
        Always returns EmptyQuerySet.
        """"""
        return self

    def filter(self, *args, **kwargs):
        """"""
        Always returns EmptyQuerySet.
        """"""
        return self

    def exclude(self, *args, **kwargs):
        """"""
        Always returns EmptyQuerySet.
        """"""
        return self

    def complex_filter(self, filter_obj):
        """"""
        Always returns EmptyQuerySet.
        """"""
        return self

    def select_related(self, *fields, **kwargs):
        """"""
        Always returns EmptyQuerySet.
        """"""
        return self

    "
7410073,-2242948624639182370,25,BigQuery,"y import MatrixSymbol
    assert str(2*(MatrixSymbol(""X"", 2, 2) + MatrixSymbol(""Y"", 2, 2))) == \
        ""2*(X + Y)""

def test_MatrixSlice():
    from sympy.matrices.expressions import MatrixSymbol
    assert str(MatrixSymbol('X', 10, 10)[:5, 1:9:2]) == 'X[:5, 1:9:2]'
    assert str(MatrixSymbol('X', 10, 10)[5, :5:2]) == 'X[5, :5:2]'

def test_true_false():
"
666471,-6349183500802971412,12,BigQuery,"        f_2 = Feature.objects.get(name='LineString')
        self.assertEqual(True, isinstance(f_2.geom, LineString))
        self.assertEqual(((0.0, 0.0), (1.0, 1.0), (5.0, 5.0)), f_2.geom.tuple)

        f_3 = Feature.objects.get(name='Polygon')
        self.assertEqual(True, isinstance(f_3.geom, Polygon))
        f_4 = Feature.objects.get(name='GeometryCollection')
        self.assertEqual("
6773063,2079714977298378354,24,BigQuery,"', import_from_module='ns.core', parent=root_module['ns3::RandomVariableStream'])
    ## error-model.h (module 'network'): ns3::ErrorModel [class]
    module.add_class('ErrorModel', import_from_module='ns.network', parent=root_module['ns3::Object'])
    ## event-impl.h (module 'core'): ns3::EventImpl [class]
    module.add_class('EventImpl', import_from_module='ns.core', parent=root_module['ns3::SimpleRefCount< ns3::EventImpl, ns3::empty, ns3::"
2347679,-46724954855444567,11,BigQuery,")
        self.assertEqual(
            request.producer.events,
            ['pause','resume', 'pause','resume']
        )
        request.unregisterProducer()
        request.finish()

        # Check that the sending loop sends all the appropriate data.
        def validate(streamID):
            frames = framesFromBytes(b.value())

            # Check that the stream is correctly terminated.
            self.assertTrue('END_STREAM' in frames[-1].flags)

            # Grab the data from the frames.
            dataChunks = [
                f.data for f in frames
                if isinstance("
2149567,1874151720519445191,9,BigQuery,"(1980,10,26,7,0,0),
d(1981,4,26,8,0,0),
d(1981,10,25,7,0,0),
d(1982,4,25,8,0,0),
d(1982,10,31,7,0,0),
d(1983,4,24,8,0,0),
d(1983,10,30,7,0,0),
d(1984,4,29,8,0,0),
d(1984,10,28,7,0,0),
d(1985,4,28,8,0,0),
d"
6075305,4454083795665288671,6,BigQuery,"('Frequency [radians / second]')
    >>> ax.set_ylabel('Amplitude [dB]')
    >>> ax.axis((10, 1000, -100, 10))
    >>> ax.grid(which='both', axis='both')
    >>> plt.show()

    """"""
    ftype, btype, output = [x.lower() for x in (ftype, btype, output)]
    Wn = asarray(Wn)
    try:
        btype = band_dict[btype]
    except KeyError:
        raise ValueError(""'%s' is an invalid bandtype for filter."" % btype"
1149724,4713573998932752558,8,BigQuery,"if c == 0:
            c = self.compare_total(other)

        if c == -1:
            ans = self
        else:
            ans = other

        return ans._fix(context)

    def _isinteger(self):
        """"""Returns whether self is an integer""""""
        if self._is_special:
            return False
        if self._exp >= 0:
            return True
        rest = self._int[self._exp:]
        return rest == '0'*len(rest)

    def _iseven(self):
        """"""Returns True if self is even.  Assumes self is an integer.""""""
        if"
6272155,3165641066395944436,7,BigQuery,":
    image: RGB image or images. Size of the last dimension must be 3.
    saturation_factor: float. Factor to multiply the saturation by.
    name: A name for this operation (optional).

  Returns:
    Adjusted image(s), same shape and DType as `image`.
  """"""
  with ops.name_scope(name, 'adjust_saturation', [image]) as name:
    image = ops.convert_to_tensor(image, name='image')
    # Remember original dtype to so we can convert back if needed
    orig_dtype = image.dtype
    flt_image = convert_image"
544871,-2778677660779172453,9,BigQuery,"lines[pos] = '##' + line
        self.set_region(head, tail, chars, lines)

    def uncomment_region_event(self, event):
        head, tail, chars, lines = self.get_region()
        for pos in range(len(lines)):
            line = lines[pos]
            if not line:
                continue
            if line[:2] == '##':
                line = line[2:]
            elif line[:1] == '#':
                line = line[1:]
            lines[pos] = line
        self.set_region(head, tail, chars, lines)"
6622319,-6083340830966030035,85,BigQuery,"_benchmark_czichowski_f5b():
    with config.using(groebner='f5b'):
        _do_test_benchmark_czichowski()

def _do_test_benchmark_cyclic_4():
    R, a,b,c,d = ring(""a,b,c,d"", ZZ, lex)

    I = [a + b + c + d,
         a*b + a*d + b*c + b*d,
         a*b*c + a*b*d + a*c*d + b*c*d,
         a*b*c*"
1149724,7709011343818403130,8,BigQuery,"convert_other(other, raiseit=True)
        modulo = _convert_other(modulo, raiseit=True)

        if context is None:
            context = getcontext()

        # deal with NaNs: if there are any sNaNs then first one wins,
        # (i.e. behaviour for NaNs is identical to that of fma)
        self_is_nan = self._isnan()
        other_is_nan = other._isnan()
        modulo_is_nan = modulo._isnan()
        if self_is_nan or other_is_nan or modulo_is_nan:
            "
3604698,5715141245259299846,8,BigQuery,"[name.lower()] = [int(x) for x in value.split(',')]

    _handle_INTERVAL = _handle_int
    _handle_COUNT = _handle_int
    _handle_BYSETPOS = _handle_int_list
    _handle_BYMONTH = _handle_int_list
    _handle_BYMONTHDAY = _handle_int_list
    _handle_BYYEARDAY = _handle_int_list
    _handle_BYEASTER = _handle_int_list
    _handle_BYWEEKNO = _handle_int_list
    _handle_BYHOUR ="
4483982,-5613664579159347144,20,BigQuery," value, field):
        """"""Coerce the value returned by the database backend into a consistent type that
        is compatible with the field type.
        """"""
        internal_type = field.get_internal_type()
        if internal_type == 'DecimalField':
            return value
        elif internal_type and internal_type.endswith('IntegerField') or internal_type == 'AutoField':
            return int(value)
        elif internal_type in ('DateField', 'DateTimeField', 'TimeField'):
            return value
        # No field, or the field isn't known to be a decimal or integer
        # Default to a float"
1664238,-2814574622000963704,6,BigQuery,"# Write everything out
            self.writeObject(fontdictObject, fontdict)
            self.writeObject(fontdescObject, descriptor)
            self.writeObject(widthsObject, widths)
            self.writeObject(charprocsObject, charprocs)

            return fontdictObject

        def embedTTFType42(font, characters, descriptor):
            """"""The Type 42-specific part of embedding a Truetype font""""""
            fontdescObject = self.reserveObject('font descriptor')
            cidFontDictObject = self.reserveObject('CID font dictionary')
            type0FontDictObject = self.reserveObject('"
2513646,4362060804777282697,10,BigQuery,"In contrast to :class:`Lasso`, `LassoSelector` is written with an interface
    similar to :class:`RectangleSelector` and :class:`SpanSelector` and will
    continue to interact with the axes until disconnected.

    Parameters:

    *ax* : :class:`~matplotlib.axes.Axes`
        The parent axes for the widget.
    *onselect* : function
        Whenever the lasso is released, the `onselect` function is called and
        passed the vertices of the selected path.

    Example usage::

        ax = subplot(111)
        ax.plot(x"
4620588,-8421113772908369601,14,BigQuery,"-loss-model.h (module 'propagation'): double ns3::FixedRssLossModel::DoCalcRxPower(double txPowerDbm, ns3::Ptr<ns3::MobilityModel> a, ns3::Ptr<ns3::MobilityModel> b) const [member function]
    cls.add_method('DoCalcRxPower', 
                   'double', 
                   [param('double', 'txPowerDbm'), param('ns3::Ptr< ns3::MobilityModel >', 'a'), param('ns3::Ptr< ns3::MobilityModel >', 'b')], 
                   is_const"
6681883,-1818784154566338459,8,BigQuery,"classroom__name='Main') & Q(school__classroom__has_blackboard=None))
        )
        self.assertSequenceEqual(qs, [hp])


class DisjunctionPromotionTests(TestCase):
    def test_disjunction_promotion_select_related(self):
        fk1 = FK1.objects.create(f1='f1', f2='f2')
        basea = BaseA.objects.create(a=fk1)
        qs = BaseA.objects.filter(Q(a=fk1) | Q(b=2))
        self.assertEqual("
4559385,-7599923305799131181,7,BigQuery,"ub, chebmul, chebdiv

    Examples
    --------

    """"""
    # c is a trimmed copy
    [c] = pu.as_series([c])
    power = int(pow)
    if power!= pow or power < 0:
        raise ValueError(""Power must be a non-negative integer."")
    elif maxpower is not None and power > maxpower:
        raise ValueError(""Power is too large"")
    elif power == 0:
        return np.array([1], dtype=c.dtype)
    elif power == 1:
        return c
    else:
        # This can be made more"
6151708,4062402789259537002,14,BigQuery,"':
        raise ValueError(
            '%s.  Tensor %s must have rank %d.  Received rank %d, shape %s' %
            (message, x.name, e.args[2], e.args[1], x.get_shape()))
      else:
        raise

  return assert_op


def assert_rank_at_least(
    x, rank, data=None, summarize=None, message=None, name=None):
  """"""Assert `x` has rank equal to `rank` or higher.

  Example of adding a dependency to an operation:

  ```python
  with tf."
3453320,1287876923377441289,11,BigQuery,"__new__(cls)
            self.__setstate(year)
            return self
        _check_date_fields(year, month, day)
        self = object.__new__(cls)
        self.__year = year
        self.__month = month
        self.__day = day
        return self

    # Additional constructors

    def fromtimestamp(cls, t):
        ""Construct a date from a POSIX timestamp (like time.time()).""
        y, m, d, hh, mm, ss, weekday, jday, dst = _time.localtime(t)
        return cls(y"
4045941,250685876571803567,29,BigQuery,"]!=None):
            # The username/password details were supplied to the
            # setproxy method so we support the USERNAME/PASSWORD
            # authentication (in addition to the standard none).
            self.sendall(struct.pack('BBBB', 0x05, 0x02, 0x00, 0x02))
        else:
            # No username/password were entered, therefore we
            # only support connections with no authentication.
            self.sendall(struct.pack('BBB', 0x05, 0x01, 0x00))
        # We'll receive the server's response to determine which
        # method was selected
        ch"
1271919,-6527001648686053509,69,BigQuery," to be a datetime.datetime object (so no time.struct_time or mx.DateTime.DateTime here!)
            dt_value = datetime.strptime(src_tstamp_str, src_format)
            if tz_offset and dst_tz_name:
                try:
                    import pytz
                    src_tz = pytz.timezone(server_tz)
                    dst_tz = pytz.timezone(dst_tz_name)
                    src_dt = src_tz.localize(dt_value, is_dst=True)
                    dt_value = src_dt.astimezone("
378761,-5459558469054438694,75,BigQuery,c6b\u3c6c\u3c6d\u3c6e\u3c6f\u3c70\u3c71\u3c72\u3c73\u3c74\u3c75\u3c76\u3c77\u3c78\u3c79\u3c7a\u3c7b\u3c7c\u3c7d\u3c7e\u3c7f\u3c80\u3c81\u3c82\u3c83\u3c84\u3c85\u3c86\u
6515695,-7440500698589276176,37,BigQuery,"multi_version and not self.no_report:
            msg += '\n' + self.__mv_warning
            if self.install_dir not in map(normalize_path, sys.path):
                msg += '\n' + self.__id_warning

        eggloc = dist.location
        name = dist.project_name
        version = dist.version
        extras = ''  # TODO: self.report_extras(req, dist)
        return msg % locals()

    __editable_msg = textwrap.dedent(""""""
        Extracted editable version of %(spec)s to %(dirname)"
1801124,-5299081264266240116,12,BigQuery," to ModelAdmin.formfield_overrides.

FORMFIELD_FOR_DBFIELD_DEFAULTS = {
    models.DateTimeField: {
        'form_class': forms.SplitDateTimeField,
        'widget': widgets.AdminSplitDateTime
    },
    models.DateField: {'widget': widgets.AdminDateWidget},
    models.TimeField: {'widget': widgets.AdminTimeWidget},
    models.TextField: {'widget': widgets.AdminTextareaWidget},
    models.URLField: {'widget': widgets.AdminURLFieldWidget},
    models.IntegerField: {'widget': widgets.AdminIntegerFieldWidget"
2528168,5301422525985223415,6,BigQuery,"if self.IsNull():
            self[:] = BB
        elif N.isnan(BB).all(): ## BB may be a regular array, so I can't use IsNull
            pass
        else:
            if BB[0,0] < self[0,0]: self[0,0] = BB[0,0]
            if BB[0,1] < self[0,1]: self[0,1] = BB[0,1]
            if BB[1,0] > self[1,0]: self[1,0] = BB[1,0]
            if BB[1,1] > self[1,1"
621007,-6774148237436082496,11,BigQuery,"int'),
            max_connections=dict(type='int'),
            max_replication_lag=dict(type='int'),
            use_ssl=dict(type='bool'),
            max_latency_ms=dict(type='int'),
            comment=dict(default='', type='str'),
            state=dict(default='present', choices=['present',
                                                   'absent']),
            save_to_disk=dict(default=True, type='bool'),
            load_to_runtime=dict(default=True, type='bool')
        ),
        supports_check_mode=True
    )

    perform_"
5771746,-7015992225116422527,9,BigQuery," self.connection.settings_dict['TEST_NAME']:
                name = self.connection.settings_dict['TEST_NAME']
        except AttributeError:
            pass
        return name

    def _test_database_create(self):
        return self.connection.settings_dict.get('TEST_CREATE', True)

    def _test_user_create(self):
        return self.connection.settings_dict.get('TEST_USER_CREATE', True)

    def _test_database_user(self):
        name = TEST_DATAB"
1952454,4127297146065790005,12,BigQuery," under the same terms as Python itself, so long as this copyright message
and disclaimer are retained in their original form.

IN NO EVENT SHALL THE AUTHOR BE LIABLE TO ANY PARTY FOR DIRECT, INDIRECT,
SPECIAL, INCIDENTAL, OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OF
THIS CODE, EVEN IF THE AUTHOR HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH
DAMAGE.

THE AUTHOR SPECIFICALLY DISCLAIMS ANY WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A
PART"
1240580,502024032941857243,8,BigQuery,":maybe-no-member makes sense due to fuzzy types
            # pylint: disable=maybe-no-member
            ind = None
            if curr_value:
                try:
                    ind = entry.index(curr_value)
                except ValueError:
                    return (False, self.yaml_dict)

            elif index is not None:
                ind = index

            if ind is not None and entry[ind]!= value:
                entry[ind] = value
                return (True, self.yaml_dict)

            # see if it exists in the list
            try:
                ind = entry.index(value)
"
514718,3339072336767172430,15,BigQuery,"
        ndtype = float
        assert_equal(easy_dtype(ndtype), np.dtype(float))
        # As string w/o names
        ndtype = ""i4, f8""
        assert_equal(easy_dtype(ndtype),
                     np.dtype([('f0', ""i4""), ('f1', ""f8"")]))
        # As string w/o names but different default format
        assert_equal(easy_dtype(ndtype, defaultfmt=""field_%03i""),
                     np.dtype([('field_000', ""i4""), ('field_001', ""f8"")]"
802871,-6534035642489907465,39,BigQuery," to avoid some race conditions
# in the threading module. See http://code.djangoproject.com/ticket/2330.
try:
    import threading  # NOQA
except ImportError:
    pass

try:
    import termios
except ImportError:
    termios = None

USE_INOTIFY = False
try:
    # Test whether inotify is enabled and likely to work
    import pyinotify

    fd = pyinotify.INotifyWrapper.create().inotify_init()
    if fd >= 0:
        USE_INOTIFY = True
        os"
2210638,-2666512718776600606,12,BigQuery,"::empty, ns3::empty >', u'ns3::McpsDataConfirmCallback')
    typehandlers.add_type_alias(u'ns3::Callback< void, ns3::McpsDataConfirmParams, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty >*', u'ns3::McpsDataConfirmCallback*')
    typehandlers.add_type_alias(u'ns3::Callback< void, ns3::McpsDataConfirmParams, ns3::empty, ns3::empty,"
7500909,-3199826853014501559,360,BigQuery,"|restart|rtoi|save|scale|scope|shortrealtobits|'
             r'showscopes|showvariables|showvars|sreadmemb|sreadmemh|'
             r'stime|stop|strobe|time|timeformat|write)\b', Name.Builtin),

            (r'(byte|shortint|int|longint|integer|time|'
             r'bit|logic|reg|'
             r'supply0|supply1|tri|triand|trior|tri0|tri1|trireg|uwire|wire|wand|wor'
             r's"
6545018,2679567636789322364,6,BigQuery,", BGP is enabled to record peer session status and event information.
              If the value is false, BGP is disabled from recording peer session status and event information.
        choices: ['no_use','true','false']
        default: no_use
    pswd_type:
        description:
            - Enable BGP peers to establish a TCP connection and perform the Message Digest 5 (MD5)
              authentication for BGP messages.
        choices: ['null','cipher','simple']
    pswd_cipher_text:
        description:
            - The character string in a password identifies the contents of the password, spaces not supported.
              The value is"
3378214,3584385274275981263,41,BigQuery,"str(''))
        self.assertEqual(q.getlist('foo'), [])
        if not six.PY3:
            self.assertEqual(q.has_key('foo'), False)
        self.assertEqual('foo' in q, False)
        self.assertEqual(list(six.iteritems(q)), [])
        self.assertEqual(list(six.iterlists(q)), [])
        self.assertEqual(list(six.iterkeys(q)), [])
        self.assertEqual(list(six.itervalues(q)), [])
        self.assertEqual("
4105908,-8229234955756586415,7,BigQuery," ):
        # Read variables from state
        ghostState = state.getGhostState( self.index )
        legalActions = state.getLegalActions( self.index )
        pos = state.getGhostPosition( self.index )
        isScared = ghostState.scaredTimer > 0

        speed = 1
        if isScared: speed = 0.5

        actionVectors = [Actions.directionToVector( a, speed ) for a in legalActions]
        newPositions = [( pos[0]+a[0], pos[1]+a[1] ) for a in actionVectors]
        pacmanPosition = state"
6773063,-5153614559138622313,24,BigQuery,"
    ## attribute.h (module 'core'): ns3::Ptr<ns3::AttributeValue> ns3::AttributeChecker::CreateValidValue(ns3::AttributeValue const & value) const [member function]
    cls.add_method('CreateValidValue', 
                   'ns3::Ptr< ns3::AttributeValue >', 
                   [param('ns3::AttributeValue const &', 'value')], 
                   is_const=True)
    ## attribute.h (module 'core'): std::string ns3::AttributeChecker::GetUnderlyingTypeInformation() const [member function]
    cls.add_method('GetUnderlyingType"
4756722,-3207111018826203478,8,BigQuery,"name=fname):
                setattr(self[0], fname, value)
            setattr(CTypesPtr, fname, property(getter, setter))

    def new_function_type(self, BArgs, BResult, has_varargs):
        nameargs = [BArg._get_c_name() for BArg in BArgs]
        if has_varargs:
            nameargs.append('...')
        nameargs = ', '.join(nameargs)
        #
        class CTypesFunctionPtr(CTypesGenericPtr):
            __slots__ = ['_own_callback', '_name']
"
75900,4528052943114427007,81,BigQuery," (most recent call last):
SyntaxError: can't assign to operator

>>> (x for x in x) = 1
Traceback (most recent call last):
SyntaxError: can't assign to generator expression

>>> 1 = 1
Traceback (most recent call last):
SyntaxError: can't assign to literal

>>> ""abc"" = 1
Traceback (most recent call last):
SyntaxError: can't assign to literal

>>> b"""" = 1
Traceback (most recent call last):
SyntaxError: can't assign to literal

>>> `1` = 1
Traceback (most recent call last):
Sy"
5953544,-8621558465027128066,6,BigQuery,"ix = namePrefix
    self.maxTokens = maxTokens

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None:
      fastbinary.decode_binary(self, iprot.trans, (self.__class__, self.thrift_spec))
      return
    iprot.readStructBegin()
    while True:
      (fname, ftype, fid) = iprot.readFieldBegin()
"
575297,-3411276071541445202,9,BigQuery,"', Text),
            (r'<!--', Comment),
            (r'//.*?\n', Comment.Single),
            (r'/\*.*?\*/', Comment.Multiline)
        ],
       'slashstartsregex': [
            include('commentsandwhitespace'),
            (r'/(\\.|[^[/\\\n]|\[(\\.|[^\]\\\n])*])+/'
             r'([gim]+\b|\B)', String.Regex, '#pop'),
            (r'(?=/)', Text, ('#pop', 'badregex')"
5878425,1151933514567753255,19,BigQuery,"=1),
                              test_scores_batch.mean(axis=1))


def test_learning_curve_n_sample_range_out_of_bounds():
    X, y = make_classification(n_samples=30, n_features=1, n_informative=1,
                               n_redundant=0, n_classes=2,
                               n_clusters_per_class=1, random_state=0)
    estimator = MockImprovingEstimator(20)
    assert_raises(ValueError, learning_curve, estimator, X, y, cv=3,"
5544771,1479500878533235650,9,BigQuery,"_processed, namespace_, name_='roleValue')
        if self.hasContent_():
            outfile.write('>')
            outfile.write(str(self.valueOf_).encode(ExternalEncoding))
            self.exportChildren(outfile, level + 1, namespace_, name_, pretty_print=pretty_print)
            showIndent(outfile, level, pretty_print)
            outfile.write('</%s%s>%s' % (namespace_, name_, eol_))
        else:
            outfile.write('/>%s' % (eol_, ))
    def"
6212165,4340091732964610794,11,BigQuery,"
            ""whatsit_set-0-index"": """",
            ""whatsit_set-0-name"": """",
            ""whatsit_set-1-owner"": ""1"",
            ""whatsit_set-1-index"": """",
            ""whatsit_set-1-name"": """",
            ""whatsit_set-2-owner"": ""1"",
            ""whatsit_set-2-index"": """",
            ""whatsit_set-2-name"": """",

            ""fancydoodad_set-TOTAL_FORMS"": ""3"",
            ""fancydoodad"
1119347,4094638617438121454,6,BigQuery,"_))
    if (self.has_how_):
      n += 1
      n += self.lengthVarInt64(self.how_)
    if (self.has_send_offset_):
      n += 1
      n += self.lengthVarInt64(self.send_offset_)
    return n

  def Clear(self):
    self.clear_socket_descriptor()
    self.clear_how()
    self.clear_send_offset()

  def OutputUnchecked(self, out):
    out.putVarInt32(10)
    out.putPrefixedString(self.socket_descriptor_"
2058305,-6983785835040540209,6,BigQuery,"setRadius(%.4f)\n' % self.emitter.getRadius())
            if (eType == ""ETCUSTOM""):
                file.write(i2+targ + '.emitter.setOuterAngle(%.4f)\n' % self.emitter.getOuterAngle())
                file.write(i2+targ + '.emitter.setInnerAngle(%.4f)\n' % self.emitter.getInnerAngle())
                file.write(i2+targ + '.emitter.setOuterMagnitude(%.4f)\n' % self.emitter.getOut"
1179863,2043879589411355444,7,BigQuery,"other, self.__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class TSentryPrivilege(object):
  """"""
  Attributes:
   - component
   - serviceName
   - authorizables
   - action
   - createTime
   - grantorPrincipal
   - grantOption
  """"""

  thrift_spec = (
    None, # 0
    (1, TType.STRING, 'component', None, None, ), # 1
    (2, TType.STRING,'serviceName', None, None,"
2999391,-4912826891055495437,17,BigQuery,"Assump(Q.irrational)),
    (Number, CheckOldAssump(Q.even)),
    (Number, CheckOldAssump(Q.odd)),
    (Number, CheckOldAssump(Q.integer)),
    (Number, CheckOldAssump(Q.imaginary)),
    # For some reason NumberSymbol does not subclass Number
    (NumberSymbol, CheckOldAssump(Q.negative)),
    (NumberSymbol, CheckOldAssump(Q.zero)),
    (NumberSymbol, CheckOldAssump(Q.positive)),
    (NumberSymbol, CheckOldAssump(Q.nonnegative)),
    (NumberSymbol,"
1861677,2063195234929575157,11,BigQuery,"x005d, 0x005e, 0x005f,
  0x0060, 0x0061, 0x0062, 0x0063, 0x0064, 0x0065, 0x0066, 0x0067,
  0x0068, 0x0069, 0x006a, 0x006b, 0x006c, 0x006d, 0x006e, 0x006f,
  0x0070, 0x0071, 0x0072, 0x0073, 0x0074, 0x0075, 0x0076, 0x0077,
  0x0078, 0x00"
2862961,2738772680822400645,38,BigQuery,"32', 'DSCP_AF33', 'DSCP_CS4', 'DSCP_AF41', 'DSCP_AF42', 'DSCP_AF43', 'DSCP_CS5', 'DSCP_EF', 'DSCP_CS6', 'DSCP_CS7'], outer_class=root_module['ns3::Ipv4Header'], import_from_module='ns.internet')
    ## ipv4-header.h (module 'internet'): ns3::Ipv4Header::EcnType [enumeration]
    module.add_enum('EcnType', ['ECN_NotECT', 'ECN_ECT"
6681883,-2367522292491763329,8,BigQuery,", i3]
        )


class Ticket23622Tests(TestCase):
    @skipUnlessDBFeature('can_distinct_on_fields')
    def test_ticket_23622(self):
        """"""
        Make sure __pk__in and __in work the same for related fields when
        using a distinct on subquery.
        """"""
        a1 = Ticket23605A.objects.create()
        a2 = Ticket23605A.objects.create()
        c1 = Ticket23605C.objects.create(field_c0=0.0)
        Ticket23605B.objects.create(
            modela_"
1240288,9174644406967996381,6,BigQuery,"type.NamedType(
            'ckuann', CAKeyUpdAnnContent().subtype(
                explicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatConstructed, 15)
            )
        ),
        namedtype.NamedType(
            'cann', CertAnnContent().subtype(
                explicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatConstructed, 16)
            )
        ),
        namedtype.NamedType(
            'rann', RevAnnContent().subtype(
                explicitTag=tag.Tag(tag.tagClassContext, tag.tagFormatConstructed, 17"
5136130,6502494982129117551,7,BigQuery,"', 
                   [param('ns3::Time const &', 'time')], 
                   is_static=True)
    return

def register_Ns3SystemCondition_methods(root_module, cls):
    ## system-condition.h (module 'core'): ns3::SystemCondition::SystemCondition(ns3::SystemCondition const & arg0) [copy constructor]
    cls.add_constructor([param('ns3::SystemCondition const &', 'arg0')])
    ## system-condition.h (module 'core'): ns3::SystemCondition::SystemCondition() [constructor]
    cls.add_constructor([])"
90558,-4092979108572048894,16,BigQuery," random sample model
            y_pred = base_estimator.predict(X)
            if y_pred.ndim == 1:
                y_pred = y_pred[:, None]

            residuals_subset = residual_metric(y_pred - y)

            # classify data into inliers and outliers
            inlier_mask_subset = residuals_subset < residual_threshold
            n_inliers_subset = np.sum(inlier_mask_subset)

            # less inliers -> skip current random sample
            if n_inliers_subset < n_inliers"
378761,-4605096970426761634,75,BigQuery,\u6a7c\u6a7d\u6a7e\u6a7f\u6a80\u6a81\u6a82\u6a83\u6a84\u6a85\u6a86\u6a87\u6a88\u6a89\u6a8a\u6a8b\u6a8c\u6a8d\u6a8e\u6a8f\u6a90\u6a91\u6a92\u6a93\u6a94\u6a95\u6a96\u6a97
6560225,-6062983389170402251,6,BigQuery,"url) module instead.
author:
- Jan-Piet Mens (@jpmens)
'''

EXAMPLES = r'''
- name: Download foo.conf
  get_url:
    url: http://example.com/path/file.conf
    dest: /etc/foo.conf
    mode: 0440

- name: Download file and force basic auth
  get_url:
    url: http://example.com/path/file.conf
    dest: /etc/foo.conf
    force_basic_auth: yes

- name: Download file with custom HTTP headers
  get_url:
    url:"
2271380,-1468561869404723790,9,BigQuery," ""
                ""when ""
                ""used with get_resource_filename. Consider a more secure ""
                ""location (set with.set_extraction_path or the ""
                ""PYTHON_EGG_CACHE environment variable)."" % path
            )
            warnings.warn(msg, UserWarning)

    def postprocess(self, tempname, filename):
        """"""Perform any platform-specific postprocessing of `tempname`

        This is where Mac header rewrites should be done; other platforms don't
        have anything special they should do.

        Resource providers should call this method ONLY after successfully
        extracting a compressed resource.  "
4363864,-3994878172109568534,70,BigQuery,"
             r'(?<=(?:\s|;)if\s)|'
             r'(?<=(?:\s|;)elsif\s)|'
             r'(?<=^when\s)|'
             r'(?<=^index\s)|'
             r'(?<=^scan\s)|'
             r'(?<=^sub\s)|'
             r'(?<=^gsub\s)|'
             r'(?<=^sub!\s)|'
             r'(?<=^gsub!\s)|'
             r'(?<=^"
1664238,-4659752685963007117,6,BigQuery,"0] == char_type:
                            chunks[-1][1].append(c)
                        else:
                            chunks.append((char_type, [c]))
                    use_simple_method = (len(chunks) == 1 and
                                         chunks[-1][0] == 1)
            return use_simple_method, chunks

        def draw_text_simple():
            """"""Outputs text using the simple method.""""""
            self.file.output(Op.begin_text,
                             self.file.fontName(prop),
                             fontsize,
                             Op.selectfont)
            self._setup_textpos(x,"
1104202,-8942838568090713421,6,BigQuery,"_types' %
                    self.__protorpc_package)
            if type_info.type_name.startswith('extra_types.'):
                self.__AddImport(
                    'from %s import extra_types' % self.__base_files_package)
            return type_info

        if type_name in self.PRIMITIVE_TYPE_INFO_MAP:
            type_info = self.PRIMITIVE_TYPE_INFO_MAP[type_name]
            if type_info.type_name.startswith('extra_types.'):
                self.__AddImport(
                    'from %s import extra_"
2196068,-1983342768019493317,115,BigQuery,"#
#    OpenERP, Open Source Business Applications
#    Copyright (C) 2004-2012 OpenERP S.A. (<http://openerp.com>).
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR"
1029243,2332453562944378992,8,BigQuery,".add_method('IsBroadcast', 
                   'bool', 
                   [], 
                   is_const=True, is_virtual=True)
    ## virtual-net-device.h: bool ns3::VirtualNetDevice::IsLinkUp() const [member function]
    cls.add_method('IsLinkUp', 
                   'bool', 
                   [], 
                   is_const=True, is_virtual=True)
    ## virtual-net-device.h: bool ns3::VirtualNetDevice::IsMulticast() const [member function]
    cls.add_method('IsMulticast', 
                   'bool"
1786173,8395818426905191696,14,BigQuery," void ns3::Object::DoInitialize() [member function]
    cls.add_method('DoInitialize', 
                   'void', 
                   [], 
                   visibility='protected', is_virtual=True)
    ## object.h (module 'core'): void ns3::Object::NotifyNewAggregate() [member function]
    cls.add_method('NotifyNewAggregate', 
                   'void', 
                   [], 
                   visibility='protected', is_virtual=True)
    return

def register_Ns3ObjectAggregateIterator_methods(root_module, cls):
    ## object."
3589631,-5942043085369791973,12,BigQuery,"
        text = self.tree.find(text=""Three"")
        self.assertEqual(text.find_previous(""b"").string, ""Three"")
        self.assertSelects(
            text.find_all_previous(""b""), [""Three"", ""Two"", ""One""])

    def test_previous_generator(self):
        start = self.tree.find(text=""One"")
        predecessors = [node for node in start.previous_elements]

        # There are four predecessors: the <b> tag containing ""One""
        # the <body> tag, the <head> tag, and the <html>"
5621125,4313843680118810163,28,BigQuery,"I32:
          self.error_time_secs = iprot.readI32();
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary is not None:
      oprot.trans.write(fastbinary.encode_binary(self, (self.__class__, self.thrift_spec)))
"
2271380,7120375919918974431,9,BigQuery,"(attrs)
        self.extras = tuple(extras)
        self.dist = dist

    def __str__(self):
        s = ""%s = %s"" % (self.name, self.module_name)
        if self.attrs:
            s += ':' + '.'.join(self.attrs)
        if self.extras:
            s +='[%s]' % ','.join(self.extras)
        return s

    def __repr__(self):
        return ""EntryPoint.parse(%r)"" % str(self)

    def load(self, require=True, *"
6106070,-6840465419150528865,8,BigQuery,"] + "".py""
        versioneer_file = os.path.relpath(me)
    except NameError:
        versioneer_file = ""versioneer.py""
    files.append(versioneer_file)
    present = False
    try:
        f = open("".gitattributes"", ""r"")
        for line in f.readlines():
            if line.strip().startswith(versionfile_source):
                if ""export-subst"" in line.strip().split()[1:]:
                    present = True
        f.close()
    except EnvironmentError:
        pass
    if not present:"
5197098,3782545847440998880,12,BigQuery,">`_ as a server.

        :param callback: The callback function.  It will be invoked with one
            argument, the Connection instance.  It should return a list of
            bytestrings representing the advertised protocols, like
            ``[b'http/1.1', b'spdy/2']``.
        """"""
        self._npn_advertise_helper = _NpnAdvertiseHelper(callback)
        self._npn_advertise_callback = self._npn_advertise_helper.callback
        _lib.SSL_CTX_set_next_protos_advertised_cb(
            self._context, self"
6166744,-8193553255586402774,13,BigQuery,"learn.metrics.scorer` submodule implements a flexible
interface for model selection and evaluation using
arbitrary score functions.

A scorer object is a callable that can be passed to
:class:`sklearn.grid_search.GridSearchCV` or
:func:`sklearn.cross_validation.cross_val_score` as the ``scoring`` parameter,
to specify how a model should be evaluated.

The signature of the call is ``(estimator, X, y)`` where ``estimator``
is the model to be evaluated, ``X`` is the test data and ``y`` is the
ground truth labeling (or ``None`` in"
5484371,6254574317197940212,9,BigQuery,"
            raise TypeError('tb_set_next arguments must be traceback objects')
        obj = _Traceback.from_address(id(tb))
        if tb.tb_next is not None:
            old = _Traceback.from_address(id(tb.tb_next))
            old.ob_refcnt -= 1
        if next is None:
            obj.tb_next = ctypes.POINTER(_Traceback)()
        else:
            next = _Traceback.from_address(id(next))
            next.ob_refcnt += 1
            obj.tb"
5544759,-7518261832299676106,9,BigQuery,"2032,10,31,1,0,0),
d(2033,3,27,1,0,0),
d(2033,10,30,1,0,0),
d(2034,3,26,1,0,0),
d(2034,10,29,1,0,0),
d(2035,3,25,1,0,0),
d(2035,10,28,1,0,0),
d(2036,3,30,1,0,0),
d(2036,10,26,1,0,0),
d(2037,3,29"
877554,7876643004543044275,7,BigQuery," An AvgPool Operation.

  Returns:
    A single-element list containing the Shape of the AvgPool output.

  Raises:
    ValueError: If the shape of the input is invalid or incompatible with
      the values of the attrs.
  """"""
  input_shape = op.inputs[0].get_shape().with_rank(4)
  try:
    data_format = op.get_attr(""data_format"")
  except ValueError:
    data_format = None

  if data_format == b""NCHW"":
    # Convert input shape to the default NHWC for inference.
    input_shape = [input_"
3589631,-134361100026003426,12,BigQuery,"):
        soup = self.soup(""<b>Argh!</b>"")
        soup.find(text=""Argh!"").replace_with(""Hooray!"")
        new_text = soup.find(text=""Hooray!"")
        b = soup.b
        self.assertEqual(new_text.previous_element, b)
        self.assertEqual(new_text.parent, b)
        self.assertEqual(new_text.previous_element.next_element, new_text)
        self.assertEqual(new_text.next_element, None)

    def test_"
5848155,-8319505986652456621,25,BigQuery," management is used improperly.
    """"""
    pass


def get_connection(using=None):
    """"""
    Get a database connection by name, or the default database connection
    if no name is provided. This is a private API.
    """"""
    if using is None:
        using = DEFAULT_DB_ALIAS
    return connections[using]


def get_autocommit(using=None):
    """"""
    Get the autocommit status of the connection.
    """"""
    return get_connection(using).get_autocommit()


def set_autocommit(autocommit, using=None):
"
377975,-306038001966867540,8,BigQuery,"pwuid_r(', _UNSAFE_FUNC_PREFIX + r'getpwuid\([^)]+\)'),
    ('gmtime(', 'gmtime_r(', _UNSAFE_FUNC_PREFIX + r'gmtime\([^)]+\)'),
    ('localtime(', 'localtime_r(', _UNSAFE_FUNC_PREFIX + r'localtime\([^)]+\)'),
    ('rand(', 'rand_r(', _UNSAFE_FUNC_PREFIX + r'rand\(\)'),
    ('strtok(','strtok_"
6166978,251406471377030368,24,BigQuery,"_module['ns3::RandomVariableStream'])
    ## aloha-noack-mac-header.h (module'spectrum'): ns3::AlohaNoackMacHeader [class]
    module.add_class('AlohaNoackMacHeader', parent=root_module['ns3::Header'])
    ## attribute.h (module 'core'): ns3::AttributeAccessor [class]
    module.add_class('AttributeAccessor', import_from_module='ns.core', parent=root_module['ns3::SimpleRefCount< ns3::AttributeAccessor, ns3::empty, ns3::DefaultDeleter<ns3::"
4574809,-5131440182855782067,9,BigQuery,"TER IE
	0x00e6: 0x0436,	# CYRILLIC SMALL LETTER ZHE
	0x00e7: 0x0437,	# CYRILLIC SMALL LETTER ZE
	0x00e8: 0x0438,	# CYRILLIC SMALL LETTER I
	0x00e9: 0x0439,	# CYRILLIC SMALL LETTER SHORT I
	0x00ea: 0x043a,	# CYRILLIC SMALL LETTER KA
	0x00eb: 0x043b,	# CYRILLIC SMALL LETTER EL
"
7288269,3818693514979131901,8,BigQuery,"operation(self):
        # All integer
        for dt1 in np.typecodes['AllInteger']:
            assert_(1 > np.array(0, dtype=dt1), ""type %s failed"" % (dt1,))
            assert_(not 1 < np.array(0, dtype=dt1), ""type %s failed"" % (dt1,))

            for dt2 in np.typecodes['AllInteger']:
                assert_(np.array(1, dtype=dt1) > np.array(0, dtype=dt2),
                        ""type %s and %s failed"" % (dt1, dt2"
1771735,-2671312862106363902,58,BigQuery,"_CALLBACKS' : [ 0x68, {
    'SizeOfFsFilterCallbacks' : [ 0x0, ['unsigned long']],
    'Reserved' : [ 0x4, ['unsigned long']],
    'PreAcquireForSectionSynchronization' : [ 0x8, ['pointer64', ['void']]],
    'PostAcquireForSectionSynchronization' : [ 0x10, ['pointer64', ['void']]],
    'PreReleaseForSectionSynchronization' : [ 0x18, ['pointer64', ['void']]],
    'PostReleaseForSectionSynchronization' : [ 0x20, ['pointer"
7500897,2662222878167481575,94,BigQuery," no concept of ""escaping"", to display one of
    the bits used in template tags, you must use the ``{% templatetag %}`` tag.

    The argument tells which template bit to output:

        ==================  =======
        Argument            Outputs
        ==================  =======
        ``openblock``       ``{%``
        ``closeblock``      ``%}``
        ``openvariable``    ``{{``
        ``closevariable``   ``}}``
        ``openbrace``       ``{``
        ``closebrace``      ``}``
        ``opencomment``     ``{#``
        ``closecomment``    ``"
5621299,6763300643237023227,72,BigQuery,"ittest_reportflags = flags
    return old


class DocTestCase(unittest.TestCase):

    def __init__(self, test, optionflags=0, setUp=None, tearDown=None,
                 checker=None):

        unittest.TestCase.__init__(self)
        self._dt_optionflags = optionflags
        self._dt_checker = checker
        self._dt_test = test
        self._dt_setUp = setUp
        self._dt_tearDown = tearDown

    def setUp(self):
        test = self._dt_test

        if self._dt"
817976,3116770146447920479,129,BigQuery,"            request=request,
            session=session,
            g=g
        )
        rv.filters['tojson'] = json.tojson_filter
        return rv

    def create_global_jinja_loader(self):
        """"""Creates the loader for the Jinja2 environment.  Can be used to
        override just the loader and keeping the rest unchanged.  It's
        discouraged to override this function.  Instead one should override
        the :meth:`jinja_loader` function instead.

        The global loader dispatches between the loaders of the application
        and the individual blueprints.

       .. versionadded::"
1786173,7932953897026223952,14,BigQuery,"s.add_method('MessageFront', 
                   'ns3::Ptr< ns3::PbbMessage >', 
                   [])
    ## packetbb.h (module 'network'): ns3::Ptr<ns3::PbbMessage> const ns3::PbbPacket::MessageFront() const [member function]
    cls.add_method('MessageFront', 
                   'ns3::Ptr< ns3::PbbMessage > const', 
                   [], 
                   is_const=True)
    ## packetbb.h (module 'network'): void ns3::PbbPacket::MessagePopBack() [member function]
    cl"
14636,-4089895790618014398,11,BigQuery,"
                        _logger.info('remaining requests')
                        openerp.tools.misc.dumpstacks()
                        t0 = t1

    def phantom_js(self, url_path, code, ready=""window"", login=None, timeout=60, **kw):
        """""" Test js code running in the browser
        - optionnally log as 'login'
        - load page given by url_path
        - wait for ready object to be available
        - eval(code) inside the page

        To signal success test do:
        console.log('ok')

        To signal failure do:
        console.log('error')

        If neither are"
6758103,4921978907744904977,26,BigQuery,"_start': balance_start, 'company_id': company_id, 'currency': currency_id}}

    def unlink(self, cr, uid, ids, context=None):
        stat = self.read(cr, uid, ids, ['state'], context=context)
        unlink_ids = []
        for t in stat:
            if t['state'] in ('draft'):
                unlink_ids.append(t['id'])
            else:
                raise osv.except_osv(_('Invalid Action!'), _('In order to delete a bank statement, you must first cancel it to delete related journal items.'))"
4726612,8549890155399444935,28,BigQuery,"desc):
    """"""Constructor for ResourceMethodParameters.

    Sets default values and defers to set_parameters to populate.

    Args:
      method_desc: Dictionary with metadata describing an API method. Value
          comes from the dictionary of methods stored in the'methods' key in
          the deserialized discovery document.
    """"""
    self.argmap = {}
    self.required_params = []
    self.repeated_params = []
    self.pattern_params = {}
    self.query_params = []
    # TODO(dhermes): Change path_params to a list if the extra URITEMPLATE
    #"
5665776,8654559234622034986,7,BigQuery," a list, or a dictionary with the
      same keys as `fetches` if that is a dictionary
      (see documentation for `run`).

    Raises:
      tf.errors.OpError: Or one of its subclasses on error.
    """"""
    # TODO(touts): Support feeding and fetching the same tensor.
    return self._run(handle, fetches, feed_dict, None, None)

  def partial_run_setup(self, fetches, feeds=None):
    """"""Sets up a graph with feeds and fetches for partial run.

    This is EXPERIMENTAL and subject to change.

    Note that"
5075938,-5085569176802469944,18,BigQuery," palettes or
        transparency information.

        Like the :meth:`read` method this method returns a 4-tuple:

        (*width*, *height*, *pixels*, *meta*)

        This method normally returns pixel values with the bit depth
        they have in the source image, but when the source PNG has an
        ``sBIT`` chunk it is inspected and can reduce the bit depth of
        the result pixels; pixel values will be reduced according to
        the bit depth specified in the ``sBIT`` chunk (PNG nerds should
        note a single result bit depth is used for all channels; the
        maximum of the ones specified in the ``sBIT`` chunk"
5514998,686142492594039002,20,BigQuery,"

        a = array([[1, 1], [3, 3]])
        b = array([1, 1], mask=[0, 1])
        a *= b
        assert_equal(a, [[1, 1], [3, 3]])
        assert_equal(a.mask, [[0, 1], [0, 1]])

    def test_inplace_addition_scalar_type(self):
        # Test of inplace additions
        for t in self.othertypes:
            with warnings.catch_warnings(record=True) as w:
                warnings.filterwarnings(""always"")
                (x, y"
5923343,1823644353375397168,8,BigQuery,"13, cradius)
            sketch.setDatum(15, cdepth)
            if sketch.ExternalGeometry[1]!= (support,  elements[0]):
                # Update the external geometry references
                angle = sketch.Constraints[12].Value
                sketch.delConstraint(19)
                sketch.delConstraint(18)
                sketch.delConstraint(17)
                sketch.delConstraint(16)
                sketch.delConstraint(15)
                sketch.delConstraint(14)
                sketch.delCon"
1119851,1510555303132454902,9,BigQuery,"bag':u'⟆', u'\\Re':u'ℜ', u'\\Rparen':u'⦆', 
      u'\\Rrightarrow':u'⇛', u'\\Rsh':u'↱', u'\\S':u'§', u'\\Sigma':u'Σ', 
      u'\\Square':u'☐', u'\\Subset':u'⋐', u'\\Sun':u'☉', u'\\Supset':u'⋑', 
      u'\\Theta':u'Θ', u'\\Uparrow':u'�"
6637234,-1854693289656914043,40,BigQuery," the ""formatter"" argument passed in
    # to methods like encode() and prettify():
    #
    # ""html"" - All Unicode characters with corresponding HTML entities
    #   are converted to those entities on output.
    # ""minimal"" - Bare ampersands and angle brackets are converted to
    #   XML entities: &amp; &lt; &gt;
    # None - The null formatter. Unicode characters are never
    #   converted to entities.  This is not recommended, but it's
    #   faster than ""minimal"".
    # A function - This function will be called on every string that
    #  needs to undergo entity"
7015701,-4586354800894171879,80,BigQuery,".2.1) support requests for URLs containing
        # userinfo.
        authority = req.host
        response = self.http_error_auth_reqed('proxy-authenticate',
                                          authority, req, headers)
        self.reset_retry_count()
        return response


# Return n random bytes.
_randombytes = os.urandom


class AbstractDigestAuthHandler(object):
    # Digest authentication is specified in RFC 2617.

    # XXX The client does not inspect the Authentication-Info header
    # in a successful response.

    # XXX It should be possible to test this implementation against
    # a mock server"
6924146,6118001185480390356,17,BigQuery,"name = fieldname
        self.allow_overlap = allow_overlap
        self.split_fn = None
        self.maptype = maptype

    def default_name(self):
        return self.fieldname

    def categorizer(self, searcher):
        return self.StoredFieldCategorizer(self.fieldname, self.allow_overlap,
                                           self.split_fn)

    class StoredFieldCategorizer(Categorizer):
        def __init__(self, fieldname, allow_overlap, split_fn):
            self.fieldname = fieldname
            self.allow_overlap ="
908613,4634958467855098686,15,BigQuery,".location)
        if options.verbose >= 1:
            row.append(get_installer(proj))

        data.append(row)

    return data, header


def format_for_json(packages, options):
    # type: (List[Distribution], Values) -> str
    data = []
    for dist in packages:
        info = {
            'name': dist.project_name,
           'version': six.text_type(dist.version),
        }
        if options.verbose >= 1:
            info['location'] = dist.location
            info['installer'] = get_installer(dist"
4787232,6437103905086096131,7,BigQuery," c_{02}..., c_{10}, c_{11}, c_{12}...

    and ``np.dot(V, c.flat)`` and ``hermval2d(x, y, c)`` will be the same
    up to roundoff. This equivalence is useful both for least squares
    fitting and for the evaluation of a large number of 2-D Hermite
    series of the same degrees and sample points.

    Parameters
    ----------
    x, y : array_like
        Arrays of point coordinates, all of the same shape. The dtypes
        will be converted to either float64 or complex128 depending on
        whether any of the elements"
378761,6922135472065212956,75,BigQuery,2525\u2526\u2527\u2528\u2529\u252a\u252b\u252c\u252d\u252e\u252f\u2530\u2531\u2532\u2533\u2534\u2535\u2536\u2537\u2538\u2539\u253a\u253b\u253c\u253d\u253e\u253f\u2540\u2541\u2542\u2543\u2544\u2545\u2546\u2547\u2548\u2549\u254a
4620588,-9087262619036725604,14,BigQuery,"])
    return

def register_Ns3TypeId_methods(root_module, cls):
    cls.add_binary_comparison_operator('!=')
    cls.add_binary_comparison_operator('<')
    cls.add_output_stream_operator()
    cls.add_binary_comparison_operator('==')
    ## type-id.h (module 'core'): ns3::TypeId::TypeId(char const * name) [constructor]
    cls.add_constructor([param('char const *', 'name')])
    ## type-id.h ("
1786173,-9124453932280594343,14,BigQuery," const &', 'arg0')])
    ## socket-factory.h (module 'network'): ns3::SocketFactory::SocketFactory() [constructor]
    cls.add_constructor([])
    ## socket-factory.h (module 'network'): ns3::Ptr<ns3::Socket> ns3::SocketFactory::CreateSocket() [member function]
    cls.add_method('CreateSocket', 
                   'ns3::Ptr< ns3::Socket >', 
                   [], 
                   is_pure_virtual=True, is_virtual=True)
    ## socket-factory.h (module 'network'): static"
1105424,1707434318728590815,206,BigQuery,"return 'DISTINCT'

    def last_executed_query(self, cursor, sql, params):
        # http://initd.org/psycopg/docs/cursor.html#cursor.query
        # The query attribute is a Psycopg extension to the DB API 2.0.
        if cursor.query is not None:
            return cursor.query.decode('utf-8')
        return None

    def return_insert_id(self):
        return ""RETURNING %s"", ()

    def bulk_insert_sql(self, fields, num_values):
        items_sql = ""(%s)"" % "", ""."
2058230,-6712200150324603565,6,BigQuery,"self.spawn(['mt.exe', '-nologo', '-manifest',
                                mffilename, out_arg])
                except DistutilsExecError as msg:
                    raise LinkError(msg)
        else:
            log.debug(""skipping %s (up-to-date)"", output_filename)

    def manifest_setup_ldargs(self, output_filename, build_temp, ld_args):
        # If we need a manifest at all, an embedded manifest is recommended.
        # See MSDN article titled
        # ""How to: Embed a Manifest Inside a C/C++ Application""
        # (currently at http"
1331325,-7991562460495269625,6,BigQuery,"
        try:
            from fortiosapi import FortiOSAPI
        except ImportError:
            module.fail_json(msg=""fortiosapi module is required"")

        fos = FortiOSAPI()

        login(module.params, fos)
        is_error, has_changed, result = fortios_wireless_controller(module.params, fos)
        fos.logout()

    if not is_error:
        module.exit_json(changed=has_changed, meta=result)
    else:
        module.fail_json(msg=""Error in repo"", meta=result)


if __name__ == '__main"
5590233,-9044630048351072804,9,BigQuery,"
            default_x11_lib_dirs += [os.path.join(""/usr/lib/"", triplet)]
            default_lib_dirs += [os.path.join(""/usr/lib/"", triplet)]
    finally:
        if tmp is not None:
            tmp.close()

if os.path.join(sys.prefix, 'lib') not in default_lib_dirs:
    default_lib_dirs.insert(0, os.path.join(sys.prefix, 'lib'))
    default_include_dirs.append(os.path.join(sys.prefix, 'include'))
"
5106590,-5308370990054653928,29,BigQuery,"s)."" % platform.platform())
            except OSError as e:
                raise CommandError(e)
        if prefixed_path not in self.symlinked_files:
            self.symlinked_files.append(prefixed_path)

    def copy_file(self, path, prefixed_path, source_storage):
        """"""
        Attempt to copy ``path`` with storage
        """"""
        # Skip this file if it was already copied earlier
        if prefixed_path in self.copied_files:
            return self.log(""Skipping '%s' (already copied earlier)"" % path)
        #"
5590233,-8608831253819398127,9,BigQuery," '''
        gbbrd gbcon gbequ gbrfs gbsv gbsvx gbtf2 gbtrf gbtrs gebak
        gebal gebd2 gebrd gecon geequ gees geesx geev geevx gegs gegv
        gehd2 gehrd gelq2 gelqf gels gelsd gelss gelsx gelsy geql2
        geqlf geqp3 geqpf geqr2 geqrf gerfs gerq2 gerqf gesc2 gesdd
        gesv gesvd gesvx getc"
6591691,5828104655572388520,45,BigQuery,"INIT(&PyType_Type, 0)
    ""_ast.AST"",
    sizeof(PyObject),
    0,
    0,                       /* tp_dealloc */
    0,                       /* tp_print */
    0,                       /* tp_getattr */
    0,                       /* tp_setattr */
    0,                       /* tp_reserved */
    0,                       /* tp_repr */
    0,                       /* tp_as_number */
    0,                       /* tp_as_sequence */
    0,                       /* tp_as_mapping */
    0,                       /* tp_"
7560649,-1297486614918309568,6,BigQuery,"params.copy()
    state = item.get('state')

    item['disable'] = True if state in ('disabled', 'absent') else False

    if state in ('enabled', 'disabled'):
        item['state'] = 'present'

    want = map_params_to_obj(module, param_to_xpath_map, param=item)
    ele = map_obj_to_ele(module, want, top, param=item)

    with locked_config(module):
        diff = load_config(module, tostring(ele), warnings, action='merge')

        commit = not module.check_mode
        if diff"
468726,-245564272825629009,7,BigQuery,"to_date()
        self.fail_test()
    return result
  def run_built_executable(self, name, *args, **kw):
    """"""
    Runs an executable built by Visual Studio.
    """"""
    configuration = self.configuration_dirname()
    # Enclosing the name in a list avoids prepending the original dir.
    program = [self.built_file_path(name, type=self.EXECUTABLE, **kw)]
    return self.run(program=program, *args, **kw)
  def built_file_path(self, name, type=None, **kw):
    "
1149724,4540997504305914476,8,BigQuery,"nearest(M, k) - _div_nearest(yshift*w, M)

    return _div_nearest(w*y, M)

def _dlog10(c, e, p):
    """"""Given integers c, e and p with c > 0, p >= 0, compute an integer
    approximation to 10**p * log10(c*10**e), with an absolute error of
    at most 1.  Assumes that c*10**e is not exactly 1.""""""

    # increase precision by 2; compensate for this by dividing
    # final result by 100
    p += 2

    # write c*10**"
4620588,1844629148499540381,14,BigQuery,"ansWifiPhy::SetStbc(bool stbc) [member function]
    cls.add_method('SetStbc', 
                   'void', 
                   [param('bool','stbc')], 
                   is_virtual=True)
    ## yans-wifi-phy.h (module 'wifi'): bool ns3::YansWifiPhy::GetStbc() const [member function]
    cls.add_method('GetStbc', 
                   'bool', 
                   [], 
                   is_const=True, is_virtual=True)
    ## yans-wifi-phy.h ("
712184,6717034997080662332,35,BigQuery,".LONG, scalars.LONG),
    0xBE: (jvmops.LDIV, scalars.LONG, scalars.LONG),
    0xBF: (jvmops.LREM, scalars.LONG, scalars.LONG),
    0xC0: (jvmops.LAND, scalars.LONG, scalars.LONG),
    0xC1: (jvmops.LOR, scalars.LONG, scalars.LONG),
    0xC2: (jvmops.LXOR, scalars.LONG, scalars.LONG),
    0xC3:"
1195335,3249552680653511756,12,BigQuery,"@master...'
stderr:
    description: The command standard error
    returned: always
    type: string
    sample: 'ls: cannot access foo: No such file or directory'
cmd:
    description: The command executed by the task
    returned: always
    type: string
    sample: 'rabbitmqctl join_cluster rabbit@master'
rc:
    description: The command return code (0 means success)
    returned: always
    type: int
    sample: 0
stdout_lines:
    description: The command standard output split in lines
    returned: always
    type: list
    sample"
1179863,-4945439963335590070,7,BigQuery,".__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class TListSentryPrivilegesByAuthRequest(object):
  """"""
  Attributes:
   - protocol_version
   - requestorUserName
   - component
   - serviceName
   - authorizablesSet
   - groups
   - roleSet
  """"""

  thrift_spec = (
    None, # 0
    (1, TType.I32, 'protocol_version', None, 2, ), # 1
    (2, TType.STRING,"
5772155,2315327088304913186,17,BigQuery,"func:`register` to register global listeners for specific events.
Listeners must inherit from one of the abstract classes below and implement
the correct functions for that class.

For example, a simple command logger might be implemented like this::

    import logging

    from pymongo import monitoring

    class CommandLogger(monitoring.CommandListener):

        def started(self, event):
            logging.info(""Command {0.command_name} with request id ""
                         ""{0.request_id} started on server ""
                         ""{0.connection_id}"".format(event))

        def succeeded(self, event):
            logging.info(""Command {"
3695961,8944485478751577388,12,BigQuery," tenant_ip: ""203.0.113.77/23""
# ```



## playbook-add.yml example

---
- hosts: openstack-stage
  remote_user: root
  tasks:

  - name: install needed network manager libs
    yum:
      name: '{{ item }}'
      state: installed
    with_items:
      - NetworkManager-glib
      - libnm-qt-devel.x86_64
      - nm-connection-editor.x86_64
      - libsemanage-python
      - policycoreutils-python

##### Working with all cloud"
3361989,5312080510855066631,6,BigQuery," THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

import operator
import sys
import types

__author__ = ""Benjamin Peterson <benjamin@python.org>""
__version__ = ""1.6.1""


# Useful for very coarse version differentiation.
PY2 = sys.version_info[0] == 2
PY3 = sys.version_info[0] == 3

if PY3:
    string_types = str,
    integer_types = int,
    class_types = type,
    text_type = str
    binary_type = bytes

    MAXSIZE = sys.maxsize"
5137002,7641923686272420322,50,BigQuery,", 11], 4)
        self.assertEntryEqual(a[0], 7)

        self.assertRaises(AttributeError, setattr, a, ""color"", ""blue"")

    def test_frombytearray(self):
        a = array.array('b', range(10))
        b = array.array(self.typecode, a)
        self.assertEqual(a, b)

class SignedNumberTest(NumberTest):
    example = [-1, 0, 1, 42, 0x7f]
    smallerexample = [-1, 0, 1, 42, 0x7e]
    biggerex"
2468954,7758979076264093754,69,BigQuery," had been hooked up using a URLconf.
    """"""
    def __init__(self, **defaults):
        self.defaults = defaults
        self.cookies = SimpleCookie()
        self.errors = StringIO()

    def _base_environ(self, **request):
        """"""
        The base environment for a request.
        """"""
        environ = {
            'HTTP_COOKIE':       self.cookies.output(header='', sep='; '),
            'PATH_INFO':         '/',
            'QUERY_STRING':      '',
            'REMOTE_ADDR':       '127.0."
5908173,-3498633790475821681,9,BigQuery,"df.take(mask[1:],convert=False) ])

        df = gen_test(900,100)
        self.assertFalse(df.index.is_unique)

        mask = np.arange(100)
        result = df.loc[mask]
        expected = gen_expected(df,mask)
        assert_frame_equal(result,expected)

        df = gen_test(900000,100000)
        self.assertFalse(df.index.is_unique)

        mask = np.arange(100000)
        result = df.loc[mask]
        expected = gen_expected(df,mask)"
4332635,-6059106225253076877,6,BigQuery," :class:`~localflavor.us.models.USPostalCodeField`.

   .. versionadded:: 1.1

    """"""
    description = _(""U.S. ZIP code"")

    def __init__(self, *args, **kwargs):
        kwargs['max_length'] = 10
        super(USZipCodeField, self).__init__(*args, **kwargs)

    def formfield(self, **kwargs):
        defaults = {'form_class': forms.USZipCodeField}
        defaults.update(kwargs)
        return super(USZipCodeField, self).formfield(**default"
1558250,-1739338580607774507,6,BigQuery," item['chain']))
        return result

    @property
    def parent(self):
        if self._values['parent'] is None:
            return None
        result = fq_name(self.partition, self._values['parent'])
        return result

    @property
    def cert_key_chain(self):
        if self._values['cert_key_chain'] is None:
            return None
        result = []
        for item in self._values['cert_key_chain']:
            if 'key' in item and 'cert' not in item:
                raise F5ModuleError(
                    ""When providing a 'key',"
196689,1972043590171118293,37,BigQuery,".4
    """"""

    def __init__(self, request_validator=None):
        self.request_validator = request_validator or RequestValidator()

    def create_token_response(self, request, token_handler):
        """"""Return token or error in JSON format.

        If the access token request is valid and authorized, the
        authorization server issues an access token as described in
        `Section 5.1`_.  A refresh token SHOULD NOT be included.  If the request
        failed client authentication or is invalid, the authorization server
        returns an error response as described in `Section 5.2`_.

       .. _`Section 5"
2393140,-6647999804455731601,22,BigQuery," cyan=36, white=37)

    def __init__(self, stream):
        self.stream = stream

    def supported(cls, stream=sys.stdout):
        """"""A class method that returns True if the current platform supports
        coloring terminal output using this method. Returns False otherwise.
        """"""
        if not stream.isatty():
            return False  # auto color only on TTYs
        try:
            import curses
        except ImportError:
            return False
        else:
            try:
                try:
                    return curses.tigetnum(""colors"") > 2
                except curses.error:
                    c"
5166836,1939618701514365440,12,BigQuery,"ensions(
    labels, predictions, expected_rank_diff=0, name=None):
  """"""Squeeze last dim if ranks differ from expected by exactly 1.

  In the common case where we expect shapes to match, `expected_rank_diff`
  defaults to 0, and we squeeze the last dimension of the larger rank if they
  differ by 1.

  But, for example, if `labels` contains class IDs and `predictions` contains 1
  probability per class, we expect `predictions` to have 1 more dimension than
  `labels`, so `expected_rank_diff` would be 1. In this case, we'd"
3803133,-7021597803576343404,71,BigQuery,"',re.I).match
EXTENSIONS = "".tar.gz.tar.bz2.tar.zip.tgz"".split()

__all__ = [
    'PackageIndex', 'distros_for_url', 'parse_bdist_wininst',
    'interpret_distro_name',
]

_SOCKET_TIMEOUT = 15

def parse_bdist_wininst(name):
    """"""Return (base,pyversion) or (None,None) for possible.exe name""""""

    lower = name.lower()
    base, py_ver, plat = None, None, None

    if lower.endswith"
3680391,-7453786193135014531,8,BigQuery,"','set-hostname', name]
        rc, out, err = self.module.run_command(cmd)
        if rc!= 0:
            self.module.fail_json(msg=""Command failed rc=%d, out=%s, err=%s"" %
                (rc, out, err))

    def get_permanent_hostname(self):
        cmd = ['hostnamectl', '--static','status']
        rc, out, err = self.module.run_command(cmd)
        if rc!= 0:
            self.module.fail_json(msg=""Command failed rc=%d, out=%"
4378742,9081981161508511485,17,BigQuery," inserted at every page bottom,
# using the given strftime format.
#html_last_updated_fmt = '%b %d, %Y'

# If true, SmartyPants will be used to convert quotes and dashes to
# typographically correct entities.
html_use_smartypants = False

# Custom sidebar templates, maps document names to template names.
#html_sidebars = {}

# Additional templates that should be rendered to pages, maps page names to
# template names.
#html_additional_pages = {}

# If false, no module index is generated.
#html_domain_indices = True

#"
2771735,895294276828789041,38,BigQuery,"('Murmur3', import_from_module='ns.core', parent=root_module['ns3::Hash::Implementation'])

def register_types_ns3_TracedValueCallback(module):
    root_module = module.get_root()
    
    typehandlers.add_type_alias(u'void ( * ) ( bool, bool ) *', u'ns3::TracedValueCallback::Bool')
    typehandlers.add_type_alias(u'void ( * ) ( bool, bool ) **', u'ns3::TracedValueCallback::Bool*')
    typehandlers.add_type_alias("
4544233,-6075754586214482971,7,BigQuery,").text().toUtf8()
        addressAtCurrentRow = self.ui.tableWidgetBlacklist.item(
            currentRow, 1).text()
        if shared.config.get('bitmessagesettings', 'blackwhitelist') == 'black':
            sqlExecute(
                '''DELETE FROM blacklist WHERE label=? AND address=?''',
                str(labelAtCurrentRow), str(addressAtCurrentRow))
        else:
            sqlExecute(
                '''DELETE FROM whitelist WHERE label=? AND address=?''',
                str(labelAtCurrentRow), str(addressAtCurrentRow))
        self."
2665144,7359801186123349420,16,BigQuery," two Laguerre series `c1` * `c2`.  The arguments
    are sequences of coefficients, from lowest order ""term"" to highest,
    e.g., [1,2,3] represents the series ``P_0 + 2*P_1 + 3*P_2``.

    Parameters
    ----------
    c1, c2 : array_like
        1-D arrays of Laguerre series coefficients ordered from low to
        high.

    Returns
    -------
    out : ndarray
        Of Laguerre series coefficients representing their product.

    See Also
    --------
    lagadd, lagsub, lagdiv"
1149724,6204422637658624656,8,BigQuery,"
            return context._raise_error(InvalidOperation,
                                        'at least one of pow() 1st argument '
                                        'and 2nd argument must be nonzero ;'
                                        '0**0 is not defined')

        # compute sign of result
        if other._iseven():
            sign = 0
        else:
            sign = self._sign

        # convert modulo to a Python integer, and self and other to
        # Decimal integers (i.e. force their exponents to be >= 0)
        modulo = abs(int(modulo))
        base = _WorkRep(self.to_integral_value())
        exp"
4923648,1162630185899356664,7,BigQuery,"
#'preamble': '',
}

# Grouping the document tree into LaTeX files. List of tuples
# (source start file, target name, title, author, documentclass [howto/manual]).
latex_documents = [
  ('index', 'Makahiki.tex', u'Makahiki Documentation',
   u'George Lee, Yongwen Xu, Robert Brewer, Philip Johnson','manual'),
]

# The name of an image file (relative to this directory) to place at the top of
# the title page.
#latex_logo = None

# For ""manual"" documents, if"
4665874,-8268719407908006324,12,BigQuery,"col_0', 'col_1', 'col_4') VALUES ('row_186', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_185', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_"
211618,-1517768099960784945,30,BigQuery,"flow_level -= 1
            self.write_indicator(u'}', False)
            # if self.event.comment and self.event.comment[0]:
            #     # eol comment on flow sequence
            #     self.write_post_comment(self.event)
            self.state = self.states.pop()
        else:
            if self.canonical or self.column > self.best_width:
                self.write_indent()
            if not self.canonical and self.check_simple_key():
                self.states.append(self.expect_flow_mapping_simple_value)
                self"
5514998,8304567088232843162,20,BigQuery,"where(True, b, b), r)

    def test_where_with_masked_choice(self):
        x = arange(10)
        x[3] = masked
        c = x >= 8
        # Set False to masked
        z = where(c, x, masked)
        assert_(z.dtype is x.dtype)
        assert_(z[3] is masked)
        assert_(z[4] is masked)
        assert_(z[7] is masked)
        assert_(z[8] is not masked)
        assert_(z[9] is not masked)
        assert_equal(x, z)"
2832140,-4281906286061380661,12,BigQuery,"1, 3, ""aps.12.01""],
    [aps12_f, aps12_fp, aps12_fpp, (4,), [1, 100], np.inf, 1.1, 4, ""aps.12.02""],
    [aps12_f, aps12_fp, aps12_fpp, (5,), [1, 100], np.inf, 1.1, 5, ""aps.12.03""],
    [aps12_f, aps12_fp, aps12_fpp, (6,), [1, 100], np.inf, 1.1, 6, ""aps.12."
378761,-8251272072823671697,75,BigQuery,u49e5\u49e6\u49e7\u49e8\u49e9\u49ea\u49eb\u49ec\u49ed\u49ee\u49ef\u49f0\u49f1\u49f2\u49f3\u49f4\u49f5\u49f6\u49f7\u49f8\u49f9\u49fa\u49fb\u49fc\u49fd\u49fe\u49ff\u4a00\u4a01\u4a02\u4a03\u4a04\u4
4620588,8360392455167261668,14,BigQuery,"-remote-station-manager.h (module 'wifi'): void ns3::WifiRemoteStationManager::PrepareForQueue(ns3::Mac48Address address, ns3::WifiMacHeader const * header, ns3::Ptr<ns3::Packet const> packet, uint32_t fullPacketSize) [member function]
    cls.add_method('PrepareForQueue', 
                   'void', 
                   [param('ns3::Mac48Address', 'address'), param('ns3::WifiMacHeader const *', 'header'), param('ns3::Ptr< ns3::Packet const >', 'packet'), param('uint32"
2225103,4640475727605866327,6,BigQuery,", y_n_nrm = norm(x_n), norm(y_n)

        m_p, m_n = x_p_nrm * y_p_nrm, x_n_nrm * y_n_nrm

        # choose update
        if m_p > m_n:
            u = x_p / x_p_nrm
            v = y_p / y_p_nrm
            sigma = m_p
        else:
            u = x_n / x_n_nrm
            v = y_n / y_n_nrm
            sigma = m_n

"
5242540,-6238595021438145315,22,BigQuery,"imator, ""accuracy"")
    assert_almost_equal(scorer(estimator, [[1]], [1]), 1.0)

    estimator = EstimatorWithFit()
    scorer = check_scoring(estimator, ""accuracy"")
    assert_true(isinstance(scorer, _PredictScorer))

    estimator = EstimatorWithFit()
    scorer = check_scoring(estimator, allow_none=True)
    assert_true(scorer is None)


def test_check_scoring_gridsearchcv():
    # test that check_scoring works on GridSearchCV and pipeline"
5544771,3927622859076354649,9,BigQuery,"return False
    def export(self, outfile, level, namespace_='', name_='typicalAgeRange', namespacedef_='', pretty_print=True):
        if pretty_print:
            eol_ = '\n'
        else:
            eol_ = ''
        showIndent(outfile, level, pretty_print)
        outfile.write('<%s%s%s' % (namespace_, name_, namespacedef_ and'' + namespacedef_ or '', ))
        already_processed = set()
        self.exportAttributes(outfile, level, already_processed, namespace_"
6606202,3720222257229284606,8,BigQuery,"__class__) and self.__dict__ == other.__dict__

  def __ne__(self, other):
    return not (self == other)

class abandonBlock_result:
  """"""
  Attributes:
   - ouch
  """"""

  thrift_spec = (
    None, # 0
    (1, TType.STRUCT, 'ouch', (ThriftIOException, ThriftIOException.thrift_spec), None, ), # 1
  )

  def __init__(self, ouch=None,):
    self.ouch = ouch

  def read(self, iprot):
    if iprot.__"
2863065,7257047465223354046,73,BigQuery,"UP_SEP
from django.db.models.query_utils import Q, refs_aggregate
from django.utils import six, timezone
from django.utils.functional import cached_property


class Combinable(object):
    """"""
    Provides the ability to combine one or two objects with
    some connector. For example F('foo') + F('bar').
    """"""

    # Arithmetic connectors
    ADD = '+'
    SUB = '-'
    MUL = '*'
    DIV = '/'
    POW = '^'
    # The following is a quoted % operator - it is quoted because it can"
4802511,2870689584782356646,8,BigQuery,"            raise ValueError(""X has different shape than during fitting.""
                             "" Expected %d, got %d.""
                             % (indices.shape[0] - 1, n_features))

        # We use only those categorical features of X that are known using fit.
        # i.e lesser than n_values_ using mask.
        # This means, if self.handle_unknown is ""ignore"", the row_indices and
        # col_indices corresponding to the unknown categorical feature are
        # ignored.
        mask = (X < self.n_values_).ravel()
        if np.any(~mask):
            if self.handle"
6151456,-7997717404352619153,9,BigQuery,"        c = count()
        self.assertEqual(list(islice(c, 1, 3, 50)), [1])
        self.assertEqual(next(c), 3)

    def test_takewhile(self):
        data = [1, 3, 5, 20, 2, 4, 6, 8]
        underten = lambda x: x<10
        self.assertEqual(list(takewhile(underten, data)), [1, 3, 5])
        self.assertEqual(list(takewhile(underten, [])), [])
        self.assertRaises(TypeError, takewhile)
        self.assert"
1786173,-8573420987726707717,14,BigQuery," const &', 'address')])
    ## socket.h (module 'network'): void ns3::Socket::SetAcceptCallback(ns3::Callback<bool, ns3::Ptr<ns3::Socket>, ns3::Address const&, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty> connectionRequest, ns3::Callback<void, ns3::Ptr<ns3::Socket>, ns3::Address const&, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty, ns3::empty>"
6773063,-2635163465687737254,24,BigQuery,"', 
                   [param('ns3::Address', 'address')], 
                   is_virtual=True)
    ## csma-net-device.h (module 'csma'): ns3::Address ns3::CsmaNetDevice::GetAddress() const [member function]
    cls.add_method('GetAddress', 
                   'ns3::Address', 
                   [], 
                   is_const=True, is_virtual=True)
    ## csma-net-device.h (module 'csma'): bool ns3::CsmaNetDevice::IsLinkUp() const [member function]
    cls.add_method('Is"
4726612,-9119415191828158549,28,BigQuery,"to_long(maxSize):
  """"""Convert a string media size, such as 10GB or 3TB into an integer.

  Args:
    maxSize: string, size as a string, such as 2MB or 7GB.

  Returns:
    The size as an integer value.
  """"""
  if len(maxSize) < 2:
    return 0
  units = maxSize[-2:].upper()
  bit_shift = _MEDIA_SIZE_BIT_SHIFTS.get(units)
  if bit_shift is not None:
    return int(maxSize[:-2]) << bit_shift
  else:
    return"
378761,5646047961071504002,75,BigQuery,882\ue883\ue884\ue885\ue886\ue887\ue888\ue889\ue88a\ue88b\ue88c\ue88d\ue88e\ue88f\ue890\ue891\ue892\ue893\ue894\ue895\ue896\ue897\ue898\ue899\ue89a\ue89b\ue89c\ue89d\ue89e\ue89f\ue8a0\ue8a1\ue8a2\ue8a3\ue8a4\ue8a5\ue8a6\ue8a7\ue8
6923487,-9077666130259694503,7,BigQuery,"[""Content-Type""] = str(ct)

        return self.body

# GZIP


def compress(body, compress_level):
    """"""Compress 'body' at the given compress_level.""""""
    import zlib

    # See http://www.gzip.org/zlib/rfc-gzip.html
    yield ntob('\x1f\x8b')       # ID1 and ID2: gzip marker
    yield ntob('\x08')           # CM: compression method
    yield ntob('\x00')           # FLG: none set
    # MTIME: 4 bytes"
1468552,-3504208199617231679,71,BigQuery,"'(\d+\.\d*|\.\d+|\d+)[eE][+-]?\d+[LlUu]*', Number.Float),
            (r'(\d+\.\d*|\.\d+|\d+[fF])[fF]?', Number.Float),
            (r'0x[0-9a-fA-F]+[LlUu]*', Number.Hex),
            (r'0[0-7]+[LlUu]*', Number.Oct),
            (r'\d+[LlUu]*', Number.Integer),
"
1437583,-8992090152383008373,8,BigQuery,"('B.__le__')
        return NotImplemented
    def __ge__(self, other):
        self.log_operation('B.__ge__')
        return NotImplemented

class C(B):
    def __eq__(self, other):
        self.log_operation('C.__eq__')
        return NotImplemented
    def __le__(self, other):
        self.log_operation('C.__le__')
        return NotImplemented
    def __ge__(self, other):
        self.log_operation('C.__ge__')
        return NotImplemented
"
5878534,855133219411759166,28,BigQuery,".failures, runner.tries

def testfile(filename, module_relative=True, name=None, package=None,
             globs=None, verbose=None, report=True, optionflags=0,
             extraglobs=None, raise_on_error=False, parser=DocTestParser()):
    """"""
    Test examples in the given file.  Return (#failures, #tests).

    Optional keyword arg ""module_relative"" specifies how filenames
    should be interpreted:

      - If ""module_relative"" is True (the default), then ""filename""
         specifies a module-relative path.  By default,"
378761,6910201709077986481,75,BigQuery,7493\u7494\u7495\u7496\u7497\u7498\u7499\u749a\u749b\u749c\u749d\u749e\u749f\u74a0\u74a1\u74a2\u74a3\u74a4\u74a5\u74a6\u74a7\u74a8\u74a9\u74aa\u74ab\u74ac\u74ad\u74ae\u74af\u74b0\u74b1\u74b2\u74b3
6652445,-6174505503487744051,55,BigQuery," 'access' or mode == 'trunk':
            mode = 'layer2'
    elif intf_type == 'loopback' or intf_type =='svi':
        mode = 'layer3'
    return mode


def get_interface_type(interface):
    if interface.upper().startswith('ET'):
        return 'ethernet'
    elif interface.upper().startswith('VL'):
        return'svi'
    elif interface.upper().startswith('LO'):
        return 'loopback'
    elif interface.upper().startswith('MG'):
        return'management"
3741729,-6298221451589837063,14,BigQuery," constructing pairs"", node.start_mark,
                    ""expected a single mapping item, but found %d items"" %
                    len(subnode.value),
                    subnode.start_mark)
            key_node, value_node = subnode.value[0]
            key = self.construct_object(key_node)
            value = self.construct_object(value_node)
            pairs.append((key, value))

    def construct_yaml_set(self, node):
        data = set()
        yield data
        value = self.construct_mapping(node)
        data.update(value)

    def construct_"
4545142,-7189506280936401308,125,BigQuery,"Nonce = struct.unpack(""<I"", f.read(4))[0]
        self.sha256 = None
        self.hash = None

    def serialize(self):
        r = """"
        r += struct.pack(""<i"", self.nVersion)
        r += ser_uint256(self.hashPrevBlock)
        r += ser_uint256(self.hashMerkleRoot)
        r += struct.pack(""<I"", self.nTime)
        r += struct.pack(""<I"", self.nBits)
        r += struct.pack(""<I"", self.nNonce)
        return r"
6576574,-4905709556607589873,43,BigQuery,"vars and cv not in os.environ:
                    flags = _config_vars[cv]
                    flags = re.sub(r'-isysroot\s+\S+(?:\s|$)','', flags)
                    _save_modified_value(_config_vars, cv, flags)

    return _config_vars


def compiler_fixup(compiler_so, cc_args):
    """"""
    This function will strip '-isysroot PATH' and '-arch ARCH' from the
    compile flags if the user has specified one them in extra_compile_flags.

    This is needed because '"
378761,6620771401760049654,75,BigQuery,d24\u4d25\u4d26\u4d27\u4d28\u4d29\u4d2a\u4d2b\u4d2c\u4d2d\u4d2e\u4d2f\u4d30\u4d31\u4d32\u4d33\u4d34\u4d35\u4d36\u4d37\u4d38\u4d39\u4d3a\u4d3b\u4d3c\u4d3d\u4d3e\u4d3f\
3863000,7635091677153853546,9,BigQuery,"    return os.path.join(*paths)

# convert_path ()


def change_root (new_root, pathname):
    """"""Return 'pathname' with 'new_root' prepended.  If 'pathname' is
    relative, this is equivalent to ""os.path.join(new_root,pathname)"".
    Otherwise, it requires making 'pathname' relative and then joining the
    two, which is tricky on DOS/Windows and Mac OS.
    """"""
    if os.name == 'posix':
        if not os.path.isabs(pathname):
            return os.path.join(new_root"
3741729,-948455591724332818,14,BigQuery,"(self, node):
        # Note: the same code as `construct_yaml_omap`.
        pairs = []
        yield pairs
        if not isinstance(node, SequenceNode):
            raise ConstructorError(
                ""while constructing pairs"", node.start_mark,
                ""expected a sequence, but found %s"" % node.id, node.start_mark)
        for subnode in node.value:
            if not isinstance(subnode, MappingNode):
                raise ConstructorError(
                    ""while constructing pairs"", node.start_mark,
                    ""expected a mapping of length 1, but found %s"" %
"
4303202,-5243772188149662318,15,BigQuery,"0)
                yield TokenInfo(STRING, contstr + line[:end],
                       strstart, (lnum, end), contline + line)
                contstr, needcont = '', 0
                contline = None
            elif needcont and line[-2:]!= '\\\n' and line[-3:]!= '\\\r\n':
                yield TokenInfo(ERRORTOKEN, contstr + line,
                           strstart, (lnum, len(line)), contline)
                contstr = ''
                contline = None
                continue
            else:
                contstr = contstr + line
                cont"
5137002,7017747106712361109,50,BigQuery,"
            IndexError,
            a.__setitem__,
            -len(self.example)-1, self.example[0]
        )

    def test_delitem(self):
        a = array.array(self.typecode, self.example)
        del a[0]
        self.assertEqual(
            a,
            array.array(self.typecode, self.example[1:])
        )

        a = array.array(self.typecode, self.example)
        del a[-1]
        self.assertEqual(
            a,
            array.array(self.typecode, self"
5832899,-9084847706070920706,19,BigQuery,"~pymongo.collection.Collection` with
        different codec options, read preference, and/or write concern from
        this :class:`Database`.

          >>> db.read_preference
          Primary()
          >>> coll1 = db.test
          >>> coll1.read_preference
          Primary()
          >>> from pymongo import ReadPreference
          >>> coll2 = db.get_collection(
         ...     'test', read_preference=ReadPreference.SECONDARY)
          >>> coll2.read_preference
          Secondary(tag_sets=None)

        :Parameters:
          - `name`: The name of the"
2027971,-3817852394203873229,7,BigQuery,"/',
)
class FormsMediaTestCase(SimpleTestCase):
    """"""Tests for the media handling on widgets and forms""""""

    def test_construction(self):
        # Check construction of media objects
        m = Media(
            css={'all': ('path/to/css1', '/path/to/css2')},
            js=('/path/to/js1', 'http://media.other.com/path/to/js2', 'https://secure.other.com/path/to/js3'),
        )
        self.assertEqual(
            str(m),
            """"""<link href=""http://media."
4378632,-7907745617836646440,13,BigQuery,"
# Copyright (C) 2007-2013 Volatility Foundation
# Copyright (C) 2005,2006,2007 4tphi Research
#
# Authors:
# {npetroni,awalters}@4tphi.net (Nick Petroni and AAron Walters)
#
# This file is part of Volatility.
#
# Volatility is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; either version 2 of the License, or
# (at your option) any later version.
#
# Volatility is distributed in the hope that it will be useful,"
6182424,-193364888144581558,54,BigQuery,")
    def __len__(self):
        return len(self.data)
    def __delitem__(self, index):
        del self.data[index]
    def __getitem__(self, index):
        if isinstance(index, slice):
            return SubPattern(self.pattern, self.data[index])
        return self.data[index]
    def __setitem__(self, index, code):
        self.data[index] = code
    def insert(self, index, code):
        self.data.insert(index, code)
    def"
4167012,5051169765622285195,10,BigQuery,"lazy('Username'))
>>> f = SomeForm()
>>> print f.as_p()
<p><label for=""id_username"">Username:</label> <input id=""id_username"" type=""text"" name=""username"" maxlength=""10"" /></p>

Translations are done at rendering time, so multi-lingual apps can define forms
early and still send back the right translation.

>>> activate('de')
>>> print f.as_p()
<p><label for=""id_username"">Benutzername:</label> <input id=""id_username"" type=""text"" name=""username"" maxlength=""10"" />"
2528285,6745330822302708101,7,BigQuery,"vel;"")
        self.assertEqual(2, cur1.fetchone()[0])

        cur2.execute(""select count(*) from isolevel;"")
        self.assertEqual(2, cur2.fetchone()[0])

    def test_isolation_level_closed(self):
        cnn = self.connect()
        cnn.close()
        self.assertRaises(psycopg2.InterfaceError, getattr,
            cnn, 'isolation_level')
        self.assertRaises(psycopg2.InterfaceError,
            cnn.set_isolation_level, 0)"
4468416,-5680465144743119801,6,BigQuery,"""], ""Error: Error loading w3_v18: Wallet requires newer version of Bitcoin Core"")
        if self.options.descriptors:
            # Descriptor wallets appear to be corrupted wallets to old software
            node_v17.assert_start_raises_init_error([""-wallet=w1""], ""Error: wallet.dat corrupt, salvage failed"")
            node_v17.assert_start_raises_init_error([""-wallet=w2""], ""Error: wallet.dat corrupt, salvage failed"")
            node_v17.assert_start_raises_init_error([""-wallet=w3""], ""Error: wallet.dat corrupt"
3711264,-3792414327592751696,14,BigQuery,"

    def create_mode(
            self,
            mode_slug,
            mode_name,
            min_price=0,
            suggested_prices='',
            currency='usd',
            expiration_datetime=None,
    ):
        """"""
        Create a new course mode
        """"""
        return CourseMode.objects.get_or_create(
            course_id=self.course_key,
            mode_display_name=mode_name,
            mode_slug=mode_slug,
            min_price=min_price,
            suggested_prices=suggested_prices,
            "
6773063,5758922267234506551,24,BigQuery,"ref_method='Ref', decref_method='Unref', peekref_method='GetReferenceCount'))
    ## simple-ref-count.h (module 'core'): ns3::SimpleRefCount<ns3::AttributeValue, ns3::empty, ns3::DefaultDeleter<ns3::AttributeValue> > [class]
    module.add_class('SimpleRefCount', automatic_type_narrowing=True, import_from_module='ns.core', template_parameters=['ns3::AttributeValue', 'ns3::empty', 'ns3::DefaultDeleter<ns3::AttributeValue>'], parent=root_module['ns"
5575932,9068123861656703348,63,BigQuery," f.srid):
                # Adding Transform() to the SQL placeholder.
                return '%s(%s(%%s,%s), %s)' % (self.transform, self.from_text, value.srid, f.srid)
            else:
                return '%s(%%s,%s)' % (self.from_text, f.srid)

    def _get_spatialite_func(self, func):
        """"""
        Helper routine for calling SpatiaLite functions and returning
        their result.
        """"""
        cursor = self.connection._cursor()
        try:
            try:
                "
6773063,8693521574218899484,24,BigQuery,"'): ns3::NodeContainer::NodeContainer(ns3::NodeContainer const & arg0) [copy constructor]
    cls.add_constructor([param('ns3::NodeContainer const &', 'arg0')])
    ## node-container.h (module 'network'): ns3::NodeContainer::NodeContainer() [constructor]
    cls.add_constructor([])
    ## node-container.h (module 'network'): ns3::NodeContainer::NodeContainer(ns3::Ptr<ns3::Node> node) [constructor]
    cls.add_constructor([param('ns3::Ptr< ns3::Node >',"
3513970,8052816161721116866,10,BigQuery,"ENT""
        if 'cache-control' not in request_headers:
            request_headers['cache-control'] = 'no-cache'
    elif 'no-cache' in cc:
        retval = ""TRANSPARENT""
    elif 'no-cache' in cc_response:
        retval = ""STALE""
    elif 'only-if-cached' in cc:
        retval = ""FRESH""
    elif 'date' in response_headers:
        date = calendar.timegm(email.utils.parsedate_tz(response_headers['date']))
        now = time.time()"
1179863,2441444663105416921,7,BigQuery," _size7) = iprot.readSetBegin()
          for _i11 in xrange(_size7):
            _elem12 = iprot.readString()
            self.groups.add(_elem12)
          iprot.readSetEnd()
        else:
          iprot.skip(ftype)
      else:
        iprot.skip(ftype)
      iprot.readFieldEnd()
    iprot.readStructEnd()

  def write(self, oprot):
    if oprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and self.thrift_spec is not None and fastbinary"
7394913,2764280632704768225,16,BigQuery," configuration"", exception=traceback.format_exc(), **camel_dict_to_snake_dict(e.response))


def main():
    argument_spec = ec2_argument_spec()
    argument_spec.update(
        dict(
            name=dict(required=True),
            image_id=dict(),
            instance_id=dict(),
            key_name=dict(),
            security_groups=dict(default=[], type='list'),
            user_data=dict(),
            user_data_path=dict(type='path'),
            kernel_id=dict(),
            volumes=dict(type='list'),
            "
5378364,-65574407509382570,7,BigQuery,"
        ::

          In [1]: %time 2**128
          CPU times: user 0.00 s, sys: 0.00 s, total: 0.00 s
          Wall time: 0.00
          Out[1]: 340282366920938463463374607431768211456L

          In [2]: n = 1000000

          In [3]: %time sum(range(n))
          CPU times: user 1.20 s, sys: 0.05 s, total: 1.25 s
          Wall time: 1.37
          Out[3]: 499999500000L

          In [4]: %time print 'hello world'
"
1786173,-8947904262184232173,14,BigQuery,"_impl', 'ld_impl'], outer_class=root_module['ns3::int64x64_t'], import_from_module='ns.core')
    ## chunk.h (module 'network'): ns3::Chunk [class]
    module.add_class('Chunk', parent=root_module['ns3::ObjectBase'])
    ## packet-socket.h (module 'network'): ns3::DeviceNameTag [class]
    module.add_class('DeviceNameTag', parent=root_module['ns3::Tag'])
    ## flow-id-tag.h (module 'network'): ns3::FlowId"
6151456,5696069696674345022,9,BigQuery," sum(imap(operator.mul, vec1, vec2))

>>> def flatten(listOfLists):
...     return list(chain.from_iterable(listOfLists))

>>> def repeatfunc(func, times=None, *args):
...     ""Repeat calls to func with specified arguments.""
...     ""   Example:  repeatfunc(random.random)""
...     if times is None:
...         return starmap(func, repeat(args))
...     else:
...         return starmap(func, repeat(args, times))

>>> def pairwise(iterable):
...     ""s -> (s"
4741364,-7922778473772802533,8,BigQuery,"DELETE', mailbox)

    def deleteacl(self, mailbox, who):
        """"""Delete the ACLs (remove any rights) set for who on mailbox.

        (typ, [data]) = <instance>.deleteacl(mailbox, who)
        """"""
        return self._simple_command('DELETEACL', mailbox, who)

    def enable(self, capability):
        """"""Send an RFC5161 enable string to the server.

        (typ, [data]) = <intance>.enable(capability)
        """"""
        if 'ENABLE' not in self.capabilities:
            raise IMAP4.error(""Server does not support E"
5106828,-1458472760060722260,249,BigQuery,"'jyaenj',    # 0xe9
'jyaenh',    # 0xea
'jyaed',    # 0xeb
'jyael',    # 0xec
'jyaelg',    # 0xed
'jyaelm',    # 0xee
'jyaelb',    # 0xef
'jyaels',    # 0xf0
'jyaelt',    # 0xf1
'jyaelp',    # 0xf2
'jyaelh',    # 0xf3
'jyaem',    # 0xf4
'jyaeb',    # 0xf5
'"
2271380,-6164232189276958676,9,BigQuery," a ``Distribution`` or
        ``None``.

        Unless `replace_conflicting=True`, raises a VersionConflict exception
        if
        any requirements are found on the path that have the correct name but
        the wrong version.  Otherwise, if an `installer` is supplied it will be
        invoked to obtain the correct version of the requirement and activate
        it.

        `extras` is a list of the extras to be used with these requirements.
        This is important because extra requirements may look like `my_req;
        extra = ""my_extra""`, which would otherwise be interpreted as a purely
        optional requirement.  Instead, we want to"
863442,4559889631104343124,88,BigQuery," Values)

        Process command-line arguments and populate 'values', consuming
        options and arguments from 'rargs'.  If 'allow_interspersed_args' is
        false, stop at the first non-option argument.  If true, accumulate any
        interspersed non-option arguments in 'largs'.
        """"""
        while rargs:
            arg = rargs[0]
            # We handle bare ""--"" explicitly, and bare ""-"" is handled by the
            # standard arg handler since the short arg case ensures that the
            # len of the opt string is greater than 1.
            if arg == ""--"":
                del rargs[0]"
3378286,-7320077626715034138,71,BigQuery,"_from_insert:
            return self.connection.ops.fetch_returned_insert_id(cursor)
        return self.connection.ops.last_insert_id(cursor,
                self.query.model._meta.db_table, self.query.model._meta.pk.column)


class SQLDeleteCompiler(SQLCompiler):
    def as_sql(self):
        """"""
        Creates the SQL for this query. Returns the SQL string and list of
        parameters.
        """"""
        assert len(self.query.tables) == 1, \
                ""Can only delete from one table at a"
3666017,2165563979111335977,25,BigQuery," = 59
KEY_F2 = 60
KEY_F3 = 61
KEY_F4 = 62
KEY_F5 = 63
KEY_F6 = 64
KEY_F7 = 65
KEY_F8 = 66
KEY_F9 = 67
KEY_F10 = 68
KEY_NUMLOCK = 69
KEY_SCROLLLOCK = 70
KEY_KP7 = 71
KEY_KP8 = 72
KEY_KP9 = 73
KEY_KPMINUS = 74
KEY_KP4 = 75
KEY_KP5 = 76
KEY_KP6 = 77
KEY_KPPLUS = 78
"
7137039,1329014912170369137,14,BigQuery,"PUSHBULLET_NOTIFY_ONSNATCH:
            self._sendPushbullet(pushbullet_api=None, event=common.notifyStrings[common.NOTIFY_SNATCH], message=ep_name, notificationType=""note"", method=""POST"")

    def notify_download(self, ep_name):
        if sickbeard.PUSHBULLET_NOTIFY_ONDOWNLOAD:
            self._sendPushbullet(pushbullet_api=None, event=common.notifyStrings[common.NOTIFY_DOWNLOAD], message=ep_name, notificationType=""note"", method=""POST"")

    def"
6652181,-1265915645733990305,18,BigQuery,"HTTPException):
    pass

class IncompleteRead(HTTPException):
    def __init__(self, partial, expected=None):
        self.args = partial,
        self.partial = partial
        self.expected = expected
    def __repr__(self):
        if self.expected is not None:
            e = ', %i more expected' % self.expected
        else:
            e = ''
        return 'IncompleteRead(%i bytes read%s)' % (len(self.partial), e)
    def __str__(self):
        return repr(self)

class ImproperConnectionState(HTTPException):
    "
1316668,-6041684729140339336,12,BigQuery,"': '(algo como ""it-it"")',
'<-prev ': '<-prev ',
'A new version of web2py is available': 'Hay una nueva versión de web2py disponible',
'A new version of web2py is available: %s': 'Hay una nueva versión de web2py disponible: %s',
'ATTENTION: Login requires a secure (HTTPS) connection or running on localhost.': 'ATENCION: Inicio de sesión requiere una conexión segura (HTTPS) o localhost.',
'ATTENTION: TESTING IS"
378761,3271666875727589934,75,BigQuery,u2d3e\u2d3f\u2d40\u2d41\u2d42\u2d43\u2d44\u2d45\u2d46\u2d47\u2d48\u2d49\u2d4a\u2d4b\u2d4c\u2d4d\u2d4e\u2d4f\u2d50\u2d51\u2d52\u2d53\u2d54\u2d55\u2d56\u2d57\u2d58\u2d59\u2
2271380,1278456710612238639,9,BigQuery,"_finders[importer_type] = distribution_finder


def find_distributions(path_item, only=False):
    """"""Yield distributions accessible via `path_item`""""""
    importer = get_importer(path_item)
    finder = _find_adapter(_distribution_finders, importer)
    return finder(importer, path_item, only)


def find_eggs_in_zip(importer, path_item, only=False):
    """"""
    Find eggs in zip files; possibly multiple nested eggs.
    """"""
    if importer.archive.endswith('.whl'"
1786173,6208739926057245686,14,BigQuery,"void', 
                   [param('std::string', 'key'), param('std::string', 'variable'), param('int', 'val')], 
                   is_pure_virtual=True, is_virtual=True)
    ## data-output-interface.h (module'stats'): void ns3::DataOutputCallback::OutputSingleton(std::string key, std::string variable, uint32_t val) [member function]
    cls.add_method('OutputSingleton', 
                   'void', 
                   [param('std::string', 'key'), param('std::string', 'variable'), param('uint32_t', 'val')], "
2634875,8023340054092208677,11,BigQuery,".test_pdb_skip_modules_with_callback[0]>(3)callback()->None
    -> return None
    (Pdb) step
    --Return--
    > <doctest test.test_pdb.test_pdb_skip_modules_with_callback[0]>(5)skip_module()->None
    -> mod.foo_pony(callback)
    (Pdb) step
    > <doctest test.test_pdb.test_pdb_skip_modules_with_callback[1]>(10)<module>()
    -> pass  # provides something to ""step"" to
    (P"
7515335,7740092753541118369,8,BigQuery,"self.assertEqual(learn.get_next_by_published().title, ""Dive into Python"")
        self.assertEqual(dive.get_previous_by_published().title, ""Learning Python"")

    def test_raw(self):
        ""test the raw() method across databases""
        dive = Book.objects.using('other').create(title=""Dive into Python"", published=datetime.date(2009, 5, 4))
        val = Book.objects.db_manager(""other"").raw('SELECT id FROM multiple_database_book')
        self.assertQuerysetEqual(val, [dive.pk"
6621537,8256463985195576266,8,BigQuery,"Xiv preprint arXiv:1409.1556 (2014).
""""""
import mxnet as mx

def get_symbol(num_classes, **kwargs):
    ## define alexnet
    data = mx.symbol.Variable(name=""data"")
    # group 1
    conv1_1 = mx.symbol.Convolution(data=data, kernel=(3, 3), pad=(1, 1), num_filter=64, name=""conv1_1"")
    relu1_1 = mx.symbol.Activation(data=conv1_1, act_type=""relu"", name=""re"
2680064,-6208559487886743540,8,BigQuery,"_timer': '165',
            'user_server_cert': 'test_value_166',
            'vdom_admin': 'enable',
            'vip_arp_range': 'unlimited',
            'virtual_server_count': '169',
            'virtual_server_hardware_acceleration': 'disable',
            'wad_affinity': 'test_value_171',
            'wad_csvc_cs_count': '172',
            'wad_csvc_db_count': '173',
            'wad_source_affinity': 'disable',
            'wad_worker_count': '175"
378761,2850883323687766023,75,BigQuery,\u28a5\u28a6\u28a7\u28a8\u28a9\u28aa\u28ab\u28ac\u28ad\u28ae\u28af\u28b0\u28b1\u28b2\u28b3\u28b4\u28b5\u28b6\u28b7\u28b8\u28b9\u28ba\u28bb\u28bc\u28bd\u28be\u28bf\u28c0\u28c1\u28c2\u28c3\u28c4\u
1149724,-1731151940228801545,8,BigQuery,"return _dec_from_triple(0, result.lstrip('0') or '0', 0)

    def logical_xor(self, other, context=None):
        """"""Applies an 'xor' operation between self and other's digits.""""""
        if context is None:
            context = getcontext()

        other = _convert_other(other, raiseit=True)

        if not self._islogical() or not other._islogical():
            return context._raise_error(InvalidOperation)

        # fill to context.prec
        (opa, opb) = self._fill_logical(context, self._int, other"
1786173,6410287419070816738,14,BigQuery,"'): uint32_t ns3::RadiotapHeader::GetAmpduStatusRef() const [member function]
    cls.add_method('GetAmpduStatusRef', 
                   'uint32_t', 
                   [], 
                   is_const=True)
    ## radiotap-header.h (module 'network'): uint8_t ns3::RadiotapHeader::GetAntennaNoisePower() const [member function]
    cls.add_method('GetAntennaNoisePower', 
                   'uint8_t', 
                   [], 
                   is_const=True)
    ## radiotap"
847026,3308478974919643659,6,BigQuery,"f, (unicode_type, bytes_type)):
                result.append(f)
            else:
                result.extend(f)
        return result

    def embedded_css(self):
        return ""\n"".join(self._get_resources(""embedded_css""))

    def css_files(self):
        result = []
        for f in self._get_resources(""css_files""):
            if isinstance(f, (unicode_type, bytes_type)):
                result.append(f)
            else:
                result.extend(f)
        return result

    def html_head(self):"
4091338,967554046038799317,23,BigQuery,"
        CMAKE_PREFIX_PATH = '/opt/ros/indigo'.split(';')
        # prepend current workspace if not already part of CPP
        base_path = os.path.dirname(__file__)
        if base_path not in CMAKE_PREFIX_PATH:
            CMAKE_PREFIX_PATH.insert(0, base_path)
        CMAKE_PREFIX_PATH = os.pathsep.join(CMAKE_PREFIX_PATH)

        environ = dict(os.environ)
        lines = []
        if not args.extend:
"
1437522,1522368979800199057,7,BigQuery,"# - in OleFileIO._open and _OleStream, use size=None instead of 0x7FFFFFFF for
#   streams with unknown size
# - use arrays of int instead of long integers for FAT/MiniFAT, to improve
#   performance and reduce memory usage? (possible issue with values >2^31)
# - provide tests with unittest (may need write support to create samples)
# - move all debug code (and maybe dump methods) to a separate module, with
#   a class which inherits OleFileIO?
# - fix docstrings to follow epydoc format
# - add support for 4K sectors?
# - add"
1786173,4616575285997508955,14,BigQuery,"network'): ns3::Inet6SocketAddress::Inet6SocketAddress(ns3::Ipv6Address ipv6) [constructor]
    cls.add_constructor([param('ns3::Ipv6Address', 'ipv6')])
    ## inet6-socket-address.h (module 'network'): ns3::Inet6SocketAddress::Inet6SocketAddress(uint16_t port) [constructor]
    cls.add_constructor([param('uint16_t', 'port')])
    ## inet6-socket-address.h (module 'network'): ns3::Inet6Socket"
3211865,-5329768038995977338,43,BigQuery,"            return d

        else:
            raise TypeError(""Unsupported AOT type: %s"" % t)

        del self.stack[-1]


    def unjelly(self, ao):
        try:
            l = [None]
            self.unjellyInto(l, 0, ao)
            for func, v in self.afterUnjelly:
                func(v[0])
            return l[0]
        except:
            log.msg(""Error jellying object! Stacktrace follows::"")
            log.msg(""\n"".join(map(repr, self.stack)))
            raise
"
378761,3163020160975890465,75,BigQuery,u1cf6\u1cf7\u1cf8\u1cf9\u1cfa\u1cfb\u1cfc\u1cfd\u1cfe\u1cff\u1dc4\u1dc5\u1dc6\u1dc7\u1dc8\u1dc9\u1dca\u1dcb\u1dcc\u1dcd\u1dce\u1dcf\u1dd0\u1dd1\u1dd2\u1dd3\u1dd4\u1dd5\u1dd6\u1dd7\
2302556,1272001470385470352,34,BigQuery,"
    sys.exit(1)

try:
    import shortuuid
except ImportError:
    print(""Error: missing test dependency:"")
    print(""  shortuuid library is needed to run test suite"")
    print(""  you can install it with 'pip install shortuuid'"")
    print(""  or use tox to automatically handle test dependencies"")
    sys.exit(1)

try:
    import dateutil
except ImportError:
    print(""Error: missing test dependency:"")
    print(""  dateutil library is needed to run test suite"")
    print(""  you can install it with 'pip install python-dateutil'"
4528648,472339450891001717,6,BigQuery,"zone.utc)
            value = value.date()
        elif isinstance(value, datetime.date):
            # Nothing to do, as dates don't have tz information
            pass
        else:
            # No explicit date / datetime value -- no checks necessary
            return []
        offset = datetime.timedelta(days=1)
        lower = (now - offset).date()
        upper = (now + offset).date()
        if lower <= value <= upper:
            return [
                checks.Warning(
                    'Fixed default value provided.',
                    hint='It seems you set a fixed date / time / datetime '"
1846550,5738264764467033254,19,BigQuery,"Vocabulary(self):
    n_classes = 2
    self._testPredictions(n_classes,
                          label_vocabulary=None,
                          label_output_fn=lambda x: ('%s' % x).encode())

  def testBinaryClassesWithLabelVocabulary(self):
    n_classes = 2
    self._testPredictions(
        n_classes,
        label_vocabulary=['class_vocab_{}'.format(i)
                          for i in range(n_classes)],
        label_output_fn=lambda x: ('class_vocab_%s' % x).encode())

"
4227028,-5408829829975269303,6,BigQuery,"
    """"""This function adds the path to the Phar Lap binaries, includes,
    and libraries, if they are not already there.""""""
    ph_path = getPharLapPath()

    try:
        env_dict = env['ENV']
    except KeyError:
        env_dict = {}
        env['ENV'] = env_dict
    addPathIfNotExists(env_dict, 'PATH',
                       os.path.join(ph_path, 'bin'))
    addPathIfNotExists(env_dict, 'INCLUDE',
                       os.path.join(ph_path, 'include'))
    "
1149724,6486719298235612464,8,BigQuery,"_nan == 2:
                return context._raise_error(InvalidOperation,'sNaN',
                                        other)
            if self_is_nan:
                return self._fix_nan(context)

            return other._fix_nan(context)
        return 0

    def _compare_check_nans(self, other, context):
        """"""Version of _check_nans used for the signaling comparisons
        compare_signal, __le__, __lt__, __ge__, __gt__.

        Signal InvalidOperation if either self or other is a (quiet
        or signaling) NaN.  Signaling NaNs take precedence over"
1694950,-4897379211399031847,8,BigQuery,",
    'SQLERROR': tokens.Keyword,
    'SQLEXCEPTION': tokens.Keyword,
    'SQLSTATE': tokens.Keyword,
    'SQLWARNING': tokens.Keyword,
    'STABLE': tokens.Keyword,
    'START': tokens.Keyword,
    'STATE': tokens.Keyword,
    'STATEMENT': tokens.Keyword,
    'STATIC': tokens.Keyword,
    'STATISTICS': tokens.Keyword,
    'STDIN': tokens.Keyword,
    'STDOUT': tokens.Keyword,
    'STORAGE': tokens.Keyword,
    "
787299,6523441857228338888,11,BigQuery,"
    self.has_blob_value_ = 1
    self.blob_value_ = x

  def clear_blob_value(self):
    if self.has_blob_value_:
      self.has_blob_value_ = 0
      self.blob_value_ = """"

  def has_blob_value(self): return self.has_blob_value_

  def entity_value(self):
    if self.entity_value_ is None:
      self.lazy_init_lock_.acquire()
      try:
        if self.entity_value_ is None: self.entity_"
408942,2763836816727879279,53,BigQuery,"(""select returnblob()"")
        val = cur.fetchone()[0]
        self.assertEqual(type(val), buffer)
        self.assertEqual(val, buffer(""blob""))

    def CheckFuncReturnLongLong(self):
        cur = self.con.cursor()
        cur.execute(""select returnlonglong()"")
        val = cur.fetchone()[0]
        self.assertEqual(val, 1<<31)

    def CheckFuncException(self):
        cur = self.con.cursor()
        try:
            cur.execute(""select raiseexception()"")
            "
5544771,4623346558019065234,9,BigQuery," name_='coverage')
    def exportChildren(self, outfile, level, namespace_='', name_='coverage', fromsubclass_=False, pretty_print=True):
        super(coverage, self).exportChildren(outfile, level, namespace_, name_, True, pretty_print=pretty_print)
    def exportLiteral(self, outfile, level, name_='coverage'):
        level += 1
        already_processed = set()
        self.exportLiteralAttributes(outfile, level, already_processed, name_)
        if self.hasContent_():
            self.exportL"
5650709,-2685035434172767347,8,BigQuery,"types


class ServerInterface (object):
    """"""
    This class defines an interface for controlling the behavior of Paramiko
    in server mode.

    Methods on this class are called from Paramiko's primary thread, so you
    shouldn't do too much work in them.  (Certainly nothing that blocks or
    sleeps.)
    """"""

    def check_channel_request(self, kind, chanid):
        """"""
        Determine if a channel request of a given type will be granted, and
        return ``OPEN_SUCCEEDED`` or an error code.  This method is
        called in server mode when the client requests a channel, after
"
7045948,4712281581315393392,36,BigQuery,"                return False
        return False

    def _is_section_header(self):
        section = self._line_iter.peek().lower()
        match = _google_section_regex.match(section)
        if match and section.strip(':') in self._sections:
            header_indent = self._get_indent(section)
            section_indent = self._get_current_indent(peek_ahead=1)
            return section_indent > header_indent
        elif self._directive_sections:
            if _directive_regex.match(section):
                for directive_section in self"
5544771,1668525314850027693,9,BigQuery," outfile, level, already_processed, namespace_='', name_='nameValue'):
        super(nameValue, self).exportAttributes(outfile, level, already_processed, namespace_, name_='nameValue')
        if self.uniqueElementName is not None and 'uniqueElementName' not in already_processed:
            already_processed.add('uniqueElementName')
            outfile.write(' uniqueElementName=%s' % (self.gds_format_string(quote_attrib(self.uniqueElementName).encode(ExternalEncoding), input_name='uniqueElementName'), ))
    def exportChildren(self,"
378761,-5762649085614350939,75,BigQuery,\u83ae\u83af\u83b0\u83b1\u83b2\u83b3\u83b4\u83b5\u83b6\u83b7\u83b8\u83b9\u83ba\u83bb\u83bc\u83bd\u83be\u83bf\u83c0\u83c1\u83c2\u83c3\u83c4\u83c5\u83c6\u83c7\u83c8\u83c9\u83ca\u83cb\u83cc\u83cd\u
5514998,-3104139666108230616,20,BigQuery,"1, count(1))
        assert_equal(0, array(1, mask=[1]))

        ott = array([0., 1., 2., 3.], mask=[1, 0, 0, 0])
        res = count(ott)
        self.assertTrue(res.dtype.type is np.intp)
        assert_equal(3, res)

        ott = ott.reshape((2, 2))
        res = count(ott)
        assert_(res.dtype.type is np.intp)
        assert_equal(3, res)
        res = count(ott, 0)
        assert_(isinstance("
378761,-1428134513547831284,75,BigQuery,\ua1cb\ua1cc\ua1cd\ua1ce\ua1cf\ua1d0\ua1d1\ua1d2\ua1d3\ua1d4\ua1d5\ua1d6\ua1d7\ua1d8\ua1d9\ua1da\ua1db\ua1dc\ua1dd\ua1de\ua1df\ua1e0\ua1e1\ua1e2\ua1e3\ua1e4\ua1e5\ua1e6\ua1e7\ua1e8\ua1e9\ua1ea\ua
2650071,4398478279373441414,15,BigQuery,"
            # specific list passed in
            objset = set()
            for o in objects:
                try:
                    state = attributes.instance_state(o)
                except exc.NO_STATE:
                    raise exc.UnmappedInstanceError(o)
                objset.add(state)
        else:
            objset = None

        # store objects whose fate has been decided
        processed = set()

        # put all saves/updates into the flush context.  detect top-level
        # orphans and throw them into deleted.
        if objset:
            proc = new.union(dirty).intersection(objset).difference("
3044959,1644235720485785938,45,BigQuery,"        enable_edit = True
        pathx = []
        for filename in args:
            pathx.append(os.path.dirname(filename))
        for dir in pathx:
            dir = os.path.abspath(dir)
            if not dir in sys.path:
                sys.path.insert(0, dir)
    else:
        dir = os.getcwd()
        if dir not in sys.path:
            sys.path.insert(0, dir)
    # check the IDLE settings configuration (but command line overrides)
    edit_start = idleConf.GetOption('main', 'General',
                               "
7076705,-8831145744205282045,56,BigQuery,"bsrv))
  dllcmd = FullLinkCommand(dllcmd, '$binary', 'dll')
  master_ninja.rule('solink' + rule_name_suffix,
                    description=dlldesc, command=dllcmd,
                    rspfile='$binary.rsp',
                    rspfile_content='$libs $in_newline $ldflags',
                    restat=True,
                    pool='link_pool')
  master_ninja.rule('solink_module' + rule_name_suffix,
                    description=dlldesc, command=dllcmd,
                    rspfile='$binary.rsp',
                    "
1679663,25193951762967393,8,BigQuery,".serialize(value)
            self.assertEqual(actual, expected)
            roundtrip = VarIntSerializer.deserialize(actual)
            self.assertEqual(value, roundtrip)
        T(0x0, b'00')
        T(0xfc, b'fc')
        T(0xfd, b'fdfd00')
        T(0xffff, b'fdffff')
        T(0x10000, b'fe00000100')
        T(0xffffffff, b'feffffffff')
        T(0x100000000, b'ff0000000001000000')
        T(0x"
4741382,-800202168191252222,8,BigQuery,"_inputs, and returns a pair
      consisting of outputs and states (as, e.g., basic_rnn_seq2seq).
    softmax_loss_function: Function (labels, logits) -> loss-batch
      to be used instead of the standard softmax (the default if this is None).
      **Note that to avoid confusion, it is required for the function to accept
      named arguments.**
    per_example_loss: Boolean. If set, the returned loss will be a batch-sized
      tensor of losses for each sequence in the batch. If unset, it will be
      a scalar with the averaged loss from all examples."
2665327,6575977807093613831,27,BigQuery,"Address>
                    </peerAF>
                  </peerAFs>
                </bgpVrfAF>
              </bgpVrfAFs>
            </bgpVrf>
          </bgpVrfs>
        </bgpcomm>
      </bgp>
    </config>
""""""


class BgpNeighborAf(object):
    """""" Manages BGP neighbor Address-family configuration """"""

    def netconf_get_config(self, **kwargs):
        """""" netconf_get_config """"""

        module = kwargs[""module""]
        conf_str = kwargs[""conf_str""]

        xml_str"
5499719,-2398634052482622439,14,BigQuery,"
#  you may not use this file except in compliance with the License.
#  You may obtain a copy of the License at
#
#       http://www.apache.org/licenses/LICENSE-2.0
#
#  Unless required by applicable law or agreed to in writing, software
#  distributed under the License is distributed on an ""AS IS"" BASIS,
#  WITHOUT WARRANTIES OR CONDITIONS "
2483113,-7434724214432875904,7,BigQuery,"66, 0x67, 0x68, 0x69, 0x6A, 0x73, 0x74, 0x75,
	0x76, 0x77, 0x78, 0x79, 0x7A, 0x82, 0x83, 0x84,
	0x85, 0x86, 0x87, 0x88, 0x89, 0x8A, 0x92, 0x93,
	0x94, 0x95, 0x96, 0x97, 0x98, 0x99, 0x9A, 0xA2,
	0xA3, 0xA4, 0"
5393603,2443562343090954672,9,BigQuery,".receive.drops',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_transmit_drops(self):
        self._test_pollster(
            port.PortPollsterTransmitDrops,
           'switch.port.transmit.drops',
            sample.TYPE_CUMULATIVE,
            'packet')

    def test_port_pollster_receive_errors(self):
        self._test_pollster(
            port.PortPollsterReceiveErrors,
           'switch.port.receive.errors',
            sample.TYPE_CUMULATIVE"
4257488,-7572916878908605138,9,BigQuery," to ::

        y[k] = np.sum(x * np.exp(2j * np.pi * k * np.arange(n)/n)) / len(x)

    As with `fft`, `ifft` has support for all floating point types and is
    optimized for real input.

    Examples
    --------
    >>> import scipy.fft
    >>> import numpy as np
    >>> scipy.fft.ifft([0, 4, 0, 0])
    array([ 1.+0.j,  0.+1.j, -1.+0.j,  0.-1.j])"
3136506,4817635750622818093,183,BigQuery,"class NullBooleanSelect(Select):
    """"""
    A Select Widget intended to be used with NullBooleanField.
    """"""
    def __init__(self, attrs=None):
        choices = (('1', ugettext_lazy('Unknown')),
                   ('2', ugettext_lazy('Yes')),
                   ('3', ugettext_lazy('No')))
        super(NullBooleanSelect, self).__init__(attrs, choices)

    def render(self, name, value, attrs=None, choices=()):
        try:
            value = {True: '2', False: '"
29732,8823869911431906178,9,BigQuery," where the disk resides.
      returned: success
      type: str
    diskEncryptionKey:
      description:
      - Encrypts the disk using a customer-supplied encryption key.
      - After you encrypt a disk with a customer-supplied key, you must provide the
        same key if you use the disk later (e.g. to create a disk snapshot or an image,
        or to attach the disk to a virtual machine).
      - Customer-supplied encryption keys do not protect access to metadata of the
        disk.
      - If you do not provide an encryption key when creating the disk, then the disk
        will be encrypted using an automatically generated key"
1664238,8572899314608190384,6,BigQuery,"                    self.file.output(Name(name), Op.use_xobject)
                    self.file.output(Op.grestore)

        # Draw any horizontal lines in the math layout
        for ox, oy, width, height in rects:
            self.file.output(Op.gsave, ox, oy, width, height,
                             Op.rectangle, Op.fill, Op.grestore)

        # Pop off the global transformation
        self.file.output(Op.grestore)

    def draw_tex(self, gc, x, y, s, prop, angle, ismath='TeX!', mtext=None):"
4363502,-1343230550150776702,15,BigQuery,".encode(""ascii"")
        except UnicodeError:
            return string
else:
    def _stringify(string):
        return string

__version__ = ""1.0.1""

# xmlrpc integer limits
MAXINT =  2L**31-1
MININT = -2L**31

# --------------------------------------------------------------------
# Error constants (from Dan Libby's specification at
# http://xmlrpc-epi.sourceforge.net/specs/rfc.fault_codes.php)

# Ranges of errors
PARSE_ERROR       = -32700
SERVER_ERROR      = -32600
AP"
4939582,-7051960639009429691,28,BigQuery,"ExosCommandModule, self).tearDown()
        self.mock_run_commands.stop()

    def load_fixtures(self, commands=None):

        def load_from_file(*args, **kwargs):
            module, commands = args
            output = list()

            for item in commands:
                try:
                    obj = json.loads(item['command'])
                    command = obj['command']
                except ValueError:
                    command = item['command']
                filename = str(command).replace(' ', '_')
                output.append(load_fixture(filename))
            return output

        self.run_"
7076043,7779457127567049990,9,BigQuery,"DM_PROMPT = 4
DM_MODIFY = 8
DM_IN_BUFFER = DM_MODIFY
DM_IN_PROMPT = DM_PROMPT
DM_OUT_BUFFER = DM_COPY
DM_OUT_DEFAULT = DM_UPDATE

# DISPLAY_DEVICE.StateFlags
DISPLAY_DEVICE_ATTACHED_TO_DESKTOP = 1
DISPLAY_DEVICE_MULTI_DRIVER = 2
DISPLAY_DEVICE_PRIMARY_DEVICE = 4
DISPLAY_DEVICE_MIRRORING_DRIVER"
621293,6371208376118301743,19,BigQuery," address from the left-hand operand. The right-hand operand's address is ignored.
	To construct an 'OSC-bundle' from multiple OSCMessage, see OSCBundle!
	
	Additional methods exist for retreiving typetags or manipulating items as (typetag, value) tuples.
	""""""
	def __init__(self, address=""""):
		""""""Instantiate a new OSCMessage.
		The OSC-address can be specified with the 'address' argument
		""""""
		self.clear(address)

	def setAddress(self, address):
		""""""Set or change the OSC-address
		""""""
"
3695899,-7351062970186923879,11,BigQuery,"xml:
            module.fail_json(msg='Error: Create bgp confed peer as failed.')

        cmds = []
        cmd = ""confederation peer-as %s"" % confed_peer_as_num
        cmds.append(cmd)

        return cmds

    def delete_bgp_confed_peer_as(self, **kwargs):
        """""" delete_bgp_confed_peer_as """"""

        module = kwargs[""module""]
        confed_peer_as_num = module.params['confed_peer_as_num']

        conf_str = CE_DELETE_BGP"
6106070,-7898378944417073283,8,BigQuery,"_PREFIX"": cfg.parentdir_prefix,
                        ""VERSIONFILE_SOURCE"": cfg.versionfile_source,
                        })

    ipy = os.path.join(os.path.dirname(cfg.versionfile_source),
                       ""__init__.py"")
    if os.path.exists(ipy):
        try:
            with open(ipy, ""r"") as f:
                old = f.read()
        except EnvironmentError:
            old = """"
        if INIT_PY_SNIPPET not in old:
            print("" appending to %s"" % ipy)
            with open"
7424899,568827964923314912,21,BigQuery,"enses/{2}/offerings/{3}/members/{4}'.format(
            self.client.provider['server'],
            self.client.provider['server_port'],
            self.want.key,
            self.want.offering_id,
            self.want.member_id,
        )
        while count < 3:
            resp = self.client.api.get(uri)
            try:
                response = resp.json()
            except ValueError as ex:
                raise F5ModuleError(str(ex))

            if 'code' in response and response['code'] == 400:
                "
1179888,5222053619747896405,7,BigQuery,"def value_to_string(self, obj):
        val = self._get_val_from_obj(obj)
        if val is None:
            data = ''
        else:
            data = val.strftime(""%H:%M:%S"")
        return data

    def formfield(self, **kwargs):
        defaults = {'form_class': forms.TimeField}
        defaults.update(kwargs)
        return super(TimeField, self).formfield(**defaults)

class URLField(CharField):
    description = _(""URL"")

    def __init__(self, verbose_name"
378761,6009918393874480968,75,BigQuery,\u41f7\u41f8\u41f9\u41fa\u41fb\u41fc\u41fd\u41fe\u41ff\u4200\u4201\u4202\u4203\u4204\u4205\u4206\u4207\u4208\u4209\u420a\u420b\u420c\u420d\u420e\u420f\u4210\u4211\u4212\u4213\u4214\u4215\u4216\u4217\u4218\u4219\u421a\u421
1862005,-1438732788670170672,24,BigQuery," objects.

        If fields are specified, they must be ForeignKey fields and only those
        related objects are included in the selection.

        If select_related(None) is called, the list is cleared.
        """"""

        if self._fields is not None:
            raise TypeError(""Cannot call select_related() after.values() or.values_list()"")

        obj = self._clone()
        if fields == (None,):
            obj.query.select_related = False
        elif fields:
            obj.query.add_select_related(fields)
        else:
            obj.query.select_related = True
        return obj

"
1801635,-8203749016284790628,91,BigQuery,"19'     #  0x19 -> END OF MEDIUM
    u'\x1a'     #  0x1A -> SUBSTITUTE
    u'\x1b'     #  0x1B -> ESCAPE
    u'\x1c'     #  0x1C -> FILE SEPARATOR
    u'\x1d'     #  0x1D -> GROUP SEPARATOR
    u'\x1e'     #  0x1E -> RECORD SEPARATOR
    u'\x1f'     #  0x1F -> UNIT SEPARATOR
    u'"
4528757,2213021238906748043,6,BigQuery,"TimeField', [], {'null': 'True', 'blank': 'True'}),
            u'id': ('django.db.models.fields.AutoField', [], {'primary_key': 'True'}),
           'max_basket_applications': ('django.db.models.fields.PositiveIntegerField', [], {'null': 'True', 'blank': 'True'}),
           'max_discount': ('django.db.models.fields.DecimalField', [], {'null': 'True','max_digits': '12', 'decimal_places': '2', 'blank': 'True"
4787232,-1833960551521735915,7,BigQuery," sequence of coefficients ordered from low to high.
    i.e., [1,2,3] is the series  ``P_0 + 2*P_1 + 3*P_2.``

    Parameters
    ----------
    c : array_like
        1-D array of Hermite series coefficients ordered from low to
        high.
    pow : integer
        Power to which the series will be raised
    maxpower : integer, optional
        Maximum power allowed. This is mainly to limit growth of the series
        to unmanageable size. Default is 16

    Returns
    -------
    coef : ndarray
        Hermite series of power.

    "
1830838,5226536686285442601,7,BigQuery,"REEN"": 375,
""KEY_PC"": 376,
""KEY_TV"": 377,
""KEY_TV2"": 378,
""KEY_VCR"": 379,
""KEY_VCR2"": 380,
""KEY_SAT"": 381,
""KEY_SAT2"": 382,
""KEY_CD"": 383,
""KEY_TAPE"": 384,
""KEY_RADIO"": 385,
""KEY_TUNER"": 386,
""KEY_PLAYER"": 387,
""KEY_TEXT"": 388,
""KEY_DVD"": 389,
""KEY_AUX"": 390,
""KEY_MP3"": 391"
7500034,1876650663812856505,7,BigQuery,"            pass
        self.check_track_dynamic(MyTuple, True)

    @support.cpython_only
    def test_bug7466(self):
        # Trying to untrack an unfinished tuple could crash Python
        self._not_tracked(tuple(gc.collect() for i in range(101)))

    def test_repr_large(self):
        # Check the repr of large list objects
        def check(n):
            l = (0,) * n
            s = repr(l)
            self.assertEqual(s,
                '(' + ', '.join(['0'] * n) + ')')"
5544950,3183250283426901585,12,BigQuery,"DER + ""/dd_factors"")
os.system('mkdir -p'+ INPUTFOLDER + ""/dd_factors"")
os.system('mv'+ INPUTFOLDER + '/dd_tmp/dd_factors*.bin'+ INPUTFOLDER + '/dd_factors')

os.system('rm -rf'+ INPUTFOLDER + ""/dd_variables"")
os.system('mkdir -p'+ INPUTFOLDER + ""/dd_variables"")
os.system('mv'+ INPUTFOLDER + '/dd_tmp/dd_variables*.bin'+ INPUTFOLDER"
3634768,-3467436519352498902,6,BigQuery,"                   'ns3::GlobalRoutingLSA::SPFStatus', 
                   [], 
                   is_const=True)
    ## global-router-interface.h: bool ns3::GlobalRoutingLSA::IsEmpty() const [member function]
    cls.add_method('IsEmpty', 
                   'bool', 
                   [], 
                   is_const=True)
    ## global-router-interface.h: void ns3::GlobalRoutingLSA::Print(std::ostream & os) const [member function]
    cls.add_method('Print', 
                   'void', 
                   [param('"
6242540,-5035773541416674532,22,BigQuery," % interpreter)
            env['VIRTUALENV_INTERPRETER_RUNNING'] = 'true'
            file = __file__
            if file.endswith('.pyc'):
                file = file[:-1]
            popen = subprocess.Popen([interpreter, file] + sys.argv[1:], env=env)
            raise SystemExit(popen.wait())

    if not args:
        print('You must provide a DEST_DIR')
        parser.print_help()
        sys.exit(2)
    if len(args) > 1:
        print('There must be only one"
5211786,-3632340274967412279,7,BigQuery,".datetime('Update Date'),
        'date_init': fields.datetime('Initialization Date')
    }

    def _module_data_uninstall(self, cr, uid, ids, context=None):
        """"""
        Delete PostgreSQL many2many relations tracked by this model.
        """""" 

        if uid!= SUPERUSER_ID and not self.pool['ir.model.access'].check_groups(cr, uid, ""base.group_system""):
            raise except_orm(_('Permission Denied'), (_('Administrator access is required to uninstall a module')))

        ids_set = set(ids)
        "
832293,-1182668383155381417,9,BigQuery,"
def _deepcopy_method(x, memo): # Copy instance methods
    return type(x)(x.__func__, deepcopy(x.__self__, memo))
_deepcopy_dispatch[types.MethodType] = _deepcopy_method

def _keep_alive(x, memo):
    """"""Keeps a reference to the object x in the memo.

    Because we remember objects by their id, we have
    to assure that possibly temporary objects are kept
    alive by referencing them.
    We store a reference at the id of the memo, which should
    normally not"
2907961,-8772786150550273288,11,BigQuery,"omplaintDelimeterTests(_HTTPParserTests, TestCase):
    """"""
    L{_HTTPParserTests} using bare LF newlines.
    """"""
    sep = b'\n'



class HTTPClientParserTests(TestCase):
    """"""
    Tests for L{HTTPClientParser} which is responsible for parsing HTTP
    response messages.
    """"""
    def test_parseVersion(self):
        """"""
        L{HTTPClientParser.parseVersion} parses a status line into its three
        components.
        """"""
        protocol = HTTPClientParser(None, None)
        self.assertEqual"
2862961,385734017444923630,38,BigQuery,"module 'core'): ns3::TypeId::SupportLevel [enumeration]
    module.add_enum('SupportLevel', ['SUPPORTED', 'DEPRECATED', 'OBSOLETE'], outer_class=root_module['ns3::TypeId'], import_from_module='ns.core')
    ## type-id.h (module 'core'): ns3::TypeId::AttributeInformation [struct]
    module.add_class('AttributeInformation', import_from_module='ns.core', outer_class=root_module['ns3::TypeId'])
    ## type-id.h (module 'core'): ns3::"
2241105,9216898566991969436,17,BigQuery,"snapshot_type=None, start_time=None,
                                   end_time=None, max_records=None,
                                   marker=None, owner_account=None):
        """"""
        Returns one or more snapshot objects, which contain metadata
        about your cluster snapshots. By default, this operation
        returns information about all snapshots of all clusters that
        are owned by you AWS customer account. No information is
        returned for snapshots owned by inactive AWS customer
        accounts.

        :type cluster_identifier: string
        :param cluster_identifier: The identifier of the cluster for which
            information about snapshots is requested.

        :type snapshot"
210997,3014422321061807580,8,BigQuery,"ess.run(train_op)
  ```

  Initialization: At creation time the hooked session does following things
  in given order:

  * calls `hook.begin()` for each given hook
  * finalizes the graph via `scaffold.finalize()`
  * create session
  * initializes the model via initialization ops provided by `Scaffold`
  * restores variables if a checkpoint exists
  * launches queue runners

  Run: When `run()` is called, the hooked session does following things:

  * calls `hook.before_run()`
  * calls TensorFlow `session.run()` with merged fetches and feed"
2938431,-5827191484515057006,10,BigQuery,"_list.append(str(listener['instance_protocol'].upper()))
        else:
            listener_list.append(str(listener['protocol'].upper()))

        if'ssl_certificate_id' in listener:
            listener_list.append(str(listener['ssl_certificate_id']))

        return tuple(listener_list)

    def _enable_zones(self, zones):
        try:
            self.elb.enable_zones(zones)
        except boto.exception.BotoServerError as e:
            self.module.fail_json(msg='"
3241950,-8439789629392809884,15,BigQuery,"seed)
        labels_a, labels_b = (random_state.randint(0, 10, i),
                              random_state.randint(0, 10, i))
        assert_almost_equal(v_measure_score(labels_a, labels_b),
                            2.0 * mutual_info_score(labels_a, labels_b) /
                            (entropy(labels_a) + entropy(labels_b)), 0)


def test_fowlkes_mallows_score():
    # General case
    score = fowlkes_mallows_score([0, 0, 0, 1, 1"
7561108,-4725218780308309775,11,BigQuery,"U234"", 9.0)

    # String indexing
    assert number.get_atom_density(""10000"", ""U238"") == 1.0
    assert number.get_atom_density(""10001"", ""U238"") == 2.0
    assert number.get_atom_density(""10002"", ""U238"") == 3.0
    assert number.get_atom_density(""10000"", ""U235"") == 4.0
    assert number.get_atom_density(""10001"", ""U235"") == 5.0
    assert number.get_atom_density(""10002"", ""U235"") == 6.0
    assert number.get_"
6515695,-669136410685247262,37,BigQuery,")
        self.install_egg_scripts(dist)
        self.installed_projects[dist.key] = dist
        log.info(self.installation_report(requirement, dist, *info))
        if (dist.has_metadata('dependency_links.txt') and
                not self.no_find_links):
            self.package_index.add_find_links(
                dist.get_metadata_lines('dependency_links.txt')
            )
        if not deps and not self.always_copy:
            return
        elif requirement is not None and dist.key!= requirement.key:
            log.warn"
181333,-2024070093828635084,15,BigQuery,"INTER(fuse_file_info))),
        ('statfs', CFUNCTYPE(c_int, c_char_p, POINTER(c_statvfs))),
        ('flush', CFUNCTYPE(c_int, c_char_p, POINTER(fuse_file_info))),
        ('release', CFUNCTYPE(c_int, c_char_p, POINTER(fuse_file_info))),
        ('fsync', CFUNCTYPE(c_int, c_char_p, c_int, POINTER(fuse_file_info))),
        ('setxattr',"
6652181,-718606545038517752,18,BigQuery,"

        self.msg = None

        # from the Status-Line of the response
        self.version = _UNKNOWN # HTTP-Version
        self.status = _UNKNOWN  # Status-Code
        self.reason = _UNKNOWN  # Reason-Phrase

        self.chunked = _UNKNOWN         # is ""chunked"" being used?
        self.chunk_left = _UNKNOWN      # bytes left to read in current chunk
        self.length = _UNKNOWN          # number of bytes left in response
        self.will_close = _UNKNOWN      # conn will close at end of response

    def _read_status(self):"
3483210,9118109077486847819,6,BigQuery," not.
        """"""

        try:
            self.hooks[event].remove(hook)
            return True
        except ValueError:
            return False


class Request(RequestHooksMixin):
    """"""A user-created :class:`Request <Request>` object.

    Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.

    :param method: HTTP method to use.
    :param url: URL to send.
    :param headers: dictionary of headers to send.
    :param files: dictionary of {filename: fileobject} files to multipart upload.
    :param data:"
4802511,7242238071537212290,8,BigQuery,"_)
        else:
            X = np.asarray(X)
            if copy:
                X = X.copy()
            if self.with_std:
                X *= self.scale_
            if self.with_mean:
                X += self.mean_
        return X


class MaxAbsScaler(BaseEstimator, TransformerMixin):
    """"""Scale each feature by its maximum absolute value.

    This estimator scales and translates each feature individually such
    that the maximal absolute value of each feature in the
    training set will be 1.0. It does not shift/center the data, and
    thus does not destroy any"
2877719,-6418873016884101377,11,BigQuery," type='json', auth=""public"", website=True)
    def post_upvote(self, forum, post, **kwargs):
        if not request.session.uid:
            return {'error': 'anonymous_user'}
        if request.uid == post.create_uid.id:
            return {'error': 'own_post'}
        upvote = True if not post.user_vote > 0 else False
        return request.registry['forum.post'].vote(request.cr, request.uid, [post.id], upvote=upvote, context=request.context)

    @http.route('/forum/<"
6773063,5488306792608410314,24,BigQuery,".h (module 'core'): double ns3::GammaRandomVariable::GetValue(double alpha, double beta) [member function]
    cls.add_method('GetValue', 
                   'double', 
                   [param('double', 'alpha'), param('double', 'beta')])
    ## random-variable-stream.h (module 'core'): uint32_t ns3::GammaRandomVariable::GetInteger(uint32_t alpha, uint32_t beta) [member function]
    cls.add_method('GetInteger', 
                   'uint32_t', 
                   [param('uint32_t', 'alpha'), param"
1876344,-2883612380534052300,6,BigQuery,"Protos.Networking.Requests.Messages.EquipBadgeMessage',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='badge_type', full_name='POGOProtos.Networking.Requests.Messages.EquipBadgeMessage.badge_type', index=0,
      number=1, type=14, cpp_type=8, label=1,
      has_default_value=False, default_value=0,
      message_type=None, enum_type=None,"
6591691,2887475349523005511,45,BigQuery,")!= 0)
        return NULL;
    else
        return res;
}

int PyAST_Check(PyObject* obj)
{
    init_types();
    return PyObject_IsInstance(obj, (PyObject*)&AST_type);
}
""""""

class ChainOfVisitors:
    def __init__(self, *visitors):
        self.visitors = visitors

    def visit(self, object):
        for v in self.visitors:
            v.visit(object)
            v.emit("""", 0)

common_msg = ""/* File automatically generated by %s. */\n\"
6500092,-8317824803508946094,12,BigQuery," erroneously reverse source and destination. For example,
  if you meant to synchronize a local directory from a bucket in the cloud but
  instead run the command:

    gsutil -m rsync -r -d./your-dir gs://your-bucket

  and your-dir is currently empty, you will quickly delete all of the objects in
  gs://your-bucket.

  You can also cause large amounts of data to be lost quickly by specifying a
  subdirectory of the destination as the source of an rsync. For example, the
  command:

    gsutil -m rsync -r -d gs://your-bucket/data gs://"
439465,325725358286659257,52,BigQuery,"        """"""
        Don't update anything.
        """"""
        return 0

    # EmptyQuerySet is always an empty result in where-clauses (and similar
    # situations).
    value_annotation = False


def get_cached_row(klass, row, index_start, using, max_depth=0, cur_depth=0,
                   requested=None, offset=0, only_load=None, local_only=False):
    """"""
    Helper function that recursively returns an object with the specified
    related attributes already populated.

    This method may be called recursively to populate deep select_related()
    cla"
4483382,6038990607407272399,7,BigQuery,".data = create_dummy_facility_data()
        self.data['interaction_log'] = ContentSessionLogFactory.create(user=self.data[""learners_one_group""][0][0])

    def test_facilityadmin_sessionlog_permissions(self):
        self.assertTrue(self.data['facility_admin'].can_create_instance(self.data['interaction_log']))
        self.assertTrue(self.data['facility_admin'].can_read(self.data['interaction_log']))
        self.assertTrue(self.data['facility_admin'].can_"
832555,457457658154724133,13,BigQuery,"shape == shape)
        assert_(a.arr_equal(a.arr, zeros(shape, dtype=self.type.dtype)))
        assert_(not a.arr.flags['FORTRAN'] and a.arr.flags['CONTIGUOUS'])

    def test_optional_from_2seq(self):
        obj = self.num2seq
        shape = (len(obj),)
        a = self.array(shape, intent.optional, obj)
        assert_(a.arr.shape == shape)
        assert_(not a.has_shared_memory())

    def test_optional_from_23seq(self):
"
3453907,513256249122226480,624,BigQuery,"
    fields = ('options',)


class ScopedEvalContextModifier(EvalContextModifier):
    """"""Modifies the eval context and reverts it later.  Works exactly like
    :class:`EvalContextModifier` but will only modify the
    :class:`~jinja2.nodes.EvalContext` for nodes in the :attr:`body`.
    """"""
    fields = ('body',)


# make sure nobody creates custom nodes
def _failing_new(*args, **kwargs):
    raise TypeError('can\'t create custom node types')
NodeType.__new__ = staticmethod(_failing_"
4590316,572777482734213489,13,BigQuery,"()
        result = import_wizard.parse_preview({
            'quoting': '""',
           'separator': ',',
        })
        self.assertFalse('error' in result)

    @mute_logger('odoo.addons.base_import.models.base_import')
    def test_csv_errors(self):
        import_wizard = self.make_import()

        result = import_wizard.parse_preview({
            'quoting': 'foo',
           'separator': ',',
        })
        self.assertTrue('error' in result)

        result = import_w"
6424473,-238809460588129307,18,BigQuery,"compile(regexp)
        except re.error as e:
            module.fail_json(msg=""Invalid Regexp (%s) in \""%s\"""" % (to_native(e), regexp))

    if validate and ""%s"" not in validate:
        module.fail_json(msg=""validate must contain %%s: %s"" % validate)

    path = assemble_from_fragments(src, delimiter, compiled_regexp, ignore_hidden, module.tmpdir)
    path_hash = module.sha1(path)
    result['checksum'] = path_hash

    # Backwards compat.  This"
7424899,-2643238278329552902,21,BigQuery,"
        if self._values['managed']:
            return None
        return self._values['device_address']

    @property
    def managed(self):
        return None


class ReportableChanges(Changes):
    pass


class Difference(object):
    def __init__(self, want, have=None):
        self.want = want
        self.have = have

    def compare(self, param):
        try:
            result = getattr(self, param)
            return result
        except AttributeError:
            return self.__default(param)

    def __default(self, param):
        attr1 = getattr"
1665264,722067654210113438,37,BigQuery,"                         '$(PROJECT_DERIVED_FILE_DIR)/$(CONFIGURATION)')
    xccl.SetBuildSetting(_shared_intermediate_var,
                         '$(SYMROOT)/DerivedSources/$(CONFIGURATION)')

    # Set user-specified project-wide build settings and config files.  This
    # is intended to be used very sparingly.  Really, almost everything should
    # go into target-specific build settings sections.  The project-wide
    # settings are only intended to be used in cases where Xcode attempts to
    # resolve variable references in a project context as opposed to a target
    # context"
7410121,-5711034175799494544,32,BigQuery,"ARTBEATPAYLOAD = _descriptor.Descriptor(
  name='HeartbeatPayload',
  full_name='ResponseEnvelop.HeartbeatPayload',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='cells', full_name='ResponseEnvelop.HeartbeatPayload.cells', index=0,
      number=1, type=11, cpp_type=10, label=3,
      has_default_value=False, default_value=[],
      message_type=None, enum_type"
3711130,7764074191301178177,11,BigQuery,",
    'NCHAR': sqltypes.NCHAR,
}


class SQLiteCompiler(compiler.SQLCompiler):
    extract_map = util.update_copy(
        compiler.SQLCompiler.extract_map,
        {
           'month': '%m',
            'day': '%d',
            'year': '%Y',
           'second': '%S',
            'hour': '%H',
            'doy': '%j',
           'minute': '%M',
            'epoch': '%s',
            'dow': '%w',
"
5832899,324674945950609095,19,BigQuery," exists."" % (
                            self.__name, self.__client.__class__.__name__))


class SystemJS(object):
    """"""Helper class for dealing with stored JavaScript.
    """"""

    def __init__(self, database):
        """"""Get a system js helper for the database `database`.

        An instance of :class:`SystemJS` can be created with an instance
        of :class:`Database` through :attr:`Database.system_js`,
        manual instantiation of this class should not be necessary.

        :class:`SystemJS` instances allow for easy manipulation and
        access to server-side JavaScript:

       .."
6591729,-1301357266186000000,58,BigQuery," list
    @type windows: list of (window number, string) tuples""""""

    __slots__ = ['algorithm', 'flags', 'iterations','salt', 'next', 'windows']

    def __init__(self, rdclass, rdtype, algorithm, flags, iterations, salt,
                 next, windows):
        super(NSEC3, self).__init__(rdclass, rdtype)
        self.algorithm = algorithm
        self.flags = flags
        self.iterations = iterations
        self.salt = salt
        self.next = next
        self.windows = windows

    def to_text("
378761,682602129618671029,75,BigQuery,u9d35\u9d36\u9d37\u9d38\u9d39\u9d3a\u9d3b\u9d3c\u9d3d\u9d3e\u9d3f\u9d40\u9d41\u9d42\u9d43\u9d44\u9d45\u9d46\u9d47\u9d48\u9d49\u9d4a\u9d4b\u9d4c\u9d4d\u9d4e\u9d4f\u9d
2255811,4321923194168588659,7,BigQuery,"(
      resource_loader.get_path_to_datafile(""_tpu_ops.so""))

  def _create_default_group_assignment():
    num_shards = tpu_function.get_tpu_context().number_of_shards
    if num_shards is None:
      logging.warning(
          ""cross_replica_sum should be used within a tpu_shard_context, but ""
          ""got unset number_of_shards. Assuming 1."")
      num_shards = 1
    group_assignment = [list(range(num_shards))]
    return group_"
4060493,3116208069808031742,9,BigQuery,"from tensorflow.python.ops import array_ops
from tensorflow.python.ops import math_ops
from tensorflow.python.ops import sparse_ops
from tensorflow.python.platform import googletest


class SparseTensorTest(test_util.TensorFlowTestCase):

  def testPythonConstruction(self):
    indices = [[1, 2], [2, 0], [3, 4]]
    values = [b""a"", b""b"", b""c""]
    shape = [4, 5]
    sp_value = sparse_tensor.SparseTensorValue(indices, values, shape)"
545341,3940270395935974165,276,BigQuery,"     #  0x96 -> <control>
    '\x97'     #  0x97 -> <control>
    '\x98'     #  0x98 -> <control>
    '\x99'     #  0x99 -> <control>
    '\x9a'     #  0x9A -> <control>
    '\x9b'     #  0x9B -> <control>
    '\x9c'     #  0x9C -> <control>
    '\x9d'     #  0x9D -> <control>
    '\x9e'     #  0x"
1241305,-3438754932381242929,37,BigQuery,"list',
                options=dict(
                    address=dict(required=True),
                    port=dict(default=80)
                )
            ),
            inbound_virtual=dict(
                type='dict',
                options=dict(
                    address=dict(required=True),
                    netmask=dict(required=True),
                    port=dict(default=80)
                )
            ),
            service_environment=dict(),
            add_analytics=dict(type='bool', default='no'),
            state=dict(
                default='present',
                choices=['present', 'absent']
            ),
            wait=dict("
1786173,-3008202054366103337,14,BigQuery," ns3::Application > > >', 
                   [], 
                   is_const=True)
    ## application-container.h (module 'network'): __gnu_cxx::__normal_iterator<const ns3::Ptr<ns3::Application>*,std::vector<ns3::Ptr<ns3::Application>, std::allocator<ns3::Ptr<ns3::Application> > > > ns3::ApplicationContainer::End() const [member function]
    cls.add_method('End', 
                   '__gnu_cxx::__normal_iterator< ns3::Ptr< ns3::Application > const, std::vector< ns3::Ptr"
5529708,-661788393314488815,11,BigQuery,"

    def setlistdefault(self, key, default_list=None):
        self._assert_mutable()
        return super(QueryDict, self).setlistdefault(key, default_list)

    def appendlist(self, key, value):
        self._assert_mutable()
        key = bytes_to_text(key, self.encoding)
        value = bytes_to_text(value, self.encoding)
        super(QueryDict, self).appendlist(key, value)

    def pop(self, key, *args):
        self._assert_mutable()
        return super(QueryDict, self"
2210638,-1621776738055598422,12,BigQuery,"[param('ns3::Ptr< ns3::LrWpanPhy >', 'phy'), param('ns3::Ptr< ns3::MobilityModel >','m')])
    ## lr-wpan-helper.h (module 'lr-wpan'): ns3::NetDeviceContainer ns3::LrWpanHelper::Install(ns3::NodeContainer c) [member function]
    cls.add_method('Install', 
                   'ns3::NetDeviceContainer', 
                   [param('ns3::NodeContainer', 'c')])
    ## lr-wpan-helper.h (module 'lr-wpan'"
7364685,5979630198463522567,31,BigQuery,"(format_spec, unicode)

    return format_dict

def _format_align(sign, body, spec):
    """"""Given an unpadded, non-aligned numeric string 'body' and sign
    string'sign', add padding and alignment conforming to the given
    format specifier dictionary'spec' (as produced by
    parse_format_specifier).

    Also converts result to unicode if necessary.

    """"""
    # how much extra space do we have to play with?
    minimumwidth = spec['minimumwidth']
    fill = spec['fill']
    padding = fill*(minimumwidth - len(sign) - len(body))

    "
3938548,6893012431016876681,6,BigQuery,"__o = None
        else: elem__o = elem._o
        if ns is None: ns__o = None
        else: ns__o = ns._o
        ret = libxml2mod.xmlValidateOneNamespace(ctxt__o, self._o, elem__o, prefix, ns__o, value)
        return ret

    def validatePopElement(self, ctxt, elem, qname):
        """"""Pop the element end from the validation stack. """"""
        if ctxt is None: ctxt__o = None
        else: ctxt__o = ctxt._o
        if elem is None: elem"
6652181,-7076906888374432118,18,BigQuery,"es = {
    100: 'Continue',
    101: 'Switching Protocols',

    200: 'OK',
    201: 'Created',
    202: 'Accepted',
    203: 'Non-Authoritative Information',
    204: 'No Content',
    205: 'Reset Content',
    206: 'Partial Content',

    300: 'Multiple Choices',
    301: 'Moved Permanently',
    302: 'Found',
    303: 'See Other',
    304: 'Not Modified',
    305: 'Use Proxy',
    306: '(Unused)',
    307: 'Temporary Redirect',

"
7394712,5735766363030811514,12,BigQuery,"num_classes=None,
              is_training=True,
              global_pool=True,
              output_stride=None,
              include_root_block=True,
              spatial_squeeze=True,
              reuse=None,
              scope=None):
  """"""Generator for v1 ResNet models.

  This function generates a family of ResNet v1 models. See the resnet_v1_*()
  methods for specific model instantiations, obtained by selecting different
  block instantiations that produce ResNets of various depths.

  Training for image classification on Imagenet is usually done with [224, 224]
  "
575297,-9023276013369089174,9,BigQuery,"`\|~]'


    # CHAR elements - fix the above elementcontentchar, quotattrcontentchar,
    #                 aposattrcontentchar
    #x9 | #xA | #xD | [#x20-#xD7FF] | [#xE000-#xFFFD] | [#x10000-#x10FFFF]

    flags = re.DOTALL | re.MULTILINE | re.UNICODE

    def punctuation_root_callback(lexer, match, ctx):
        yield match.start(), Punctuation, match.group(1)
        # transition to root always - don't pop"
7289048,7510753176897736249,52,BigQuery," self.dispose_func:
            for value in values:
                self.dispose_func(value)

    def keys(self):
        with self.lock:
            return list(iterkeys(self._container))


class HTTPHeaderDict(MutableMapping):
    """"""
    :param headers:
        An iterable of field-value pairs. Must not contain multiple field names
        when compared case-insensitively.

    :param kwargs:
        Additional field-value pairs to pass in to ``dict.update``.

    A ``dict`` like container for storing HTTP Headers.

    Field names are stored and compared case-ins"
4665874,1738365103789456484,12,BigQuery,"
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_284', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_285', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY,"
3044959,-59482851243978165,45,BigQuery,"1] in "" \t"":
            i = i-1
        if i > 0 and line[i-1] == ""\n"":
            i = i-1
        while i > 0 and line[i-1] in "" \t"":
            i = i-1
        line = line[:i]
        more = self.interp.runsource(line)

    def open_stack_viewer(self, event=None):
        if self.interp.rpcclt:
            return self.interp.remote_stack_viewer()
        try:
            sys.last_traceback
        except:
            tkMessage"
7364471,-836811920642439673,16,BigQuery,"

        _java_obj.set(""evaluator"", evaluator)
        _java_obj.set(""estimator"", estimator)
        _java_obj.set(""estimatorParamMaps"", epms)
        return _java_obj


if __name__ == ""__main__"":
    import doctest

    from pyspark.sql import SparkSession
    globs = globals().copy()

    # The small batch size here ensures that we see multiple batches,
    # even in these small test examples:
    spark = SparkSession.builder\
       .master(""local[2]"")\
       .appName(""ml."
378761,-4835601664795879749,75,BigQuery,e1a\u2e1b\u2e1e\u2e1f\u2e20\u2e21\u2e22\u2e23\u2e24\u2e25\u2e26\u2e27\u2e28\u2e29\u2e2a\u2e2b\u2e2c\u2e2d\u2e2e\u2e2f\u2e30\u2e31\u2e32\u2e33\u2e34\u2e35\u2e36\u2e37\u2
5197502,-8492846960879524237,31,BigQuery,"Boise',
 'America/Buenos_Aires',
 'America/Cambridge_Bay',
 'America/Campo_Grande',
 'America/Cancun',
 'America/Caracas',
 'America/Catamarca',
 'America/Cayenne',
 'America/Cayman',
 'America/Chicago',
 'America/Chihuahua',
 'America/Coral_Harbour',
 'America/Cordoba',
 'America/Costa_Rica',
 'America/Creston',
 'America/Cuiaba',
 'America/Curacao',
 'America/Danmarkshavn',
"
5014689,6616317808987232458,8,BigQuery,"er <mail@renemoser.net>
# (c) 2015, Stefan Berggren <nsg@nsg.cc>
# (c) 2014, Ramon de la Fuente <ramon@delafuente.nl>
#
# This file is part of Ansible
#
# Ansible is free software: you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation, either version 3 of the License, or
# (at your option) any later version.
#
# Ansible is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY;"
1149724,696222938484823168,8,BigQuery,"
            return _dec_from_triple(dup._sign, '0', 0)
        exp_max = [context.Emax, context.Etop()][context._clamp]
        end = len(dup._int)
        exp = dup._exp
        while dup._int[end-1] == '0' and exp < exp_max:
            exp += 1
            end -= 1
        return _dec_from_triple(dup._sign, dup._int[:end], exp)

    def quantize(self, exp, rounding=None, context=None, watchexp=True):
        """"""Quantize self so"
6242110,1407477497419377175,9,BigQuery,"))

    def depart_emphasis(self, node):
        self.body.append('</em>')

    def visit_entry(self, node):
        atts = {'class': []}
        if isinstance(node.parent.parent, nodes.thead):
            atts['class'].append('head')
        if node.parent.parent.parent.stubs[node.parent.column]:
            # ""stubs"" list is an attribute of the tgroup element
            atts['class'].append('stub')
        if atts['class']:
            tagname = 'th'
            atts['class'] =''.join(atts"
5197098,2004579449670119946,12,BigQuery,"NO_INTERNAL_LOOKUP
SESS_CACHE_NO_INTERNAL_STORE = _lib.SSL_SESS_CACHE_NO_INTERNAL_STORE
SESS_CACHE_NO_INTERNAL = _lib.SSL_SESS_CACHE_NO_INTERNAL

SSL_ST_CONNECT = _lib.SSL_ST_CONNECT
SSL_ST_ACCEPT = _lib.SSL_ST_ACCEPT
SSL_ST_MASK = _lib.SSL_ST_MASK
if _lib.Cryptography_HAS_SSL_ST:
    SSL_ST_"
5514998,1866829921869421751,20,BigQuery,"_value(a['B']['BB']))
        assert_equal(test[1], maximum_fill_value(a['B']))

    def test_fillvalue_individual_fields(self):
        # Test setting fill_value on individual fields
        ndtype = [('a', int), ('b', int)]
        # Explicit fill_value
        a = array(list(zip([1, 2, 3], [4, 5, 6])),
                  fill_value=(-999, -999), dtype=ndtype)
        aa = a['a']
        aa.set_fill_value(10)
        assert_equal(aa"
4665874,-6938679807316472553,12,BigQuery,"', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_255', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_254', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', '"
6075305,7372167903983140797,6,BigQuery,"idx = _nearest_real_complex_idx(z, p2,'real')
                    z2 = z[z2_idx]
                    assert np.isreal(z2)
                    z = np.delete(z, z2_idx)
                p = np.delete(p, p2_idx)
        p_sos[si] = [p1, p2]
        z_sos[si] = [z1, z2]
    assert len(p) == len(z) == 0  # we've consumed all poles and zeros
    del p, z

    # Construct the system, reversing"
2528237,-1097328847729104556,6,BigQuery," __init__(self, *args, **kw):
        super().__init__(*args, **kw)
        self._read_fut = self._loop._proactor.recv(self._sock, 16)
        self._read_fut.add_done_callback(self._pipe_closed)

    def _pipe_closed(self, fut):
        if fut.cancelled():
            # the transport has been closed
            return
        assert fut.result() == b''
        if self._closing:
            assert self._read_fut is None
            return
        assert fut is self._read_fut, (f"
1119347,8851894591604516275,6,BigQuery,"ip_): n += 1 + self.lengthString(self.proxy_external_ip_.ByteSizePartial())
    return n

  def Clear(self):
    self.clear_proxy_external_ip()

  def OutputUnchecked(self, out):
    if (self.has_proxy_external_ip_):
      out.putVarInt32(18)
      out.putVarInt32(self.proxy_external_ip_.ByteSize())
      self.proxy_external_ip_.OutputUnchecked(out)

  def OutputPartial(self, out):
    if (self.has_proxy_external_ip_):
      out"
5317939,-8259852151078044977,17,BigQuery,"terminal.nextLine()
        self.lineReceived(line)

    def handle_INSERT(self):
        assert self.mode in ('typeover', 'insert')
        if self.mode == 'typeover':
            self.setInsertMode()
        else:
            self.setTypeoverMode()

    def lineReceived(self, line):
        pass

class HistoricRecvLine(RecvLine):
    """"""L{TerminalProtocol} which adds both basic line-editing features and input history.

    Everything supported by L{RecvLine} is also supported by this class.  In addition, the
    up and down arrows"
3513970,1217753017887331425,10,BigQuery,"Is this response from our local cache""""""
    fromcache = False

    """"""HTTP protocol version used by server. 10 for HTTP/1.0, 11 for HTTP/1.1. """"""
    version = 11

    ""Status code returned by server. ""
    status = 200

    """"""Reason phrase returned by server.""""""
    reason = ""Ok""

    previous = None

    def __init__(self, info):
        # info is either an email.message or
        # an httplib.HTTPResponse object.
        if isinstance(info, http.client.HTTPResponse):
            for key, value in info.getheaders"
5483945,-1740999708054066433,6,BigQuery,"    def dump_stats(self, file):
        f = open(file, 'wb')
        self.create_stats()
        marshal.dump(self.stats, f)
        f.close()

    def create_stats(self):
        self.simulate_cmd_complete()
        self.snapshot_stats()

    def snapshot_stats(self):
        self.stats = {}
        for func, (cc, ns, tt, ct, callers) in self.timings.items():
            callers = callers.copy()
            nc = 0
            for callcnt in callers.values():
                "
3361989,8120057749591830598,6,BigQuery," an exception."""""")


def with_metaclass(meta, *bases):
    """"""Create a base class with a metaclass.""""""
    return meta(""NewBase"", bases, {})

def add_metaclass(metaclass):
    """"""Class decorator for creating a class with a metaclass.""""""
    def wrapper(cls):
        orig_vars = cls.__dict__.copy()
        orig_vars.pop('__dict__', None)
        orig_vars.pop('__weakref__', None)
        slots = orig_vars.get('__slots__')
        if"
6773063,-6627823289340006142,24,BigQuery,"std::string n1, ns3::AttributeValue const & v1) [member function]
    cls.add_method('SetChannelAttribute', 
                   'void', 
                   [param('std::string', 'n1'), param('ns3::AttributeValue const &', 'v1')])
    ## csma-helper.h (module 'csma'): void ns3::CsmaHelper::SetDeviceAttribute(std::string n1, ns3::AttributeValue const & v1) [member function]
    cls.add_method('SetDeviceAttribute', 
                   'void', 
                   [param('std::string', 'n1'),"
5424214,-7960499827417447559,29,BigQuery,"option.runxfail:
        old = pytest.xfail
        config._cleanup.append(lambda: setattr(pytest, ""xfail"", old))
        def nop(*args, **kwargs):
            pass
        nop.Exception = XFailed
        setattr(pytest, ""xfail"", nop)

    config.addinivalue_line(""markers"",
        ""skipif(condition): skip the given test function if eval(condition) ""
        ""results in a True value.  Evaluation happens within the ""
        ""module global context. Example: skipif('sys.platform == \""win32\""') ""
"
2027971,1391175762560648246,7,BigQuery,"/to/js4')

        w1 = MyWidget1()
        w2 = MyWidget2()
        w3 = MyWidget3()
        self.assertEqual(
            str(w1.media + w2.media + w3.media),
            """"""<link href=""http://media.example.com/static/path/to/css1"" type=""text/css"" media=""all"" rel=""stylesheet"">
<link href=""/path/to/css2"" type=""text/css"" media=""all"" rel=""stylesheet"">
<link href=""/path/to/css3"" type=""text/css"" media=""all"" rel="""
2453726,7280360593393378296,49,BigQuery," for x in t:
   ...     print(' '+x, end='')
     A B C D E F G H I J K L M N O P Q R S T U V W X Y Z

""""""

# Examples from Iterator-List and Python-Dev and c.l.py.

email_tests = """"""

The difference between yielding None and returning it.

>>> def g():
...     for i in range(3):
...         yield None
...     yield None
...     return
>>> list(g())
[None, None, None, None]

Ensure that explicitly raising StopIteration acts like any other"
378761,5870223940566252888,75,BigQuery,uf72c\uf72d\uf72e\uf72f\uf730\uf731\uf732\uf733\uf734\uf735\uf736\uf737\uf738\uf739\uf73a\uf73b\uf73c\uf73d\uf73e\uf73f\uf740\uf741\uf742\uf743\uf744\uf745\uf746\uf747\uf748\uf749\uf74a\uf74b\uf74c\uf74d\uf74e\uf74f\uf750\uf751\uf752\uf753\uf
378761,1268410037001209628,75,BigQuery,\u3aae\u3aaf\u3ab0\u3ab1\u3ab2\u3ab3\u3ab4\u3ab5\u3ab6\u3ab7\u3ab8\u3ab9\u3aba\u3abb\u3abc\u3abd\u3abe\u3abf\u3ac0\u3ac1\u3ac2\u3ac3\u3ac4\u3ac5\u3ac6\u3ac7\u3ac8\u3ac9\u3aca\u3acb\u3acc\
4620588,7182623579642825463,14,BigQuery,"): bool ns3::YansWifiPhy::IsStateIdle() [member function]
    cls.add_method('IsStateIdle', 
                   'bool', 
                   [], 
                   is_virtual=True)
    ## yans-wifi-phy.h (module 'wifi'): bool ns3::YansWifiPhy::IsStateBusy() [member function]
    cls.add_method('IsStateBusy', 
                   'bool', 
                   [], 
                   is_virtual=True)
    ## yans-wifi-phy.h (module 'wifi'): bool ns3"
1104728,6683896041503954941,10,BigQuery,"vert_to_simple_type(c, formatted))
			nres.append(nr)
		return nres

	def build_conditions(self, filters):
		""""""Convert filters sent as dict, lists to SQL conditions. filter's key
		is passed by map function, build conditions like:

		* ifnull(`fieldname`, default_value) = %(fieldname)s
		* `fieldname` [=,!=, >, >=, <, <=] %(fieldname)s
		""""""
		conditions = []
		values = {}
		def _build_condition(key):
			""""""
				filter's key is passed by"
1830869,7861414817322192896,7,BigQuery," n1 + [base] + n2
      targets = number_to_lower_endian(result, base)
      yield {""inputs"": inputs, ""targets"": targets}


@registry.register_problem
class AlgorithmicMultiplicationDecimal40(AlgorithmicMultiplicationBinary40):
  """"""Problem spec for algorithmic decimal multiplication task.""""""

  @property
  def num_symbols(self):
    return 10


@registry.register_problem
class AlgorithmicReverseBinary40Test(AlgorithmicReverseBinary40):
  """"""Test Problem with tiny dataset."""""""
2649704,-4895079050242025167,8,BigQuery,"rq
   zunmqr

""""""
#
# Author: Pearu Peterson, March 2002
#

from __future__ import division, print_function, absolute_import

__all__ = ['get_lapack_funcs']

from.blas import _get_funcs

# Backward compatibility:
from.blas import find_best_blas_type as find_best_lapack_type

from scipy.linalg import _flapack
try:
    from scipy.linalg import _clapack
except ImportError:
    _clapack = None

# Backward"
7410073,-1838525424887964537,25,BigQuery,"    from sympy.matrices.expressions import MatrixSymbol
    assert str(MatrixSymbol('X', 10, 10)[:5, 1:9:2]) == 'X[:5, 1:9:2]'
    assert str(MatrixSymbol('X', 10, 10)[5, :5:2]) == 'X[5, :5:2]'

def test_true_false():
    assert str(true) == repr(true) == sstr(true) == ""True""
    assert str(false) == repr(false) == sstr(false) == ""False""

def test_Equivalent():
    assert str("
6137054,-4861010494690859283,64,BigQuery," (c) 2012-TODAY OpenERP S.A. <http://openerp.com>
#
#    This program is free software: you can redistribute it and/or modify
#    it under the terms of the GNU Affero General Public License as
#    published by the Free Software Foundation, either version 3 of the
#    License, or (at your option) any later version.
#
#    This program is distributed in the hope that it will be useful,
#    but WITHOUT ANY WARRANTY; without even the implied warranty of
#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#    GNU"
5590233,1740320622660337675,9,BigQuery,"_info
  gtkp_x11_2_info
  gtkp_2_info
  xft_info
  freetype2_info
  umfpack_info

Usage:
    info_dict = get_info(<name>)
  where <name> is a string 'atlas','x11','fftw','lapack','blas',
  'lapack_src', 'blas_src', etc. For a complete list of allowed names,
  see the definition of get_info() function below.

  Returned info_dict is a dictionary which is compatible with
  distutils.setup keyword arguments. If info_dict"
3605482,-2658270861543801082,69,BigQuery,"        assert_array_almost_equal(
            rout.cov_beta,
            np.array([[0.6391799462548782, -0.1955657291119177],
               [-0.1955657291119177, 0.0624888159223392]]),
        )

    # Lorentz Peak
    # The data is taken from one of the undergraduate physics labs I performed.

    def lorentz(self, beta, x):
        return (beta[0]*beta[1]*beta[2] / np.sqrt(np.power(x*x -
            beta[2"
5483945,-391535813861303830,6,BigQuery," name
            self.co_firstlineno = 0

        def __repr__(self):
            return repr((self.co_filename, self.co_line, self.co_name))

    class fake_frame:
        def __init__(self, code, prior):
            self.f_code = code
            self.f_back = prior

    def simulate_call(self, name):
        code = self.fake_code('profile', 0, name)
        if self.cur:
            pframe = self.cur[-2]
        else:
            pframe = None
        frame = self.fake_frame(code"
6151926,768054194567390416,26,BigQuery,"et(_MSG_KEXDH_GEX_INIT)

    def _parse_kexdh_gex_request_old(self, m):
        # same as above, but without min_bits or max_bits (used by older clients like putty)
        self.preferred_bits = m.get_int()
        # smoosh the user's preferred size into our own limits
        if self.preferred_bits > self.max_bits:
            self.preferred_bits = self.max_bits
        if self.preferred_bits < self.min_bits:
            self.preferred_bits = self.min_bits"
1861551,4839024154874220996,9,BigQuery," main playbook code, don't use this for other
        reasons.
        """"""
        if restriction is None:
            return
        elif not isinstance(restriction, list):
            restriction = [restriction]
        self._restriction = [h.name for h in restriction]

    def subset(self, subset_pattern):
        """"""
        Limits inventory results to a subset of inventory that matches a given
        pattern, such as to select a given geographic of numeric slice amongst
        a previous 'hosts' selection that only select roles, or vice versa.
        Corresponds to --limit parameter to ansible-playbook
        """"""
        if subset"
5514998,-4782244174944101539,20,BigQuery,"        (x, y, a10, m1, m2, xm, ym, z, zm, xf) = self.d
        assert_equal(np.cos(x), cos(xm))
        assert_equal(np.cosh(x), cosh(xm))
        assert_equal(np.sin(x), sin(xm))
        assert_equal(np.sinh(x), sinh(xm))
        assert_equal(np.tan(x), tan(xm))
        assert_equal(np.tanh(x), tanh(xm))
        assert_equal(np.sq"
6439377,5193110732999316824,10,BigQuery,"ator_.predict_proba(self.transform(X))


class RFECV(RFE, MetaEstimatorMixin):
    """"""Feature ranking with recursive feature elimination and cross-validated
       selection of the best number of features.

    Parameters
    ----------
    estimator : object
        A supervised learning estimator with a `fit` method that updates a
        `coef_` attribute that holds the fitted parameters. Important features
        must correspond to high absolute values in the `coef_` array.

        For instance, this is the case for most supervised learning
        algorithms such as Support Vector Classifiers and Generalized
        Linear Models from"
3408348,2490817415198930479,20,BigQuery,"
  in-memory array. There is also an option to skew the distribution by
  applying a distortion power to the weights.

  In addition, this operation returns tensors `true_expected_count`
  and `sampled_expected_count` representing the number of times each
  of the target classes (`true_classes`) and the sampled
  classes (`sampled_candidates`) is expected to occur in an average
  tensor of sampled classes.  These values correspond to `Q(y|x)`
  defined in [this
  document](http://www.tensorflow.org/extras/candidate_sampling.pdf).
"
6151456,-1531602329884245764,9,BigQuery,"ly this code in would fail in debug mode
        # with Undetected Error and Stop Iteration
        r1 = Repeater(1, 3, StopIteration)
        r2 = Repeater(2, 4, StopIteration)
        def run(r1, r2):
            result = []
            for i, j in izip_longest(r1, r2, fillvalue=0):
                with test_support.captured_output('stdout'):
                    print (i, j)
                result.append((i, j))
            return result
        self.assertEqual(run(r1, r2), [(1"
1407487,-8922277310613118863,11,BigQuery,"):
    """"""
    A template variable, resolvable against a given context. The variable may
    be a hard-coded string (if it begins and ends with single or double quote
    marks)::

        >>> c = {'article': {'section':u'News'}}
        >>> Variable('article.section').resolve(c)
        u'News'
        >>> Variable('article').resolve(c)
        {'section': u'News'}
        >>> class AClass: pass
        >>> c = AClass()
        >>> c.article = AClass()
        >>> c.article.section = u'News'

    (The example"
5621125,-5827682115935474385,28,BigQuery," None, ), # 2
  )

  def __hash__(self):
    return 0 + hash(self.func_args) + hash(self.request_id)

  def __init__(self, func_args=None, request_id=None,):
    self.func_args = func_args
    self.request_id = request_id

  def read(self, iprot):
    if iprot.__class__ == TBinaryProtocol.TBinaryProtocolAccelerated and isinstance(iprot.trans, TTransport.CReadableTransport) and self.thrift_spec is not None and fastbinary is not None"
2467897,6651151514966361201,6,BigQuery," be useful for array-like arguments of a fixed length
    such as the coefficients for a polynomial as in `polyval`:

    >>> def mypolyval(p, x):
   ...     _p = list(p)
   ...     res = _p.pop(0)
   ...     while _p:
   ...         res = res*x + _p.pop(0)
   ...     return res
    >>> vpolyval = np.vectorize(mypolyval, excluded=['p'])
    >>> vpolyval(p=[1, 2, 3], x=[0, 1])
    array([3, 6])

    Pos"
4939398,-1655947536610962751,17,BigQuery,"Dict()
        if iterable:
            for o in iterable:
                self.add(o)


if sys.version_info >= (2, 5):
    class PopulateDict(dict):
        """"""A dict which populates missing values via a creation function.

        Note the creation function takes a key, unlike
        collections.defaultdict.

        """"""

        def __init__(self, creator):
            self.creator = creator

        def __missing__(self, key):
            self[key] = val = self.creator(key)
            return val
else:
    class PopulateDict(dict):
        """""""
6773063,-1538361687984208378,24,BigQuery,"TypeId', 
                   [], 
                   is_static=True)
    ## random-variable-stream.h (module 'core'): ns3::LogNormalRandomVariable::LogNormalRandomVariable() [constructor]
    cls.add_constructor([])
    ## random-variable-stream.h (module 'core'): double ns3::LogNormalRandomVariable::GetMu() const [member function]
    cls.add_method('GetMu', 
                   'double', 
                   [], 
                   is_const=True)
    ## random-variable-stream.h (module 'core'): double ns3::LogNormalRandomVariable"
1786173,-6296021083279550815,14,BigQuery,"stats'): ns3::DataOutputCallback::DataOutputCallback(ns3::DataOutputCallback const & arg0) [copy constructor]
    cls.add_constructor([param('ns3::DataOutputCallback const &', 'arg0')])
    ## data-output-interface.h (module'stats'): void ns3::DataOutputCallback::OutputSingleton(std::string key, std::string variable, int val) [member function]
    cls.add_method('OutputSingleton', 
                   'void', 
                   [param('std::string', 'key'), param('std::string', 'variable'), param('int', 'val')], "
2164964,4574808907428463055,268,BigQuery,"if lineno!= lastlineno:
        yield (addr, lineno)

def _test():
    """"""Simple test program to disassemble a file.""""""
    if sys.argv[1:]:
        if sys.argv[2:]:
            sys.stderr.write(""usage: python dis.py [-|file]\n"")
            sys.exit(2)
        fn = sys.argv[1]
        if not fn or fn == ""-"":
            fn = None
    else:
        fn = None
    if fn is None:
        f = sys.stdin
    else:
        f = open"
7409348,6897326248722767744,7,BigQuery,"_to_create_string_with_double_quotes(self):
        string = 'He said, ""Go Away.""'
        self.assertEqual(__, string)
  
    def test_use_double_quotes_to_create_strings_with_single_quotes(self):
        string = ""Don't""
        self.assertEqual(__, string)
    
    def test_use_backslash_for_escaping_quotes_in_strings(self):
        a = ""He said, \""Don't\""""
        b = 'He said, ""Don\'t""'
        self.assertEqual"
1240697,3416995341958515075,9,BigQuery,"rk[p+1] >> 16) & 0xff] & 0xff] ^
          Td2[Te4[(rk[p+1] >>  8) & 0xff] & 0xff] ^
          Td3[Te4[(rk[p+1]      ) & 0xff] & 0xff])
        rk[p+2] = (
          Td0[Te4[(rk[p+2] >> 24)       ] & 0xff] ^
          Td1[Te4[(rk[p+2] >> 16) & 0xff] & 0xff] ^
          Td2[Te"
4787914,-1006024684519598509,23,BigQuery," only once time on same taxes.')
    ]

account_fiscal_position_tax()

class account_fiscal_position_account(osv.osv):
    _name = 'account.fiscal.position.account'
    _description = 'Accounts Fiscal Position'
    _rec_name = 'position_id'
    _columns = {
        'position_id': fields.many2one('account.fiscal.position', 'Fiscal Position', required=True, ondelete='cascade'),
        'account_src_id': fields.many2one('account.account', 'Account Source', domain=[('type','<>"
1786173,-93909203279319267,14,BigQuery,"
                   'double', 
                   [], 
                   is_pure_virtual=True, is_const=True, is_virtual=True)
    ## data-calculator.h (module'stats'): double ns3::StatisticalSummary::getSum() const [member function]
    cls.add_method('getSum', 
                   'double', 
                   [], 
                   is_pure_virtual=True, is_const=True, is_virtual=True)
    ## data-calculator.h (module'stats'): double ns3::StatisticalSummary::getVariance() const [member function]
    cls.add"
4787232,-996600254995722771,7,BigQuery,"xdeg + 1, ydeg + 1, zdeg + 1) in the order

   .. math:: c_{000}, c_{001}, c_{002},..., c_{010}, c_{011}, c_{012},...

    and  ``np.dot(V, c.flat)`` and ``hermval3d(x, y, z, c)`` will be the
    same up to roundoff. This equivalence is useful both for least squares
    fitting and for the evaluation of a large number of 3-D Hermite
    series of the same degrees and sample points.

    Parameters
    ----------
    x, y, z : array_like
        "
4665874,-4667268648485306257,12,BigQuery,"', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_222', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_cf_standard (KEY, 'col_2', 'col_3', 'col_0', 'col_1', 'col_4') VALUES ('row_223', 'val_2', 'val_3', 'val_0', 'val_1', 'val_4');
INSERT INTO rolling_"
6515695,5047939285203454389,37,BigQuery,"add(dist)
        self.install_egg_scripts(dist)
        self.installed_projects[dist.key] = dist
        log.info(self.installation_report(requirement, dist, *info))
        if (dist.has_metadata('dependency_links.txt') and
                not self.no_find_links):
            self.package_index.add_find_links(
                dist.get_metadata_lines('dependency_links.txt')
            )
        if not deps and not self.always_copy:
            return
        elif requirement is not None and dist.key!= requirement.key:
            "
7455041,6399842627568391011,17,BigQuery,"match_argument = self._match_argument
            action_tuples = []
            while True:

                # if we found no optional action, skip it
                if action is None:
                    extras.append(arg_strings[start_index])
                    return start_index + 1

                # if there is an explicit argument, try to match the
                # optional's string arguments to only this
                if explicit_arg is not None:
                    arg_count = match_argument(action, 'A')

                    # if the action is a single-dash option and takes no
                    # arguments, try to parse more single-dash options out
                    # of the tail of the"
4182737,-8399839599297778789,69,BigQuery,":
                failures = True
            if s['unreachable'] > 0:
                unreachable = True

            t.add_row([h] + [s[k] for k in ['ok', 'changed', 'unreachable',
                                            'failures']])

        self.send_msg(""%s: Playbook complete"" % self.playbook_name,
                      notify=True)

        if failures or unreachable:
            color ='red'
            self.send_msg(""%s: Failures detected"" % self.playbook_name,
                          color=color, notify=True)
        else:
            color"
4909263,-6752170028439293908,28,BigQuery,"f',
0x1d60e:u'g', 0x1d60f:u'h', 0x1d610:u'i', 0x1d611:u'j',
0x1d612:u'k', 0x1d613:u'l', 0x1d614:u'm', 0x1d615:u'n',
0x1d616:u'o', 0x1d617:u'p', 0x1d618:u'q', 0x1d619:u'r',
0x1d61a:u's', 0x1d61b:"
1573604,-1135752599123754204,7,BigQuery,"
    y = activation_fn(y)
  ops.add_to_collections(list(output_collections or []) +
                         [ops.GraphKeys.ACTIVATIONS], y)
  return y


def repeat(inputs, repetitions, layer, *args, **kwargs):
  """"""Applies the same layer with the same arguments repeatedly.

  ```python
    y = repeat(x, 3, conv2d, 64, [3, 3], scope='conv1')
    # It is equivalent to:

    x = conv2d(x, 64, [3, 3], scope='conv1/conv1_1')
"
2378077,3549685832728453739,31,BigQuery,"_user)'
    default: postgres
  password:
    description:
      - The password to authenticate with.
      - 'Alias: I(login_password))'
    default: null
    required: no
notes:
  - Default authentication assumes that postgresql_privs is run by the
    C(postgres) user on the remote host. (Ansible's C(user) or C(sudo-user)).
  - This module requires Python package I(psycopg2) to be installed on the
    remote host. In the default case of the remote host also being the
    PostgreSQL server, PostgreSQL has to be installed there as well"
377975,-4544429207816859976,8,BigQuery,"ATTERN_INCLUDE.search(line)
  if match:
    include = match.group(2)
    is_system = (match.group(1) == '<')
    duplicate_line = include_state.FindHeader(include)
    if duplicate_line >= 0:
      error(filename, linenum, 'build/include', 4,
            '""%s"" already included at %s:%s' %
            (include, filename, duplicate_line))
    elif (include.endswith('.cc') and
          os.path.dirname(fileinfo.RepositoryName())!= os.path.dir"
1512768,2169595074293123711,6,BigQuery," = "">""
        elif self._prefix == II:
            self._endian = ""<""
        else:
            raise SyntaxError(""not a TIFF IFD"")
        self.reset()
        self.next, = self._unpack(""L"", ifh[4:])
        self._legacy_api = False

    prefix = property(lambda self: self._prefix)
    offset = property(lambda self: self._offset)
    legacy_api = property(lambda self: self._legacy_api)

    @legacy_api.setter
    def legacy_api(self, value):
        raise Exception(""Not allowing setting"
3863000,9171473051086108199,9,BigQuery," = importlib.util.cache_from_source(
                    file, optimization=opt)
            else:
                cfile = importlib.util.cache_from_source(file)
            dfile = file
            if prefix:
                if file[:len(prefix)]!= prefix:
                    raise ValueError(""invalid prefix: filename %r doesn't start with %r""
                           % (file, prefix))
                dfile = dfile[len(prefix):]
            if base_dir:
                dfile = os.path.join(base_dir, dfile)

            cfile_base = os.path.basename(cfile)
"
923177,-4392309613211835070,8,BigQuery,"# test again with failing write()
                    with self.assertRaises(FileNotFoundError):
                        tempfile._get_default_tempdir()
                    self.assertEqual(os.listdir(our_temp_directory), [])


class TestGetCandidateNames(BaseTestCase):
    """"""Test the internal function _get_candidate_names.""""""

    def test_retval(self):
        # _get_candidate_names returns a _RandomNameSequence object
        obj = tempfile._get_candidate_names()
        self.assertIsInstance(obj, tempfile._RandomNameSequence)

    def test_same_"
6802952,7128453686522336799,12,BigQuery,".append('-r')

        cmd.append(self.name)
        return self.execute_command(cmd)


    def _check_usermod_append(self):
        # check if this version of usermod can append groups
        usermod_path = self.module.get_bin_path('usermod', True)

        # for some reason, usermod --help cannot be used by non root
        # on RH/Fedora, due to lack of execute bit for others
        if not os.access(usermod_path, os.X_OK):
            return False

        cmd = [usermod_path, '--help']
        ("
4528648,-1112009052547022955,6,BigQuery,"creation_counter is used for fields that Django implicitly
    # creates, creation_counter is used for all user-specified fields.
    creation_counter = 0
    auto_creation_counter = -1
    default_validators = []  # Default set of validators
    default_error_messages = {
        'invalid_choice': _('Value %(value)r is not a valid choice.'),
        'null': _('This field cannot be null.'),
        'blank': _('This field cannot be blank.'),
        'unique': _('%(model_name)s with this %(field_label)s '
                    'already exists"
378761,9043785890655832911,75,BigQuery,u4544\u4545\u4546\u4547\u4548\u4549\u454a\u454b\u454c\u454d\u454e\u454f\u4550\u4551\u4552\u4553\u4554\u4555\u4556\u4557\u4558\u4559\u455a\u455b\u455c\u455d\u455e\u455f\u4560\u4561\u4562\u4563\u4564\u4565\u4566\u4567\u4568\u45
1119851,7476222708470990817,9,BigQuery," all runtime messages')
    Trace.error('')
    Trace.error('  Advanced options:')
    Trace.error('    --debug:                enable debugging messages (for developers)')
    Trace.error('    --version:              show version number and release date')
    Trace.error('    --lyxformat:            return the highest LyX version supported')
    Trace.error('  Options for HTML output:')
    Trace.error('    --title ""title"":        set the generated page title')
    Trace.error('    --css ""file.css"":       use a custom CSS file')
    Trace.error('    "
5454018,3306219778644726566,7,BigQuery,"
        stream = asyncio.StreamReader(loop=self.loop)

        stream.feed_data(self.DATA)
        self.assertEqual(self.DATA, stream._buffer)

    def test_read_zero(self):
        # Read zero bytes.
        stream = asyncio.StreamReader(loop=self.loop)
        stream.feed_data(self.DATA)

        data = self.loop.run_until_complete(stream.read(0))
        self.assertEqual(b'', data)
        self.assertEqual(self.DATA, stream._buffer)

    def test_read(self):
        "
5136130,1865617556484499520,7,BigQuery,"::ObjectPtrContainerAccessor::ObjectPtrContainerAccessor(ns3::ObjectPtrContainerAccessor const & arg0) [copy constructor]
    cls.add_constructor([param('ns3::ObjectPtrContainerAccessor const &', 'arg0')])
    ## object-ptr-container.h (module 'core'): bool ns3::ObjectPtrContainerAccessor::Get(ns3::ObjectBase const * object, ns3::AttributeValue & value) const [member function]
    cls.add_method('Get', 
                   'bool', 
                   [param('ns3::ObjectBase const *', 'object'), param('ns3::AttributeValue &"
6878521,4004758251052798919,10,BigQuery,"        self.assertRaises(RuntimeError, shouldThrow)

class ContextmanagerAssertionMixin(object):
    TEST_EXCEPTION = RuntimeError(""test exception"")

    def assertInWithManagerInvariants(self, mock_manager):
        self.assertTrue(mock_manager.enter_called)
        self.assertFalse(mock_manager.exit_called)
        self.assertEqual(mock_manager.exit_args, None)

    def assertAfterWithManagerInvariants(self, mock_manager, exit_args):
        self.assertTrue("
529317,527777302645782686,6,BigQuery,"_SMTPUTF8 = True
            elif smtputf8 is not False:
                self.push('501 Error: SMTPUTF8 takes no arguments')
                return
        size = params.pop('SIZE', None)
        if size:
            if not size.isdigit():
                self.push(syntaxerr)
                return
            elif self.data_size_limit and int(size) > self.data_size_limit:
                self.push('552 Error: message size exceeds fixed maximum message size')
                return
        if len(params.keys()) > 0:
            self.push('555 MAIL FROM parameters not recognized"
1710834,3848236299094537708,22,BigQuery," collection)

        self.local_remote_pairs = self._deannotate_pairs(lrp)
        self.synchronize_pairs = self._deannotate_pairs(sync_pairs)
        self.secondary_synchronize_pairs = \
            self._deannotate_pairs(secondary_sync_pairs)

    @util.memoized_property
    def remote_columns(self):
        return self._gather_join_annotations(""remote"")

    @util.memoized_property
    def local_columns(self):
        return self._gather_join_annotations("""
6166978,6911810831136368220,24,BigQuery," &', 'arg0')])
    ## single-model-spectrum-channel.h (module'spectrum'): ns3::SingleModelSpectrumChannel::SingleModelSpectrumChannel() [constructor]
    cls.add_constructor([])
    ## single-model-spectrum-channel.h (module'spectrum'): void ns3::SingleModelSpectrumChannel::AddPropagationLossModel(ns3::Ptr<ns3::PropagationLossModel> loss) [member function]
    cls.add_method('AddPropagationLossModel', 
                   'void', 
                   [param('ns3::Ptr<"
6197007,-243626918912164243,9,BigQuery,"conn, ident)
        _ora_drop_ignore(conn, ""%s_ts1"" % ident)
        _ora_drop_ignore(conn, ""%s_ts2"" % ident)


@_update_db_opts.for_db(""oracle"")
def _oracle_update_db_opts(db_url, db_opts):
    pass


def reap_dbs(idents_file):
    log.info(""Reaping databases..."")

    urls = collections.defaultdict(set)
    idents = collections.defaultdict(set)

    with open(idents_file) as file_:
        for line"
3241292,8678203413085559682,6,BigQuery,"supports_download=False,
    )

    def List(self, request, global_params=None):
      """"""Retrieves the list of groups contained within the specified project.

      Args:
        request: (ClouduseraccountsGroupsListRequest) input message
        global_params: (StandardQueryParameters, default: None) global arguments
      Returns:
        (GroupList) The response message.
      """"""
      config = self.GetMethodConfig('List')
      return self._RunMethod(
          config, request, global_params=global_params)

    List.method_config = lambda: base_api.ApiMethodInfo(
        "
1786173,8325448199655917890,14,BigQuery,"h (module 'network'): uint32_t ns3::Socket::GetRxAvailable() const [member function]
    cls.add_method('GetRxAvailable', 
                   'uint32_t', 
                   [], 
                   is_pure_virtual=True, is_const=True, is_virtual=True)
    ## socket.h (module 'network'): int ns3::Socket::GetSockName(ns3::Address & address) const [member function]
    cls.add_method('GetSockName', 
                   'int', 
                   [param('ns3::Address &', 'address')], 
"
3695899,-1198173915774416504,11,BigQuery,"_HEADER + ""<vrfName>%s</vrfName>"" % vrf_name + \
                    ""<holdTime></holdTime>"" + CE_GET_BGP_INSTANCE_TAIL
                recv_xml = self.netconf_get_config(
                    module=module, conf_str=conf_str)

                if state == ""present"":
                    if ""<data/>"" in recv_xml:
                        need_cfg = True
                    else:
                        re_find = re.findall(
                            r'.*<holdTime>(.*)</holdTime>.*', recv_xml)

                        if re_find:
                            result"
5030102,-5357106549134306941,11,BigQuery,"group
from distutils import log
from distutils.dir_util import mkpath
from distutils.command import build_ext as _build_ext
from distutils import sysconfig

extension_name_re = _build_ext.extension_name_re

show_compilers = _build_ext.show_compilers

class Optimization(object):
    def __init__(self):
        self.flags = (
            'OPT',
            'CFLAGS',
            'CPPFLAGS',
            'EXTRA_CFLAGS',
            'BASECFLAGS',
            'PY_CFLAGS',"
862703,-1617104596437603955,9,BigQuery,".get('form')
        # (end NOTE)
        return ret

password_reset = PasswordResetView.as_view()


class PasswordResetDoneView(TemplateView):
    template_name = ""account/password_reset_done.html""

password_reset_done = PasswordResetDoneView.as_view()


class PasswordResetFromKeyView(AjaxCapableProcessFormViewMixin, FormView):
    template_name = ""account/password_reset_from_key.html""
    form_class = ResetPasswordKeyForm
    success_url = reverse_lazy(""account_reset_password_from_"
2134244,1652456147476385223,7,BigQuery,"property(self):
		d = self.get_customize_form(""User"")
		self.assertEquals(frappe.db.get_value(""Property Setter"",
			{""doc_type"": ""User"", ""property"": ""allow_copy""}, ""value""), None)

		d.allow_copy = 1
		d.run_method(""save_customization"")
		self.assertEquals(frappe.db.get_value(""Property Setter"",
			{""doc_type"": ""User"", ""property"": ""allow_copy""}, ""value""), '1')

		d.allow_copy = 0
		d.run_method(""save"
210730,2853239212333992275,6,BigQuery,"
            issued_at = self['issued_at']
        else:
            issued_at = self['token']['issued_at']
        return _parse_and_normalize_time(issued_at)

    @property
    def audit_id(self):
        if self.version is V3:
            return self.get('audit_ids', [None])[0]
        return self['token'].get('audit_ids', [None])[0]

    @property
    def audit_chain_id(self):
        if self.version is V3:
            return self.get('audit_ids', [None"
2665086,-5674758076926003772,270,BigQuery,"  0xC4 -> LATIN CAPITAL LETTER A WITH DIAERESIS
    '\xc5'     #  0xC5 -> LATIN CAPITAL LETTER A WITH RING ABOVE
    '\xc6'     #  0xC6 -> LATIN CAPITAL LETTER AE
    '\xc7'     #  0xC7 -> LATIN CAPITAL LETTER C WITH CEDILLA
    '\xc8'     #  0xC8 -> LATIN CAPITAL LETTER E WITH GRAVE
    '\xc9'     #  0xC9 -> LATIN CAPITAL LETTER E WITH ACUTE
    '"
3105539,1645432752083858362,18,BigQuery,": utf-8 -*-
##
## This file is part of Invenio.
## Copyright (C) 2009, 2010, 2011 CERN.
##
## Invenio is free software; you can redistribute it and/or
## modify it under the terms of the GNU General Public License as
## published by the Free Software Foundation; either version 2 of the
## License, or (at your option) any later version.
##
## Invenio is distributed in the hope that it will be useful, but
## WITHOUT ANY WARRANTY; without even the implied warranty of
## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU"
6773063,7342492980193147495,24,BigQuery,"er', import_from_module='ns.network', parent=root_module['ns3::AttributeChecker'])
    ## ipv6-address.h (module 'network'): ns3::Ipv6AddressValue [class]
    module.add_class('Ipv6AddressValue', import_from_module='ns.network', parent=root_module['ns3::AttributeValue'])
    ## ipv6-address.h (module 'network'): ns3::Ipv6PrefixChecker [class]
    module.add_class('Ipv6PrefixChecker', import_from_module='ns.network', parent"
6681883,-5065870161825241833,8,BigQuery,"(connection) as captured_queries:
            Author.objects.order_by('name').get(pk=self.a1.pk)
        self.assertNotIn('order by', captured_queries[0]['sql'].lower())

    def test_tickets_4088_4306(self):
        self.assertQuerysetEqual(
            Report.objects.filter(creator=1001),
            ['<Report: r1>']
        )
        self.assertQuerysetEqual(
            Report.objects.filter(creator__num=1001),
            ['<Report: r1>']
        )"
59903,5118256724023883340,7,BigQuery,"text, ""text"")
        self.assertEqual(ex.details, (""details"",))

@unittest.skipUnless(sys.platform == ""win32"", 'Windows-specific test')
class TestWinError(unittest.TestCase):
    def test_winerror(self):
        # see Issue 16169
        import errno
        ERROR_INVALID_PARAMETER = 87
        msg = FormatError(ERROR_INVALID_PARAMETER).strip()
        args = (errno.EINVAL, msg, None, ERROR_INVALID_PARAMETER)

        e = WinError(ERROR_"
5120791,-1298873708812091202,6,BigQuery," can be used to
    associate additional data with an object owned by other parts of
    an application without adding attributes to those objects. This
    can be especially useful with objects that override attribute
    accesses.
    """"""

    def __init__(self, dict=None):
        self.data = {}
        def remove(k, selfref=ref(self)):
            self = selfref()
            if self is not None:
                if self._iterating:
                    self._pending_removals.append(k)
                else:
                    del self.data[k]
        self._remove = remove
        # A list of dead weak"
5878534,-4859075484618395367,28,BigQuery,"relative"" is False, then ""filename"" specifies an
        os-specific path.  The path may be absolute or relative (to
        the current working directory).

    Optional keyword arg ""name"" gives the name of the test; by default
    use the file's basename.

    Optional keyword argument ""package"" is a Python package or the
    name of a Python package whose directory should be used as the
    base directory for a module relative filename.  If no package is
    specified, then the calling module's directory is used as the base
    directory for module relative filenames.  It is an error to
    specify ""package"" if ""module_relative"" is False"
272490,-7049203563525320537,60,BigQuery,"else:
        # use bitmap
        data = _mk_bitmap(charmap)
        outappend((CHARSET, data))
        return out
    return charset

def _mk_bitmap(bits):
    data = []
    dataappend = data.append
    if _sre.CODESIZE == 2:
        start = (1, 0)
    else:
        start = (1L, 0L)
    m, v = start
    for c in bits:
        if c:
            v = v + m
        m = m + m
        if m > MAXCODE:
            dataappend"
2271380,-5457092534201525249,9,BigQuery,"resource_name)

    def resource_isdir(self, package_or_requirement, resource_name):
        """"""Is the named resource an existing directory?""""""
        return get_provider(package_or_requirement).resource_isdir(
            resource_name
        )

    def resource_filename(self, package_or_requirement, resource_name):
        """"""Return a true filesystem path for specified resource""""""
        return get_provider(package_or_requirement).get_resource_filename(
            self, resource_name
        )

    def resource_stream(self, package_or_requirement, resource_name):
        "
3029018,3000947920528569681,7,BigQuery,"
        points = (-1, -1, 1, 1)
        x0, y0, x1, y1 = points
        x0, x1 = min(x0, x1), max(x0, x1)
        y0, y1 = min(y0, y1), max(y0, y1)
        
        position = np.zeros((4,2))
        position[0,:] = (x0, y0)
        position[1,:] = (x1, y0)
        position[2,:] = (x0, y1)
        position[3,:] = (x1, y"
378761,7349850773650910505,75,BigQuery,0\u5cd1\u5cd2\u5cd3\u5cd4\u5cd5\u5cd6\u5cd7\u5cd8\u5cd9\u5cda\u5cdb\u5cdc\u5cdd\u5cde\u5cdf\u5ce0\u5ce1\u5ce2\u5ce3\u5ce4\u5ce5\u5ce6\u5ce7\u5ce8\u5ce9\u5cea\u5ceb\u5cec\u5ced\u5cee
1862005,-7429219098999409795,24,BigQuery,"Error, so we use
            # a try/except construct rather than hasattr in order to remain
            # consistent between PY2 and PY3 (hasattr would swallow
            # the TypeError on PY2).
            try:
                arg.default_alias
            except (AttributeError, TypeError):
                raise TypeError(""Complex aggregates require an alias"")
            kwargs[arg.default_alias] = arg

        query = self.query.clone()
        for (alias, aggregate_expr) in kwargs.items():
            query.add_annotation(aggregate_expr, alias, is_summary=True)
            if not"
4741382,6115587308793371333,8,BigQuery," variable_scope.variable_scope(""AttnOutputProjection""):
        inputs = [cell_output] + attns
        output = Linear(inputs, output_size, True)(inputs)
      if loop_function is not None:
        prev = output
      outputs.append(output)

  return outputs, state


def embedding_attention_decoder(decoder_inputs,
                                initial_state,
                                attention_states,
                                cell,
                                num_symbols,
                                embedding_size,
                                num_heads=1,
                                output_size=None,
                                output_projection=None,"
7121592,-2303207396607218482,13,BigQuery," name was requested.'
            sys.exit(2)

    elif len(options.example) == 0 and len(options.pyexample) == 0:
        if len(options.constrain):
            path_cmd = os.path.join(""utils"", test_runner_name + "" --print-test-name-list --test-type=%s"" % options.constrain)
            (rc, suites, standard_err, et) = run_job_synchronously(path_cmd, os.getcwd(), False, False)
        else:
            path_cmd = os.path.join(""utils"", test_runner_name +"
6272155,-1818652986259401168,7,BigQuery," _CheckAtLeast3DImage(image, require_static=False)

  batch, height, width, depth = _ImageDimensions(image, rank=4)

  after_padding_width = target_width - offset_width - width
  after_padding_height = target_height - offset_height - height

  assert_ops += _assert(offset_height >= 0, ValueError,
                        'offset_height must be >= 0')
  assert_ops += _assert(offset_width >= 0, ValueError,
                        'offset_width must be >= 0')
  assert_ops += _assert(after_padding_width >= 0, ValueError,"
5484160,6679389820547492622,7,BigQuery,"os.path.dirname(output_filename))
            try:
                if target_desc == CCompiler.EXECUTABLE:
                    linker = self.linker_exe[:]
                else:
                    linker = self.linker_so[:]
                if target_lang == ""c++"" and self.compiler_cxx:
                    # skip over environment variable settings if /usr/bin/env
                    # is used to set up the linker's environment.
                    # This is needed on OSX. Note: this assumes that the
                    # normal and C++ compiler have the same environment
                    # settings.
                    i = 0
                    "
1710093,3279210894079309402,7,BigQuery," input_data, input_name=''):
            return '%e' % input_data
        def gds_validate_double(self, input_data, node, input_name=''):
            return input_data
        def gds_format_double_list(self, input_data, input_name=''):
            return '%s' % input_data
        def gds_validate_double_list(self, input_data, node, input_name=''):
            values = input_data.split()
            for value in values:
                try:
                    fvalue = float(value)
                except (TypeError"
2665144,651772694725374073,16,BigQuery," coefficients may be thought of as stored in
        the columns of `c`.
    tensor : boolean, optional
        If True, the shape of the coefficient array is extended with ones
        on the right, one for each dimension of `x`. Scalars have dimension 0
        for this action. The result is that every column of coefficients in
        `c` is evaluated for every element of `x`. If False, `x` is broadcast
        over the columns of `c` for the evaluation.  This keyword is useful
        when `c` is multidimensional. The default value is True.

       .. versionadded:: 1.7.0

    Returns
    -------
    values"
3468239,-4399101504105068141,8,BigQuery,"set):
        name_list = self._set_to_list(name_set)
        for i in range(len(name_list)):
            self._request['EnvironmentNames.member.' + misc.to_unicode(i + 1)] \
                = misc.to_unicode(name_list[i])

    def set_env_ids(self, id_set):
        id_list = self._set_to_list(id_set)
        for i in range(len(id_list)):
            self._request['EnvironmentIds.member.' + misc.to_unicode(i + 1)] \
                = misc"
4620588,-706755527966318631,14,BigQuery,"): bool ns3::WifiRemoteStationManager::NeedDataRetransmission(ns3::Mac48Address address, ns3::WifiMacHeader const * header, ns3::Ptr<ns3::Packet const> packet) [member function]
    cls.add_method('NeedDataRetransmission', 
                   'bool', 
                   [param('ns3::Mac48Address', 'address'), param('ns3::WifiMacHeader const *', 'header'), param('ns3::Ptr< ns3::Packet const >', 'packet')])
    ## wifi-remote-station-manager.h (module 'wifi'): bool ns3::W"
2771735,-912709977935734002,38,BigQuery,"LiIonEnergySourceHelper const & arg0) [copy constructor]
    cls.add_constructor([param('ns3::LiIonEnergySourceHelper const &', 'arg0')])
    ## li-ion-energy-source-helper.h (module 'energy'): ns3::LiIonEnergySourceHelper::LiIonEnergySourceHelper() [constructor]
    cls.add_constructor([])
    ## li-ion-energy-source-helper.h (module 'energy'): void ns3::LiIonEnergySourceHelper::Set(std::string name, ns3::AttributeValue const & v) [member function]
"
6423628,-1950029596527939328,6,BigQuery," checked.'
           '%s '
            'Check your credentials' % (len(names), str(names)))

    # We select the last ready auth handler that was loaded, to allow users to
    # customize how auth works in environments where there are shared boto
    # config files (e.g., /etc/boto.cfg and ~/.boto): The more general,
    # system-wide shared configs should be loaded first, and the user's
    # customizations loaded last. That way, for example, the system-wide
    # config might include a plugin_directory that includes a service account
    # auth plugin shared by all users of a Google Compute Engine"
4848498,-7848905445620469859,26,BigQuery,"Host` header.

    :param environ: the WSGI environment to get the host of.
    """"""
    if 'HTTP_X_FORWARDED_HOST' in environ:
        return environ['HTTP_X_FORWARDED_HOST']
    elif 'HTTP_HOST' in environ:
        return environ['HTTP_HOST']
    result = environ['SERVER_NAME']
    if (environ['wsgi.url_scheme'], environ['SERVER_PORT']) not \
       in (('https', '443'), ('http', '80')):
        result += ':' + environ"
4348622,8993279666508200493,37,BigQuery,"iled_source)
              if extension == '.c':
                extensions_excluded_from_precompile = ['.cc', '.cpp', '.cxx']
              else:
                extensions_excluded_from_precompile = ['.c']

          if precompiled_source == source:
            condition = _GetConfigurationCondition(config_name, configuration)
            detail.append(['PrecompiledHeader',
                           {'Condition': condition},
                           'Create'
                          ])
          else:
            # Turn off precompiled header usage for source files of a
            # different type than the file that generated the
            # precompiled header"
1588974,-4330094319390855998,11,BigQuery,"yspark.sql.dataframe import DataFrame
        return DataFrame(jdf, self._spark)

    @since(1.4)
    def format(self, source):
        """"""Specifies the input data source format.

        :param source: string, name of the data source, e.g. 'json', 'parquet'.

        >>> df = spark.read.format('json').load('python/test_support/sql/people.json')
        >>> df.dtypes
        [('age', 'bigint'), ('name','string')]

        """"""
        self._jreader = self._jreader.format(source)
        "
3136506,7610404522716374581,183,BigQuery," module level so that CheckboxInput is picklable (#17976)
def boolean_check(v):
    return not (v is False or v is None or v == '')


class CheckboxInput(Widget):
    def __init__(self, attrs=None, check_test=None):
        super(CheckboxInput, self).__init__(attrs)
        # check_test is a callable that takes a value and returns True
        # if the checkbox should be checked for that value.
        self.check_test = boolean_check if check_test is None else check_test

    def render(self, name, value, att"
3045010,8753327269572341612,57,BigQuery,"=_('Designates that this user has all permissions without '
                    'explicitly assigning them.'))
    last_login = models.DateTimeField(_('last login'), default=timezone.now)
    date_joined = models.DateTimeField(_('date joined'), default=timezone.now)
    groups = models.ManyToManyField(Group, verbose_name=_('groups'),
        blank=True, help_text=_('The groups this user belongs to. A user will '
                                'get all permissions granted to each of '
                                'his/her group.'))
    user_permissions = models.ManyToManyField(Permission"
1029767,-2682963355554445001,20,BigQuery,"logd_override_setting_idempotent(mocker):
    schema_method_mock = mocker.patch('ansible.module_utils.network.fortios.fortios.FortiOSHandler.schema')

    set_method_result = {'status': 'error', 'http_method': 'DELETE', 'http_status': 404}
    set_method_mock = mocker.patch('ansible.module_utils.network.fortios.fortios.FortiOSHandler.set', return_value=set_method_result)

    input_data = {
        'username': 'admin',
       'state':"
6500025,5385011629961133936,11,BigQuery,"/combq.bin' )
pl3d.SetScalarFunctionNumber( 100 )
pl3d.SetVectorFunctionNumber( 202 )
pl3d.Update()
pl3d_output = pl3d.GetOutput().GetBlock(0)

outline = vtk.vtkStructuredGridOutlineFilter()
outline.SetInputData(pl3d_output)

outlineMapper = vtk.vtkPolyDataMapper()
outlineMapper.SetInputConnection(outline.GetOutputPort())

outlineActor = vtk.vtkActor()
outlineActor.SetMapper(outlineMapper"
893839,-7816193542071589666,251,BigQuery," sys.modules[moduleOrReq]
    loader = getattr(module, '__loader__', None)
    return _find_adapter(_provider_factories, loader)(module)

def _macosx_vers(_cache=[]):
    if not _cache:
        version = platform.mac_ver()[0]
        # fallback for MacPorts
        if version == '':
            plist = '/System/Library/CoreServices/SystemVersion.plist'
            if os.path.exists(plist):
                if hasattr(plistlib,'readPlist'):
                    plist_content = pl"
6241962,2061339987962069069,8,BigQuery,".objects.filter(state='TX').aggregate(Collect('location__point'))['location__point__collect']
        # Even though Dallas and Ft. Worth share same point, Collect doesn't
        # consolidate -- that's why 4 points in MultiPoint.
        self.assertEqual(4, len(coll))
        self.assertTrue(ref_geom.equals(coll))

    def test15_invalid_select_related(self):
        ""Testing doing select_related on the related name manager of a unique FK. See #13934.""
        qs = Article.objects.select_related('author__article')
        # This triggers TypeError"
4620588,-3957937420018845823,14,BigQuery,"
                   [], 
                   is_static=True)
    ## wifi-phy.h (module 'wifi'): static ns3::WifiMode ns3::WifiPhy::GetErpOfdmRate18Mbps() [member function]
    cls.add_method('GetErpOfdmRate18Mbps', 
                   'ns3::WifiMode', 
                   [], 
                   is_static=True)
    ## wifi-phy.h (module 'wifi'): static ns3::WifiMode ns3::WifiPhy::GetErpOfdmRate24Mbps() [member function]
    cls.add_method"
2483153,-5074419185460414034,8,BigQuery,"if (model_state.app_label, model_state.name_lower) in related_models:
                states_to_be_rendered.append(model_state)

        # 2. All related models of migrated apps
        for rel_app_label, rel_model_name in related_models:
            try:
                model_state = self.models[rel_app_label, rel_model_name]
            except KeyError:
                pass
            else:
                states_to_be_rendered.append(model_state)

        # Render all models
        self.apps.render_multiple(states_to_be_rendered)

    "
3666150,4390197585528322873,48,BigQuery,"exists"" if os.path.isdir(USER_BASE) else ""doesn't exist"")
        print ""USER_SITE: %r (%s)"" % (USER_SITE,
            ""exists"" if os.path.isdir(USER_SITE) else ""doesn't exist"")
        print ""ENABLE_USER_SITE: %r"" %  ENABLE_USER_SITE
        sys.exit(0)

    buffer = []
    if '--user-base' in args:
        buffer.append(USER_BASE)
    if '--user-site' in args:
        buffer.append(USER_SITE"
377975,-7901081813607288642,8,BigQuery," ('tuple',)),
    ('<utility>', ('pair',)),
    ('<vector>', ('vector',)),

    # gcc extensions.
    # Note: std::hash is their hash, ::hash is our hash
    ('<hash_map>', ('hash_map', 'hash_multimap',)),
    ('<hash_set>', ('hash_set', 'hash_multiset',)),
    ('<slist>', ('slist',)),
    )

_RE_PATTERN_STRING = re.compile(r'\bstring\b')

_re_pattern_algorithm_header = []
"
6773063,-5210319659797883403,24,BigQuery,"=True)
    return

def register_Ns3CallbackValue_methods(root_module, cls):
    ## callback.h (module 'core'): ns3::CallbackValue::CallbackValue(ns3::CallbackValue const & arg0) [copy constructor]
    cls.add_constructor([param('ns3::CallbackValue const &', 'arg0')])
    ## callback.h (module 'core'): ns3::CallbackValue::CallbackValue() [constructor]
    cls.add_constructor([])
    ## callback.h (module 'core'): ns3::CallbackValue::CallbackValue(ns3::CallbackBase const &"
1119851,-1406294555789346731,9,BigQuery,"+$2+1', 
      u'\\dfrac':u'$1+$2', u'\\frac':u'$1+$2', u'\\tbinom':u'$1+$2+1', 
      }

  labelfunctions = {
      u'\\label':u'a name=""#""', 
      }

  limitcommands = {
      u'\\biginterleave':u'⫼', u'\\bigsqcap':u'⨅', u'\\fint':u'⨏', 
      u'\\iiiint':u'⨌', u'\\int':"
3817928,-3890936691580378880,17,BigQuery,"google.com/g/2005#kind'
    term='http://schemas.google.com/apps/2006#user'/>
  <atom:title type=""text"">TestUser</atom:title>
  <atom:link rel=""self"" type=""application/atom+xml""
    href=""https://apps-apis.google.com/a/feeds/example.com/user/2.0/TestUser""/>
  <atom:link rel=""edit"" type=""application/atom+xml""
    href=""https://apps-apis.google.com/a/feeds/example.com/user/2.0/TestUser""/>"
3802102,-6959212037516438069,6,BigQuery,"-app@appspot.gserviceaccount.com.   - group:emailid - An email
        address that represents a Google group. For example,
        group:admins@example.com.   - domain:domain - A Google Apps domain
        name that represents all the users of that domain. For example,
        domain:google.com or domain:example.com.   - projectOwner:projectid -
        Owners of the given project. For example, projectOwner:my-example-
        project   - projectEditor:projectid - Editors of the given project.
        For example, projectEditor:my-example-project   -
        projectViewer:projectid"
377975,-7522279597393379257,8,BigQuery,"  # Don't warn on out-of-line method definitions, as we would warn on the
  # in-line declaration, if it isn't marked with 'override'.
  if IsOutOfLineMethodDefinition(clean_lines, linenum):
    return

  # Long type names may be broken across multiple lines, usually in one
  # of these forms:
  #   LongType
  #       ::LongTypeContinued &identifier
  #   LongType::
  #       LongTypeContinued &identifier
  #   LongType<
  #      ...>::LongTypeContinued &identifier
  #
  # If we detected a type split"
2862961,4507131969534178372,38,BigQuery,"cls.add_method('RemoveAddress', 
                   'bool', 
                   [param('uint32_t', 'interface'), param('ns3::Ipv4Address', 'address')], 
                   is_pure_virtual=True, is_virtual=True)
    ## ipv4.h (module 'internet'): ns3::Ipv4Address ns3::Ipv4::SelectSourceAddress(ns3::Ptr<const ns3::NetDevice> device, ns3::Ipv4Address dst, ns3::Ipv4InterfaceAddress::InterfaceAddressScope_e scope) [member function]
    cls.add_method('"
6151456,4893339350679188652,9,BigQuery,".pow, xrange(1,4), repeat(3)):
...    print cube
...
1
8
27

>>> reportlines = ['EuroPython', 'Roster', '', 'alex', '', 'laura', '','martin', '', 'walter', '','samuele']
>>> for name in islice(reportlines, 3, None, 2):
...    print name.title()
...
Alex
Laura
Martin
Walter
Samuele

>>> from operator import itemgetter
>>> d = dict(a=1, b=2, c=1, d=2, e=1,"
5137002,7416827554483585041,50,BigQuery,"code, self.example)
        self.assertEqual(a, b)

        # non-iterable argument
        self.assertRaises(TypeError, array.array, self.typecode, 10)

        # pass through errors raised in __iter__
        class A:
            def __iter__(self):
                raise UnicodeError
        self.assertRaises(UnicodeError, array.array, self.typecode, A())

        # pass through errors raised in next()
        def B():
            raise UnicodeError
            yield None
        self.assertRaises(UnicodeError, array.array, self.typecode, B"
5424214,-7764698699202268322,29,BigQuery," tuple:
        # skipped by mark.skipif; change the location of the failure
        # to point to the item definition, otherwise it will display
        # the location of where the skip exception was raised within pytest
        filename, line, reason = rep.longrepr
        filename, line = item.location[:2]
        rep.longrepr = filename, line, reason

# called by terminalreporter progress reporting
def pytest_report_teststatus(report):
    if hasattr(report, ""wasxfail""):
        if report.skipped:
            return ""xfailed"", ""x"", ""xfail""
        elif report."
